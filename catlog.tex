\documentclass{amsart}
\usepackage{mathpartir,cancel}
\newif\ifcref\creftrue
\input{decls}
\title{Categorical logic from a categorical point of view}
\author{}
\date{\today}
\autodefs{\bSet\bPoset\bRelGr\bCat\bGr}
\let\types\vdash
\let\sect\section
\def\idfunc{\mathsf{id}}
\begin{document}
\maketitle
\setcounter{tocdepth}{1}
\tableofcontents

\section{Appetizer: inverses in group objects}
\label{sec:intro}

In this section we consider an extended example.
We do not expect the reader to understand it very deeply, but we hope it will give some motivation for what follows, as well as a taste of the power and flexibility of categorical logic as a tool for category theory.

Our example will consist of several varations on the following theorem:

\begin{thm}
  If a monoid has inverses (hence is a group), then those inverses are unique.
\end{thm}

When ``monoid'' and ``group'' have their usual meaning, namely sets equipped with structure, the proof is easy.
For any $x$, if $i(x)$ and $j(x)$ are both two-sided inverse of $x$, then we have
\[ i(x) = i(x) \cdot e = i(x) \cdot (x \cdot j(x)) = (i(x)\cdot x)\cdot j(x) = e\cdot j(x) = j(x) \]
However, the theorem is true much more generally than this.
We consider first the case of monoid/group objects in a category with products.
A \emph{monoid object} is an object $M$ together with maps $m:M\times M \to M$ and $e:1\to M$ satisfying associativity and unitality axioms:
\begin{equation}
  \vcenter{\xymatrix{
      M\times M\times M\ar[r]^-{1\times m}\ar[d]_{m\times 1} &
      M\times M\ar[d]^m\\
      M\times M\ar[r]_m &
      M
    }}
  \qquad
  \vcenter{\xymatrix{ M \ar[r]^-{(1,e)} \ar[dr]_{1} &
    M\times M \ar[d]_m & M \ar[l]_-{(e,1)} \ar[dl]^{1} \\
    & M }}
\end{equation}
An \emph{inverse operator} for a monoid object is a map $i:M\to M$ such that
\begin{equation}
  \vcenter{\xymatrix@C=1pc{& M\times M \ar[rr]^{i\times 1} && M\times M \ar[dr]^m \\
      M \ar[ur]^{\Delta} \ar[dr]_{\Delta} \ar[rr]^{!} && 1 \ar[rr]^{e} && M \\
      & M\times M \ar[rr]_{1\times i} && M\times M \ar[ur]_{m}}}
\end{equation}
The internalized claim, then, is that any two inverse operators are equal.
The \emph{internal logic of a category with products} allows us to prove this by using essentially the same argument that we did in the case of ordinary monoids in \bSet.
The morphisms $m$ and $e$ are represented in this logic by the notations
\begin{mathpar}
  x:M,y:M \types x\cdot y :M \and
  \types e:M.
\end{mathpar}
Don't worry if this notation doesn't make a whole lot of sense yet.
The symbol $\types$ (called a ``turnstile'') is the logic version of a morphism arrow $\to$, and the entire notation is called a \emph{sequent} or a \emph{judgment}.
The fact that $m$ is a morphism $M\times M \to M$ is indicated by the fact that $M$ appears twice to the left of $\types$ and once to the right; the comma ``$,$'' in between $x:M$ and $y:M$ represents the product $\times$, and the variables $x,y$ are there so that we have a good notation ``$x\cdot y$'' for the morphism $m$.
In particular, the notation $x:M,y:M \types x\cdot y :M$ should be bracketed as
\[ ((x:M),(y:M)) \types ((x\cdot y) :M). \]
Similarly, the associativity, unit, and inverse axioms are indicated by the notations
\begin{mathpar}
  x:M,y:M,z:M \types (x\cdot y)\cdot z = x\cdot (y\cdot z) : M \\
  x:M \types x\cdot e = x : M \and
  x:M \types e\cdot x = x : M \\F
  x:M \types x\cdot i(x) = e : M \and
  x:M \types i(x) \cdot x = e : M
\end{mathpar}
Now the \bSet-based proof can be essentially copied in this notation:
\[ x:M \types i(x) = i(x) \cdot e = i(x) \cdot (x \cdot j(x)) = (i(x)\cdot x)\cdot j(x) = e\cdot j(x) = j(x) : M.\]
The essential point is that the notation \emph{looks set-theoretic}, with ``variables'' representing ``elements'', and yet (as we will see) its formal structure is such that it can be interpreted into \emph{any} category with products.
Therefore, writing the proof in this way yields automatically a proof of the general theorem that any two inverse \emph{operators} for a monoid \emph{object} in a category with products are equal.

To be sure, such a proof could be written in standard categorical style using commutative diagrams as well (\cref{ex:fp-inv-uniq}), and experienced category theorists become quite good at translating proofs in this way.
The point of categorical logic is that this sort of work is \emph{so} straightforward that it's actually completely unnecessary to do at all: we can prove a ``meta-theorem'' that does all the work for us.

Before leaving this appetizer section, we mention some further generalizations of this result.
While type theory allows us to use set-like notation to prove facts about any category with finite products, the allowable notation is fairly limited, essentially restricting us to algebraic calculations with variables.
However, if our category has more structure, then we can ``internalize'' more set-theoretic arguments.

As an example, note that for ordinary monoids in sets the uniqueness of inverses can be expressed ``pointwise'' rather than in terms of inverse-assigning operators.
In other words, for each element $x\in M$, if $x$ has two two-sided inverses $y$ and $z$, then $y=z$.
If we think hard enough, we can express this diagrammatically in terms of the category \bSet is to consider the following two sets:
\begin{align*}
  A &= \setof{(x,y,z)\in M^3 | xy=e, yx=e, xz=e, zx=e}\\
  B &= \setof{(y,z)\in M^2 | y=z}
\end{align*}
In other words, $A$ is the set of elements $x$ equipped with two inverses, and $B$ is the set of pairs of equal elements.
Then the uniqueness of pointwise inverses can be expressed by saying there is a commutative diagram
\[ \xymatrix{ A \ar[d] \ar[r] & B \ar[d] \\ M^3 \ar[r]_{\pi_{23}} & M^2 } \]
where the vertical arrows are inclusions and the lower horizontal arrow projects to the second and third components.

This is a statement that makes sense for a monoid object $M$ in any category with finite \emph{limits}.
The object $B$ can be constructed categorically as the equalizer of the two projections $M\times M \toto M$ (which is in fact isomorphic to $M$ itself), while the object $A$ is a ``joint equalizer'' of four parallel pairs, one of which is
\[ \vcenter{\xymatrix{ & M \times M \ar[dr]^m \\
    M\times M\times M \ar[ur]^{\pi_{12}} \ar[dr]_{!} && M \\
    & 1 \ar[ur]_e }} \]
and the others are similar.
We can then try to \emph{prove}, in this generality, that there is a commutative square as above.
We can do this by manipulating arrows, or by appealing to the Yoneda lemma, but we can also use the \emph{internal logic of a category with finite limits}.
This is a syntax like the internal logic for categories with finite products, but which also allows us to \emph{hypothesize equalities}.
The judgment in question is
\begin{equation}\label{eq:pointwise-unique-inverses}
  x:M, y:M, z:M, x\cdot y = e, y\cdot x=e, x\cdot z = e, z\cdot x = e \types y=z.
\end{equation}
As before, the comma binds the most loosely, so this should be read as
\[ ((x:M), (y:M), (z:M), (x\cdot y = e), (y\cdot x=e), (x\cdot z = e), (z\cdot x = e)) \types (y=z). \]
We can prove this by set-like equational reasoning, essentially just as before.
The ``interpretation machine'' then produces from this a morphism $A\to B$, for the objects $A$ and $B$ constructed above.

Next, note that in the category \bSet, the uniqueness of inverses ensures that if every element $x\in M$ has an inverse, then there is a \emph{function} $i:M\to M$ assigning inverses --- even without using the axiom of choice.
(If we define functions as sets of ordered pairs, as is usual in set-theoretic foundations, we could take $i = \setof{(x,y) | xy=e}$; the pointwise uniqueness ensures that this is indeed a function.)
This fact can be expressed in the internal logic of an \emph{elementary topos}.
We postpone the definition of a topos until later; for now we just remark that its structure allows both sides of the turnstile $\types$ to contain \emph{logical formulas} such as $\exists x, \forall y, \phi(x,y)$ rather than just elements and equalities.
In this language we can state and prove the following:
\[ \forall x:M, \exists y:M, x\cdot y = e \land y\cdot x = e \types
\exists i:M^M, \forall x:M, (x\cdot i(x) = e \land i(x)\cdot x = e)
\]
As before, the proof is essentially exactly like the usual set-theoretic one.
Moreover, the interpretation machine allows us to actually extract an ``inverse operator'' morphism in the topos from this proof.
As before, such a result can also be stated and proved using arrows and commutative diagrams, but as the theorems get more complicated, the translation gets more tedious to do by hand, and the advantage of type-theoretic notation becomes greater.

So much for adding extra structure.
In fact, we can also take structure away!
A monoid object can be defined internal to any \emph{monoidal} category, not just a cartesian monoidal one; now the structure maps are $m:M\otimes M\to M$ and $e:I\to M$, and the commutative diagrams are essentially the same.

To define an inverse operator in this case, however, we need some sort of ``diagonal'' $\Delta:M\to M\otimes M$ and also a ``projection'' or ``augmentation'' $\varepsilon:M\to I$.
The most natural hypothesis is that these maps make $M$ into a \emph{comonoid} object, i.e.\ a monoid in the opposite monoidal category, and that the monoid and comonoid structures preserve each other; this is the notion of a \emph{bimonoid} (or ``bialgebra'').
(\cref{ex:cartmon-bimon-uniq}: in a cartesian monoidal category, every object is a bimonoid in a unique way.)

Now given a bimonoid $M$, we can define an ``inverse operator'' --- which in this context is usually called an \emph{antipode} --- to be a map $i:M\to M$ such that
\begin{equation}
  \vcenter{\xymatrix@C=1pc{& M\otimes M \ar[rr]^{i\otimes 1} && M\otimes M \ar[dr]^m \\
      M \ar[ur]^{\Delta} \ar[dr]_{\Delta} \ar[rr]^{\varepsilon} && I \ar[rr]^{e} && M \\
      & M\otimes M \ar[rr]_{1\otimes i} && M\otimes M \ar[ur]_{m}}}
\end{equation}
commutes, where now $\Delta$ and $\varepsilon$ are the comonoid structure of $M$ rather than the diagonal and projection of a cartesian product.
A bimonoid equipped with an antipode is called a \emph{Hopf monoid} (or ``Hopf algebra'').
The obvious question then is, if a bimonoid has two antipodes, are they equal?

In some cases it is possible to apply the previous results directly.
For instance, the category of \emph{(co)commutative} comonoids in a symmetric monoidal category inherits a monoidal structure that turns out to be \emph{cartesian} (\cref{ex:ccmon-cart}), so a cocommutative bimonoid is actually a monoid in a cartesian monoidal category, and we can apply the first version of our result.
Similarly, the category of commutative monoids is cocartesian, so a commutative bimonoid is a comonoid in a cocartesian monoidal category, so we can apply the dual of the first version of our result.
But what if neither the multiplication nor the comultiplication is commutative?

Internal logic is up to this task.
In a monoidal category we can consider judgments with multiple outputs as well as multiple inputs.
This allows us to describe monoids and comonoids in a roughly ``dual'' way:
\begin{alignat*}{2}
  x:M, y:M &\types x\cdot y:M &\qquad x:M &\types (x_1,x_2):(M,M)\\
  &\types e:M &\qquad x:M &\types \cancel{x}:()\\
  x:M,y:M,z:M &\types (x\cdot y)\cdot z = x\cdot (y\cdot z) :M &\qquad x:M &\types (x_{11},x_{12},x_2)=(x_1,x_{21},x_{22}):(M,M,M)\\
  x:M &\types x\cdot e=x:M &\qquad x:M &\types (x_1,\cancel{x_2}) = x:M\\
  x:M &\types e\cdot x=x:M &\qquad x:M &\types (\cancel{x_1},x_2) = x:M
\end{alignat*}
In this language, the bimonoid axioms are
\begin{align*}
  x:M,y:M &\types (x_1\cdot y_1,x_2\cdot y_2) = ((x\cdot y)_1,(x\cdot y)_2) :(M,M)\\
          &\types (e_1,e_2)=(e,e):(M,M)\\
  x:M,y:M &\types \cancel{x\cdot y} = (\cancel{x},\cancel{y}) : ()\\
  &\types \cancel{e}=():()
\end{align*}
And an antipode is a map $x:M \types i(x):M$ such that
\begin{align*}
  x:M &\types x_1\cdot i(x_2) = (e,\cancel{x}) :M\\
  x:M &\types i(x_1)\cdot x_2 = (e,\cancel{x}) :M
\end{align*}
Now if we have another antipode $j$, we can compute
\begin{align*}
  x:M \types i(x)
  &= i(x)\cdot e\\
  &= (i(x_1)\cdot e,\cancel{x_2})\\
  &= i(x_1)\cdot (x_{21} \cdot j(x_{22}))\\
  &= (i(x_1)\cdot x_{21}) \cdot j(x_{22})\\
  &= (\cancel{x_1},e \cdot j(x_{2}))\\
  &= e\cdot j(x)\\
  &= j(x) \qquad :M
\end{align*}
yielding the same result $i=j$.
So even in a non-cartesian situation, we can use a very similar set-like argument, as long as we keep track of where elements get ``duplicated and discarded''.

This concludes our ``appetizer''; I hope it has given you a taste of what categorical logic looks like, and what it can do for category theory.
In the next section we will rewind back to the beginning and start with very simple cases.

\subsection*{Exercises}

\begin{ex}\label{ex:fp-inv-uniq}
  Prove, using arrows and commutative diagrams, that any two inverse operators for a monoid object in a category with finite products are equal.
\end{ex}

\begin{ex}\label{ex:cartmon-bimon-uniq}
  Prove that in a cartesian monoidal category, every object is a bimonoid in a unique way.
\end{ex}

\begin{ex}\label{ex:ccmon-cart}
  Show that the category of cocommutative comonoids in a symmetric monoidal category inherits a monoidal structure, and that this monoidal structure is cartesian.
\end{ex}

\begin{ex}\label{ex:antipode}
  Prove, using arrows and commutative diagrams, that any two antipodes for a bimonoid (not necessarily commutative or cocommutative) are equal.
\end{ex}


\section{Posets}
\label{sec:poset}

To begin our formal investigation of categorical logic, we start with the simplest sort of categories: those in which each hom-set has at most one element.
These are well-known to be equivalent to \emph{preordered sets}, where the existence of an arrow $A\to B$ is regarded as the assertion that $A\le B$.
I will abusively call them \emph{posets}, although traditionally posets (partially ordered sets) also satisfy the antisymmetry axiom (if $A\le B$ and $B\le A$ then $A=B$); from a category-theoretic perspective, antisymmetry means asking a category to be skeletal, which is both unnatural and pointless.
Conveniently, posets also correspond to the simplest version of logic, namely \emph{propositional} logic.

From a category-theoretic perspective, the question we are concerned with is the following.
Suppose we have some objects in a poset, and some ordering relations between them.
For instance, we might have
\begin{mathpar}
  A\le B \and A\le C \and D\le A \and B \le E \and D\le C
\end{mathpar}
Now we ask, given two of these objects --- say, $D$ and $E$ --- is it necessarily the case that $D\le E$?
In other words, is it the case in \emph{any} poset containing objects $A,B,C,D,E$ satisfying the given relations that $D\le E$?
In this example, the answer is yes, because we have $D\le A$ and  $A\le B$ and $B\le E$, so by transitivity $D\le E$.
More generally, we would like a method to answer all possible questions of this sort.

There is an elegant categorical way to do this based on the notion of \emph{free structure}.
Namely, consider the category \bPoset of posets, and also the category \bRelGr of \emph{relational graphs}, by which I mean sets equipped with an arbitrary binary relation.
There is a forgetful functor $U:\bPoset \to \bRelGr$, which has a left adjoint $F$.

Now, the abstract information about ``five objects $A,B,C,D,E$ satisfying five given relations'' can be regarded as an object $\cG$ of \bRelGr, and to give five such objects satisfying those relations in a poset \cP is to give a map $\cG \to U\cP$ in \bRelGr.
By the adjunction, therefore, this is equivalent to giving a map $F\cG \to \cP$ in \bPoset.
Therefore, a given inequality such as $A\le E$ will hold in \emph{all} posets if and only if it holds in the \emph{particular, universal} poset $F\cG$ freely generated by the assumed data.

Thus, to answer all such questions at once, it suffices to give a concrete presentation of the free poset $F\cG$ generated by a relational graph \cG.
In this simple case, it is easy to give an explicit description of $F$: it is the reflexive-transitive closure.
But since soon we will be trying to generalize vastly, we want instead a general method to describe free objects.
From our current perspective, this is the role of type theory.

As noted in \cref{sec:intro}, when we move into type theory we use the symbol $\types$ instead of $\to$ or $\le$.
Type theory is concerned with \emph{(hypothetical) judgments}, which (roughly speaking) are syntactic gizmos of the form ``$\Gamma\types\Delta$'', where $\Gamma$ and $\Delta$ are syntactic gadgets whose specific nature is determined by the specific type theory under consideration (and, thus, by the particular kind of categories we care about).
We call $\Gamma$ the \emph{antecedent} or \emph{context}, and $\Delta$ the \emph{consequent} or \emph{co-context}.
In our simple case of posets, the judgments are simply
\[ A \types B \]
where $A$ and $B$ are objects of our (putative) poset; such a judgment represents the relation $A\le B$.
In general, the categorical view is that a hypothetical judgment represents a sort of \emph{morphism} (or, as we will see later, a sort of \emph{object}) in some sort of categorical structure.

In addition to a class of judgments, a type theory consists of a collection of \emph{rules} by which we can operate on such judgments.
Each rule can be thought of as a partial $n$-ary operation on the set of possible judgments for some $n$ (usually a finite natural number), taking in $n$ judgments (its \emph{premises}) that satisfy some compatibility conditions and producing an output judgment (its \emph{conclusion}).
We generally write a rule in the form
\begin{mathpar}
  \inferrule{\cJ_1 \\ \cJ_2 \\ \cdots \\ \cJ_n}{\cJ}
\end{mathpar}
with the premises above the line and the conclusion below.
A rule with $n=0$ is sometimes called an \emph{axiom}.
The categorical view is that we have a given ``starting'' set of judgments representing some objects and putative morphisms in the ``underlying data'' of a categorical structure, and the closure of this set under application of the rules yields the objects and morphisms in the \emph{free} structure it generates.

This is all very general and abstract, so let's bring it back down to earth in our very simple example.
Since the properties distinguishing a poset are reflexivity and transitivity, we have two rules:
\begin{mathpar}
  \inferrule{ }{A\types A} \and
  \inferrule{A\types B \\ B\types C}{A\types C}
\end{mathpar}
in which $A,B,C$ represent arbitrary objects.
In other words, the first rule is that for any object $A$ we have a $0$-ary rule whose conclusion is $A\types A$, while the second is that for any objects $A,B,C$ we have a $2$-ary rule whose premises are $A\types B$ and $B\types C$ (that is, any two judgments of which the consequent of the first is the antecedent of the second) and whose conclusion is $A\types C$.
We will refer to the pair of these two rules as the \textbf{free type theory of posets}.

Hopefully it makes sense that we can construct the reflexive-transitive closure of a relational graph by expressing its relations in this funny syntax and then closing up under these two rules, since they are exactly reflexivity and transitivity.
Categorically, of course, that means identities and composition.
In type theory the composition/transitivity rule is often called \emph{cut}, and plays a unique role; we will touch on this later on, though it is not as important from a purely categorical standpoint.

In the example we started from,
\begin{mathpar}
  A\le B \and A\le C \and D\le A \and B \le E \and D\le C
\end{mathpar}
we have the two instances of the transitivity rule
\begin{mathpar}
  \inferrule{D\types A \\ A\types B}{D\types B}\and
  \inferrule{D\types B \\ B\types E}{D\types E}
\end{mathpar}
allowing us to conclude $D\types E$.
When applying multiple rules in sequence to reach a conclusion, it is customary to write them in a ``tree'' structure like so:
\begin{mathpar}
  \inferrule*{\inferrule*{D\types A \\ A\types B}{D\types B} \\ B\types E}{D\types E}
\end{mathpar}
Such a tree is called a \emph{derivation}.
The way to typeset rules and derivations in \LaTeX\ is with the \texttt{mathpartir} package; the above diagram was produced with
\begin{verbatim}
  \inferrule*{
    \inferrule*{D\types A \\ A\types B}{D\types B} \\
    B\types E
  }{
    D\types E
  }
\end{verbatim}
Note that \texttt{mathpartir} has only recently made it into standard distributions of \LaTeX, so if you have an older system you may need to download it manually.

Formally speaking, what we have observed is the following \emph{initiality theorem}.

\begin{thm}\label{thm:initiality-posets-1}
  For any relational graph \cG, the free poset $F_{\bPoset}\cG$ that it generates is has the same objects and its morphisms are the judgments that are derivable from \cG in free type theory of posets.
\end{thm}

This enables us to reach conclusions about arbitrary posets by deriving judgments in type theory.

Another way to express this result is to incorporate \cG into the rules.
Given a relational graph \cG, we define the \textbf{type theory of posets under \cG} to be the free type theory of posets together with a 0-ary rule
\begin{mathpar}
  \inferrule{ }{A\types B}
\end{mathpar}
for any relation $A\le B$ in \cG.
Now a derivation can be written without any ``leaves'' at the top, such as
\begin{mathpar}
  \inferrule*{\inferrule*{\inferrule*{ }{D\types A} \\ \inferrule*{ }{A\types B}}{D\types B} \\ \inferrule*{ }{B\types E}}{D\types E}
\end{mathpar}
Clearly this produces the same judgments; thus the initiality theorem can also be expressed as follows.

\begin{thm}\label{thm:initiality-posets-2}
  For any relational graph \cG, the free poset $F_{\bPoset}\cG$ that it generates is has the same objects and its morphisms are the derivable judgments in the type theory of posets under \cG.
\end{thm}

We can extract from this our first general statement about categorical logic: it is \emph{a syntax for generating free categorical structures using derivations from rules}.
The reader may be forgiven at this time for wondering what the point is; but bear with us and things will get less trivial.


\section{Categories}
\label{sec:categories}

Let's now generalize from posets to categories.
The relevant adjunction is now between categories \bCat and \emph{directed graphs} \bGr; the latter are sets $\cG$ of ``vertices'' equipped with a set $\cG(A,B)$ of ``edges'' for each $x,y\in \cG$.
Thus, we hope to generate the free category $F_{\bCat}\cG$ on a directed graph \cG type-theoretically.

Our judgments $A\types B$ will still represent morphisms from $A$ to $B$, but now of course there can be more than one such morphism.
Thus, to specify a particular morphism, we need more information than the simple \emph{derivability} of a judgment $A\types B$.
Na\"ively, the first thing we might try is to identify this extra information with the \emph{derivation} of such a judgment, i.e.\ with the tree of rules that were applied to reach it.
This makes the most sense if we take the approach of \cref{thm:initiality-posets-2} rather than \cref{thm:initiality-posets-1}, so that distinct edges $f,g\in \cG(A,B)$ can be regarded as distinct \emph{rules}
\begin{mathpar}
  \inferrule*[right=$f$]{ }{A\types B} \and
  \inferrule*[right=$g$]{ }{A\types B} \and
\end{mathpar}
Thus, for instance, if we have also $h\in \cG(B,C)$, the distinct composites $h\circ g$ and $h\circ f$ will be represented by the distinct derivations
\begin{mathpar}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{B\types C} \\
    \inferrule*[right=$g$]{ }{A\types B}
  }{
    A\types C
  }\and
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{B\types C} \\
    \inferrule*[right=$f$]{ }{A\types B}
  }{
    A\types C
  }\and
\end{mathpar}
Note that when we have distinct rules with the same premises and conclusion, we have to label them so that we can tell which is being applied.
For consistency, we begin labeling the identity and composition rules too, with $\circ$ and $\idfunc$.

Of course, this na\"ive approach founders on the fact that composition in a category is supposed to be associative and unital, since the two composites $h\circ (g\circ f)$ and $(h\circ g)\circ f$, which ought to be equal, nevertheless correspond to distinct derivations:
\begin{equation}\label{eq:assoc}
  \begin{array}{c}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{C\types D} \\
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$g$]{ }{B\types C} \\
      \inferrule*[right=$f$]{ }{A\types B}
    }{
      A\types C
    }}{
    A\types D
  }\\\\
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$h$]{ }{C\types D} \\
      \inferrule*[right=$g$]{ }{B\types C}
    }{
      B\types D
    }\\
    \inferrule*[right=$f$]{ }{A\types B}
  }{
    A\types D
  }
  \end{array}
\end{equation}
Thus, with this type theory we don't get the free category on \cG, but rather some free category-like structure that lacks associativity and unitality.

There are two ways to deal with this problem.
The first is to simply quotient by an equivalence relation.
Our equivalence relation will have to identify the two derivations in~\eqref{eq:assoc}, and also the similar pairs for identities:
\begin{mathpar}
  \inferrule{\inferrule*[right=$\idfunc$]{ }{A\types A}\\ {A\types B}}{A\types B}\circ
  \qquad\equiv\qquad A\types B
  \\
  \inferrule{A\types B \\ \inferrule*[right=$\idfunc$]{ }{B\types B}}{A\types B}\circ
  \qquad\equiv\qquad A\types B
\end{mathpar}
Our equivalence relation must also be a ``congruence for the tree-construction of derivations'', meaning that these identifications can be made anywhere in the middle of a long derivation, such as:
\begin{mathpar}
  \inferrule{\inferrule*{}{\sD_1\\\\\vdots} \\
    \inferrule*[right=$\circ$]{\inferrule*[right=$\idfunc$]{ }{A\types A}\\ \inferrule*{\sD_2\\\\\vdots}{A\types B}}{A\types B}
  }{\vdots\\\\\sD_3}
  \qquad\equiv\qquad
  \inferrule{\inferrule*{}{\sD_1\\\\\vdots} \\
    \inferrule*{\sD_2\\\\\vdots}{A\types B}
  }{\vdots\\\\\sD_3}
\end{mathpar}
We will also have to close it up under reflexivity, symmetry, and transitivity to make an equivalence relation.

Of course, it quickly becomes tedious to draw such derivations, so it is convenient to adopt a more succinct syntax for them.
We begin by labeling each judgment with a one-dimensional syntactic representation of its derivation tree, such as:
\begin{mathpar}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$g$]{ }{g:(B\types C)} \\
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$\idfunc$]{ }{\idfunc_B:(B\types B)} \\
      \inferrule*[right=$f$]{ }{f:(A\types B)}
    }{
      (\idfunc_B\circ f):(A\types B)
    }}{
    (g\circ (\idfunc_B\circ f)) : (A\types C)
  }
\end{mathpar}
These labels are called \emph{terms}.
Of course, in this case they are none other than the usual notation for composition and identities.
Now the generators of our equivalence relation look even more familiar:
\begin{align*}
  \phi \circ (\psi \circ \chi) &\equiv (\phi\circ\psi)\circ\chi\\
  \phi \circ \idfunc_A &\equiv \phi\\
  \idfunc_B \circ \phi &\equiv \phi
\end{align*}
Here $\phi,\psi,\chi$ denote \emph{arbitrary terms}, corresponding to the fact that arbitrary derivations can appear at the top of our identified trees; and similarly these identifications can also happen anywhere inside another term, so that for instance
\[ k\circ (h\circ (g\circ f)) \equiv k\circ ((h\circ g)\circ f).  \]
Of course, we only impose these relations when they make sense.
We can describe the conditions under which this happens using rules for a secondary judgment $\phi\equiv \psi : (A\types B)$, like so:
\begin{mathpar}
  \inferrule{\phi:(A\types B) \\ \psi:(B\types C) \\ \chi:(C\types D)}{(\phi \circ (\psi \circ \chi) \equiv (\phi\circ\psi)\circ\chi) : (A\types D)}\\
  \inferrule{\phi:(A\types B)}{(\phi \circ \idfunc_A \equiv \phi):(A\types B)}\and
  \inferrule{\phi:(A\types B)}{(\idfunc_B \circ \phi \equiv \phi):(A\types B)}
\end{mathpar}
When the rules for $\circ$ and $\idfunc$ are augmented by these rules generating a congruence $\equiv$, and we add axioms for the edges of a given directed graph \cG, we call the result the \textbf{cut-ful type theory of categories under \cG}.
It should hopefully be clear that this does work:

\begin{thm}\label{thm:initiality-categories-1}
  The free category on a directed graph $\cG$ has the same objects as \cG, and its morphisms are the terms such that $\phi :(A\types B)$ is derivable in the cut-ful type theory of categories under \cG, modulo the equivalence relation $\phi\equiv \psi:(A\types B)$.
\end{thm}

Of course, once again very little seems to be happening; we are just using a complicated funny syntax to build a free algebraic structure.
Therefore, it is the second way to deal with the problem of associativity that is more interesting.
In this case what we do is \emph{remove the composition rule $\circ$ entirely}; instead we ``build (post)composition into the axioms''.
That is, the only rule independent of \cG is identities:
\[ \inferrule{ }{A\types A}\,\idfunc \]
while for every edge $f\in \cG(A,B)$ we take the following rule:
\[ \inferrule{X\types A}{X\types B} \,f \]
for any $X$.
Informally, one might say that we represent $f$ by its ``image under the Yoneda embedding''.
We will call this the \textbf{cut-free type theory of categories under \cG}.

In this theory, if we have $f\in\cG(A,B)$, $g\in\cG(B,C)$, and $h\in \cG(C,D)$ there is \emph{only one way} to derive $A\types D$:
\begin{mathpar}
  \inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{A\types A}
      }{
        A\types B
      }
    }{
      A\types C
    }
  }{
    A\types D
  }
\end{mathpar}
Thus, we no longer have to worry about distinguishing between $h\circ (g\circ f)$ and $(h\circ g)\circ f$.
Of course, we have a new problem: if we are trying to build a category, then we \emph{do} need to be able to compose arrows!
So we need the following theorem:

\begin{thm}\label{thm:cutadm-categories}
  If we have derivations of $A\types B$ and $B\types C$ in the cut-free type theory of categories under \cG, then we can construct a derivation of $A\types C$.
\end{thm}
\begin{proof}
  We induct on the number of rules appearing in the given derivation of $B\types C$.

  If there is only one rule, then that rule must be $\idfunc$, since all the other rules have a premise and would need a second rule to feed into them.
  In this case, it be that $B=C$; so our given derivation of $A\types B$ is also a derivation of $A\types C$.

  If there are $n+1$ rules, then we must have $f\in\cG(D,C)$ and our derivation of $B\types C$ ends like this:
  \begin{mathpar}
    \inferrule*[right=$f$]{\inferrule*{\sD\\\\\vdots}{B\types D}}{B\types C}
  \end{mathpar}
  In particular, it contains a derivation \sD of $B\types D$ with only $n$ rules.
  Thus, by the inductive hypothesis we have a derivation, say $\sD'$, of $A\types D$.
  Now we can simply follow this with the rule for $f$:
  \begin{mathpar}
    \inferrule*[right=$f$]{\inferrule*{\sD'\\\\\vdots}{A\types D}}{A\types C}
  \end{mathpar}
\end{proof}

In type-theoretic lingo, \cref{thm:cutadm-categories} says that \textbf{the cut rule is admissible} in the cut-free type theory of categories under \cG.
In other words, although the cut/composition rule
\begin{mathpar}
  \inferrule*[right=$\circ$]{A\types B \\ B\types C}{A\types C}
\end{mathpar}
is not \emph{part of the type theory} as defined, it is nevertheless true that whenever we have derivations of the premises of this rule, we can construct a derivation of its conclusion.

Of course, another way to say what is going on is that the morphisms in the free category on a directed graph \cG have an explicit description as \emph{finite strings of composable edges} in \cG.
We have just given an inductive definition of ``finite string of composable edges'': we have such a finite string (of length 0) from $A$ to $A$, and if we have such a string from $X$ to $A$ and an edge $f\in\cG(A,B)$, we can construct a string from $X$ to $B$.
Thus we have the analogous theorem:

\begin{thm}\label{thm:initiality-categories-2}
  The free category on a directed graph $\cG$ has the same objects as \cG, and its morphisms are the derivations of $A\types B$ in the cut-free type theory for categories under \cG.
\end{thm}

This gives us a second, more interesting, general statement about categorical logic: it is
{a syntax for generating free categorical structures using derivations from rules}
\emph{that yield elements in canonical form}, eliminating (or reducing) the need for quotients.

To end this section, we note that we can introduce \emph{terms} for the cut-free type theory as well in a pleasing way.
Rather than describing the entire judgment $A\types B$ with a term, we assign a formal variable such as $x$ to the domain $A$, and then a term containing $x$ to the codomain $B$, such as $h(g(f(x)))$.
We write this as
\[ x:A \types h(g(f(x))) : B\]
The identity rules and axiom rules can now be written as
\begin{mathpar}
  \inferrule{ }{x:A\types x:A}\,\idfunc \and
  \inferrule{x:X\types \alpha:A}{x:X\types f(\alpha):B} \,f
\end{mathpar}
Here $\alpha$ denotes an arbitrary term.
Thus, for instance, the composite of $h$, $g$, and $f$ would be written like so:
\begin{mathpar}
  \inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{x:A\types x:A}
      }{
        x:A\types f(x):B
      }
    }{
      x:A\types g(f(x)): C
    }
  }{
    x:A\types h(g(f(x))):D
  }
\end{mathpar}
If we trace through the proof of \cref{thm:cutadm-categories} in this notation, it says that if we have $x:A\types \beta:B$ and $y:B\types \gamma:C$, we can \emph{substitute} $\beta$ for $y$ in $\gamma$ to obtain a new judgment $x:A \types \gamma[\beta/y]:C$.
For example, given $x:A \types g(f(x)):B$ and $y:B\types h(y):C$, the derivation constructed is $x:A\types h(g(f(x))):C$, in which the $y$ in $h(y)$ has been replaced by $g(f(x))$.
Thus, \cref{thm:cutadm-categories} can be rephrased to say that \textbf{substitution is admissible} in the cut-free type theory of categories under \cG.

\begin{ex}
  Prove \cref{thm:initiality-categories-2}.
  You will have to start by proving that the composition defined in \cref{thm:cutadm-categories} is in fact associative and unital, so that it forms a category.
\end{ex}


\bibliographystyle{alpha}
\bibliography{all}

\end{document}
