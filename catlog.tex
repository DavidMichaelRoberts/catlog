\documentclass{book}
\usepackage{mathpartir,cancel,cmll,appendix}
\newif\ifcref\creftrue
\input{decls}
\title{Categorical logic from a categorical point of view}
\author{Michael Shulman}
\date{\today}
\makeatletter
\autodefs{\bSet\bMag\bVect\bPoset\bRelGr\bCat\bGr\bmSLat\ftype\bPrCat\bCoprCat\bMCat\bMGr\bMPos\bRelMGr\bMonPos\bMonCat\bSMPos\bSMC\bProp}
\def\tr{\mathrm{tr}}
\let\sect\section
\def\idfunc{\mathsf{id}}
\def\finsubset{\subset_{\mathrm{fin}}}
% well-founded trees and algebras
\def\ay{\mathrm{ar}}
\def\sig{\Sigma}
\def\axes{\Lambda}
\def\act#1{[#1]}
\def\equivsym{\mathord{\equiv}}
% judgments
\let\types\vdash
\def\type{\;\ftype}
\newcommand{\pc}{\mathrel{\mathord{:}?}}
\newcommand{\atom}{\mathrel{\downarrow}}
\newcommand{\can}{\mathrel{\uparrow}}
\newcommand{\atomcan}{\ensuremath{\mathord{\downarrow\uparrow}}}
\newcommand{\focus}[1]{[#1]}
\newcommand{\postcsym}[1]{#1\mathord{\circ}}
\newcommand{\postc}[2]{\postcsym{#1}(#2)}
\newcommand{\precsym}[1]{\mathord{\circ}#1}
\renewcommand{\prec}[2]{(#1)\precsym{#2}}
% substitution
\newcommand{\hsub}[1]{\llbracket #1\rrbracket}
\newcommand{\hid}[1]{\mathbbb{id}_{#1}}
% free functors
\newcommand{\F}[1]{\mathfrak{F}_{#1}}
% meet
\let\meet\wedge
\def\meetL{\mathord{\meet}L}
\def\meetR{\mathord{\meet}R}
\def\meetE{\mathord{\meet}E}
\def\meetI{\mathord{\meet}I}
% join
\let\join\vee
\def\joinL{\mathord{\join}L}
\def\joinR{\mathord{\join}R}
\def\joinE{\mathord{\join}E}
\def\joinI{\mathord{\join}I}
% unit
\def\unit{\mathbf{1}}
\def\ttt{\mathsf{tt}}
% product
\def\timesE{\ensuremath{\mathord{\times}E}}
\def\timesI{\ensuremath{\mathord{\times}I}}
\def\timesR{\ensuremath{\mathord{\times}R}}
\def\timesL{\ensuremath{\mathord{\times}L}}
\def\pair#1#2{\langle #1,#2\rangle}
% coproduct
\def\plusE{\mathord{+}E}
\def\plusI{\mathord{+}I}
\def\inl{\mathsf{inl}}
\def\inr{\mathsf{inr}}
\def\case{\mathsf{match}_+}
% general positives
\def\match{\mathsf{match}}
% empty
\def\zero{\mathbf{0}}
\def\zeroE{\mathbf{0}E}
%\def\abort{\mathsf{abort}}
\def\abort{\match_{\zero}}
% one
\def\one{\mathbf{1}}
\def\discard#1in{\mathsf{discard}\; #1 \; \mathsf{in} \;}
% tensor
\let\tensor\otimes
\def\tensorL{\mathord{\tensor}L}
\def\tensorR{\mathord{\tensor}R}
\def\tensorI{\mathord{\tensor}I}
\def\tensorE{\mathord{\tensor}E}
\let\bigtensor\bigotimes
\def\flet#1:{\mathsf{let}\;#1 \@ifnextchar:\@fletdoublecolon\@fletsinglecolon}
\def\fletp#1:{\mathsf{let}'\;#1 \@ifnextchar:\@fletdoublecolon\@fletsinglecolon}
\def\@fletdoublecolon:=#1in{\Coloneqq #1\;\mathsf{in}\;}
\def\@fletsinglecolon=#1in{\coloneqq #1\;\mathsf{in}\;}
\makeatother
\begin{document}
\maketitle
\setcounter{tocdepth}{1}
\tableofcontents

\setcounter{chapter}{-1}
\chapter{Introduction}
\label{chap:intro}


\section{Appetizer: inverses in group objects}
\label{sec:intro}

In this section we consider an extended example.
We do not expect the reader to understand it very deeply, but we hope it will give some motivation for what follows, as well as a taste of the power and flexibility of categorical logic as a tool for category theory.

Our example will consist of several varations on the following theorem:

\begin{thm}
  If a monoid has inverses (hence is a group), then those inverses are unique.
\end{thm}

When ``monoid'' and ``group'' have their usual meaning, namely sets equipped with structure, the proof is easy.
For any $x$, if $i(x)$ and $j(x)$ are both two-sided inverse of $x$, then we have
\[ i(x) = i(x) \cdot e = i(x) \cdot (x \cdot j(x)) = (i(x)\cdot x)\cdot j(x) = e\cdot j(x) = j(x) \]
However, the theorem is true much more generally than this.
We consider first the case of monoid/group objects in a category with products.
A \emph{monoid object} is an object $M$ together with maps $m:M\times M \to M$ and $e:1\to M$ satisfying associativity and unitality axioms:
\begin{equation}
  \vcenter{\xymatrix{
      M\times M\times M\ar[r]^-{1\times m}\ar[d]_{m\times 1} &
      M\times M\ar[d]^m\\
      M\times M\ar[r]_m &
      M
    }}
  \qquad
  \vcenter{\xymatrix{ M \ar[r]^-{(1,e)} \ar[dr]_{1} &
    M\times M \ar[d]_m & M \ar[l]_-{(e,1)} \ar[dl]^{1} \\
    & M }}
\end{equation}
An \emph{inverse operator} for a monoid object is a map $i:M\to M$ such that
\begin{equation}
  \vcenter{\xymatrix@C=1pc{& M\times M \ar[rr]^{i\times 1} && M\times M \ar[dr]^m \\
      M \ar[ur]^{\Delta} \ar[dr]_{\Delta} \ar[rr]^{!} && 1 \ar[rr]^{e} && M \\
      & M\times M \ar[rr]_{1\times i} && M\times M \ar[ur]_{m}}}
\end{equation}
The internalized claim, then, is that any two inverse operators are equal.
The \emph{internal logic of a category with products} allows us to prove this by using essentially the same argument that we did in the case of ordinary monoids in \bSet.
The morphisms $m$ and $e$ are represented in this logic by the notations
\begin{mathpar}
  x:M,y:M \types x\cdot y :M \and
  \types e:M.
\end{mathpar}
Don't worry if this notation doesn't make a whole lot of sense yet.
The symbol $\types$ (called a ``turnstile'') is the logic version of a morphism arrow $\to$, and the entire notation is called a \emph{sequent} or a \emph{judgment}.
The fact that $m$ is a morphism $M\times M \to M$ is indicated by the fact that $M$ appears twice to the left of $\types$ and once to the right; the comma ``$,$'' in between $x:M$ and $y:M$ represents the product $\times$, and the variables $x,y$ are there so that we have a good notation ``$x\cdot y$'' for the morphism $m$.
In particular, the notation $x:M,y:M \types x\cdot y :M$ should be bracketed as
\[ ((x:M),(y:M)) \types ((x\cdot y) :M). \]
Similarly, the associativity, unit, and inverse axioms are indicated by the notations
\begin{mathpar}
  x:M,y:M,z:M \types (x\cdot y)\cdot z = x\cdot (y\cdot z) : M \\
  x:M \types x\cdot e = x : M \and
  x:M \types e\cdot x = x : M \\F
  x:M \types x\cdot i(x) = e : M \and
  x:M \types i(x) \cdot x = e : M
\end{mathpar}
Now the \bSet-based proof can be essentially copied in this notation:
\[ x:M \types i(x) = i(x) \cdot e = i(x) \cdot (x \cdot j(x)) = (i(x)\cdot x)\cdot j(x) = e\cdot j(x) = j(x) : M.\]
The essential point is that the notation \emph{looks set-theoretic}, with ``variables'' representing ``elements'', and yet (as we will see) its formal structure is such that it can be interpreted into \emph{any} category with products.
Therefore, writing the proof in this way yields automatically a proof of the general theorem that any two inverse \emph{operators} for a monoid \emph{object} in a category with products are equal.

To be sure, such a proof could be written in standard categorical style using commutative diagrams as well (\cref{ex:fp-inv-uniq}), and experienced category theorists become quite good at translating proofs in this way.
The point of categorical logic is that this sort of work is \emph{so} straightforward that it's actually completely unnecessary to do at all: we can prove a ``meta-theorem'' that does all the work for us.

Before leaving this appetizer section, we mention some further generalizations of this result.
While type theory allows us to use set-like notation to prove facts about any category with finite products, the allowable notation is fairly limited, essentially restricting us to algebraic calculations with variables.
However, if our category has more structure, then we can ``internalize'' more set-theoretic arguments.

As an example, note that for ordinary monoids in sets the uniqueness of inverses can be expressed ``pointwise'' rather than in terms of inverse-assigning operators.
In other words, for each element $x\in M$, if $x$ has two two-sided inverses $y$ and $z$, then $y=z$.
If we think hard enough, we can express this diagrammatically in terms of the category \bSet is to consider the following two sets:
\begin{align*}
  A &= \setof{(x,y,z)\in M^3 | xy=e, yx=e, xz=e, zx=e}\\
  B &= \setof{(y,z)\in M^2 | y=z}
\end{align*}
In other words, $A$ is the set of elements $x$ equipped with two inverses, and $B$ is the set of pairs of equal elements.
Then the uniqueness of pointwise inverses can be expressed by saying there is a commutative diagram
\[ \xymatrix{ A \ar[d] \ar[r] & B \ar[d] \\ M^3 \ar[r]_{\pi_{23}} & M^2 } \]
where the vertical arrows are inclusions and the lower horizontal arrow projects to the second and third components.

This is a statement that makes sense for a monoid object $M$ in any category with finite \emph{limits}.
The object $B$ can be constructed categorically as the equalizer of the two projections $M\times M \toto M$ (which is in fact isomorphic to $M$ itself), while the object $A$ is a ``joint equalizer'' of four parallel pairs, one of which is
\[ \vcenter{\xymatrix{ & M \times M \ar[dr]^m \\
    M\times M\times M \ar[ur]^{\pi_{12}} \ar[dr]_{!} && M \\
    & 1 \ar[ur]_e }} \]
and the others are similar.
We can then try to \emph{prove}, in this generality, that there is a commutative square as above.
We can do this by manipulating arrows, or by appealing to the Yoneda lemma, but we can also use the \emph{internal logic of a category with finite limits}.
This is a syntax like the internal logic for categories with finite products, but which also allows us to \emph{hypothesize equalities}.
The judgment in question is
\begin{equation}\label{eq:pointwise-unique-inverses}
  x:M, y:M, z:M, x\cdot y = e, y\cdot x=e, x\cdot z = e, z\cdot x = e \types y=z.
\end{equation}
As before, the comma binds the most loosely, so this should be read as
\[ ((x:M), (y:M), (z:M), (x\cdot y = e), (y\cdot x=e), (x\cdot z = e), (z\cdot x = e)) \types (y=z). \]
We can prove this by set-like equational reasoning, essentially just as before.
The ``interpretation machine'' then produces from this a morphism $A\to B$, for the objects $A$ and $B$ constructed above.

Next, note that in the category \bSet, the uniqueness of inverses ensures that if every element $x\in M$ has an inverse, then there is a \emph{function} $i:M\to M$ assigning inverses --- even without using the axiom of choice.
(If we define functions as sets of ordered pairs, as is usual in set-theoretic foundations, we could take $i = \setof{(x,y) | xy=e}$; the pointwise uniqueness ensures that this is indeed a function.)
This fact can be expressed in the internal logic of an \emph{elementary topos}.
We postpone the definition of a topos until later; for now we just remark that its structure allows both sides of the turnstile $\types$ to contain \emph{logical formulas} such as $\exists x, \forall y, \phi(x,y)$ rather than just elements and equalities.
In this language we can state and prove the following:
\[ \forall x:M, \exists y:M, x\cdot y = e \land y\cdot x = e \types
\exists i:M^M, \forall x:M, (x\cdot i(x) = e \land i(x)\cdot x = e)
\]
As before, the proof is essentially exactly like the usual set-theoretic one.
Moreover, the interpretation machine allows us to actually extract an ``inverse operator'' morphism in the topos from this proof.
As before, such a result can also be stated and proved using arrows and commutative diagrams, but as the theorems get more complicated, the translation gets more tedious to do by hand, and the advantage of type-theoretic notation becomes greater.

So much for adding extra structure.
In fact, we can also take structure away!
A monoid object can be defined internal to any \emph{monoidal} category, not just a cartesian monoidal one; now the structure maps are $m:M\otimes M\to M$ and $e:I\to M$, and the commutative diagrams are essentially the same.

To define an inverse operator in this case, however, we need some sort of ``diagonal'' $\Delta:M\to M\otimes M$ and also a ``projection'' or ``augmentation'' $\varepsilon:M\to I$.
The most natural hypothesis is that these maps make $M$ into a \emph{comonoid} object, i.e.\ a monoid in the opposite monoidal category, and that the monoid and comonoid structures preserve each other; this is the notion of a \emph{bimonoid} (or ``bialgebra'').
(\cref{ex:cartmon-bimon-uniq}: in a cartesian monoidal category, every object is a bimonoid in a unique way.)

Now given a bimonoid $M$, we can define an ``inverse operator'' --- which in this context is usually called an \emph{antipode} --- to be a map $i:M\to M$ such that
\begin{equation}
  \vcenter{\xymatrix@C=1pc{& M\otimes M \ar[rr]^{i\otimes 1} && M\otimes M \ar[dr]^m \\
      M \ar[ur]^{\Delta} \ar[dr]_{\Delta} \ar[rr]^{\varepsilon} && I \ar[rr]^{e} && M \\
      & M\otimes M \ar[rr]_{1\otimes i} && M\otimes M \ar[ur]_{m}}}
\end{equation}
commutes, where now $\Delta$ and $\varepsilon$ are the comonoid structure of $M$ rather than the diagonal and projection of a cartesian product.
A bimonoid equipped with an antipode is called a \emph{Hopf monoid} (or ``Hopf algebra'').
The obvious question then is, if a bimonoid has two antipodes, are they equal?

In some cases it is possible to apply the previous results directly.
For instance, the category of \emph{(co)commutative} comonoids in a symmetric monoidal category inherits a monoidal structure that turns out to be \emph{cartesian} (\cref{ex:ccmon-cart}), so a cocommutative bimonoid is actually a monoid in a cartesian monoidal category, and we can apply the first version of our result.
Similarly, the category of commutative monoids is cocartesian, so a commutative bimonoid is a comonoid in a cocartesian monoidal category, so we can apply the dual of the first version of our result.
But what if neither the multiplication nor the comultiplication is commutative?

Internal logic is up to this task.
In a monoidal category we can consider judgments with multiple outputs as well as multiple inputs.\footnote{For the benefit of readers who are already experts, I should mention that this is \emph{not} ordinary ``classical linear logic'': the comma represents the same monoidal structure $\tensor$ on both sides of the turnstile, rather than $\tensor$ on the left and $\parr$ on the right.}
This allows us to describe monoids and comonoids in a roughly ``dual'' way.
Don't worry about the precise syntax being used on the right; it will be explained in \cref{sec:prop-smc}.
\begin{alignat*}{2}
  x:M, y:M &\types x\cdot y:M &\qquad x:M &\types (x_1,x_2):(M,M)\\
  &\types e:M &\qquad x:M &\types (\mid\cancel{x}):()\\
  x:M,y:M,z:M &\types (x\cdot y)\cdot z = x\cdot (y\cdot z) :M &\qquad x:M &\types (x_{11},x_{12},x_2)=(x_1,x_{21},x_{22}):(M,M,M)\\
  x:M &\types x\cdot e=x:M &\qquad x:M &\types (x_1\mid\cancel{x_2}) = x:M\\
  x:M &\types e\cdot x=x:M &\qquad x:M &\types (x_2\mid\cancel{x_1}) = x:M
\end{alignat*}
In this language, the bimonoid axioms are
\begin{align*}
  x:M,y:M &\types (x_1\cdot y_1,x_2\cdot y_2) = ((x\cdot y)_1,(x\cdot y)_2) :(M,M)\\
          &\types (e_1,e_2)=(e,e):(M,M)\\
  x:M,y:M &\types (\mid\cancel{x\cdot y}) = (\mid\cancel{x},\cancel{y}) : ()\\
  &\types (\mid\cancel{e})=():()
\end{align*}
And an antipode is a map $x:M \types i(x):M$ such that
\begin{align*}
  x:M &\types x_1\cdot i(x_2) = (e\mid\cancel{x}) :M\\
  x:M &\types i(x_1)\cdot x_2 = (e\mid\cancel{x}) :M
\end{align*}
Now if we have another antipode $j$, we can compute
\begin{align*}
  x:M \types i(x)
  &= i(x)\cdot e\\
  &= (i(x_1)\cdot e\mid\cancel{x_2})\\
  &= i(x_1)\cdot (x_{21} \cdot j(x_{22}))\\
  &= (i(x_1)\cdot x_{21}) \cdot j(x_{22})\\
  &= (e \cdot j(x_{2})\mid\cancel{x_1})\\
  &= e\cdot j(x)\\
  &= j(x) \qquad :M
\end{align*}
yielding the same result $i=j$.
So even in a non-cartesian situation, we can use a very similar set-like argument, as long as we keep track of where elements get ``duplicated and discarded''.

This concludes our ``appetizer''; I hope it has given you a taste of what categorical logic looks like, and what it can do for category theory.
In the next section we will rewind back to the beginning and start with very simple cases.

\subsection*{Exercises}

\begin{ex}\label{ex:fp-inv-uniq}
  Prove, using arrows and commutative diagrams, that any two inverse operators for a monoid object in a category with finite products are equal.
\end{ex}

\begin{ex}\label{ex:cartmon-bimon-uniq}
  Prove that in a cartesian monoidal category, every object is a bimonoid in a unique way.
\end{ex}

\begin{ex}\label{ex:ccmon-cart}
  Show that the category of cocommutative comonoids in a symmetric monoidal category inherits a monoidal structure, and that this monoidal structure is cartesian.
\end{ex}

\begin{ex}\label{ex:antipode}
  Prove, using arrows and commutative diagrams, that any two antipodes for a bimonoid (not necessarily commutative or cocommutative) are equal.
\end{ex}


\section{On type theory and category theory}
\label{sec:generalities}

Since there are many other introductions to categorical logic (a non-exhaustive list could include~\cite{mr:focl,ls:hocl,jacobs:cltt,goldblatt:topoi,ptj:elephant}), it seems appropriate to say a few words about what distinguishes this one.
These words may not make very much sense to the beginner who doesn't yet know what we are talking about, but it may help to orient the expert, and as the beginner becomes more expert he or she can return to it later on.

Our perspective is very much that of the category theorist: our primary goal is to use type theory as a convenient syntax to prove things about categories.
The way that it does this is by giving concrete presentations of \emph{free} categorical structures, so that by working in those presentations we can deduce conclusions about \emph{any} such structure.
There are other such syntaxes for category theory, notably string diagram calculi, that function in a similar way (giving a concrete presentation of free structures) to the extent that they are made precise.
Indeed, the \emph{usual} way of reasoning in category theory, in which we speak explicitly about objects, arrows, commutative diagrams, and so on, can also be interpreted, from this point of view, to be simply making use of the \emph{obvious} presentation of a free structure rather than some fancier one.

In particular, this means that we are not interested in aspects of type theory such as computability, canonicity, proof search, cut-elimination, focusing, and so on \emph{for their own sake}.
However, at the same time we recognize their importance for type theory as a subject in its own right, which suggests that they should not be ignored by the category theorist.
If nothing else, the category theorist will encounter these words when speaking to type theorists, and so it is advantageous to have at least a passing familiarity with them.

In fact, our perspective is that it is precisely the esoteric-sounding notion of \emph{cut elimination} (or \emph{admissibility of substitution}) that essentially \emph{defines} what we mean by a \emph{type theory}.
Of course this is not literally true; a more careful statement would be that type theories with cut elimination are those that exhibit the most behavior most characteristic of type theories.
(Jean-Yves Girard remarked that ``a logic without cut-elimination is like a car without an engine.'')
A ``type theory without cut elimination'' may still yield explicit presentations of free structures, but reasoning with such a presentation will not yield the characteristic benefits of categorical logic.

So what is this mysterious cut-elimination, from a categorical perspective?
Informally, it says that the morphisms in a free categorical structure can be presented \emph{without explicit reference to composition}.
This is a bit of a cheat, because as we will see, in fact what we do is to build just enough ``implicit'' reference to composition into our rules to ensure that we no longer need to talk about composition explicitly.
However, this does not make the process trivial, and it can still yield valuable results.

(As a simple example of nontriviality, if an arrow is constructed by applying a universal property, then that property automatically determines some of the composites of that arrow.
For instance, a pairing $\pair{f}{g}:X\to A\times B$ must compose with the projections $\pi_1:A\times B\to A$ and $\pi_2:A\times B\to B$ to give $f$ and $g$ respectively.
Thus, composites of this sort do not need to be ``built in'' by hand; this provides an interesting point of view on the value of universal properties.)

Foremost among the categorical applications of ensuring cut-elimination is that it simplifies the presentation of free structures by isolating a class of \emph{canonical forms}, reducing the need for equivalence relations.
For instance, when composition is obtained by cut-elimination, we can present the free category on a directed graph without needing to impose any equivalence relation.
There are many other simple cases where we can do without any equivalence relation at all; in more complicated situations we can still make do with a much simplified one.

These presentations of free structures are often presented as \emph{term calculi} that exhibit another characteristic advantage of categorical logic: we can use ``set-like'' reasoning to prove things about arbitrary categories.
From this perspective, the admissibility of substitution (which is another name for cut-elimination) says that \emph{the meaning of a notation can be evaluated simply on the basis of the notation as written, without having to guess at the thought processes of the person who wrote it down}.
This is obviously a desirable feature, and arguably even a necessary one if our ``notation'' is to be worthy of the name.

Another unusual feature of our treatment is the emphasis on multicategories (of various generalized sorts, including also the still more general ``polycategories'' and their generalizations).
Although multicategories have been present in categorical logic from close to the beginnings of both (Lambek's original definition of multicategory~\cite{lambek:dedsys-ii} was motivated by logical considerations), they are rarely mentioned in introductions to the subject.
One concrete advantage of using multicategories is a more direct correspondence between the type theory and the category theory: type theory distinguishes between a sequent $A,B\types C$ and a sequent $A\times B\types C$ (even though they are bijectively related), so it seems natural to work with a categorical structure that also distinguishes between morphisms $(A,B)\to C$ and $A\times B\to C$.

However, the correspondence and motivation goes deeper than that.
We may ask \emph{why} type theory distinguishes these two kinds of sequents?
We will discuss this in more detail in \cref{sec:why-multicats}, but the short answer is that ``it makes cut-elimination work''.
More specifically, it enables us to formulate type theory in such a way that \emph{each rule refers to at most one type former}; this enables us to ``commute these rules past each other'' in the proof of cut-elimination.
Moreover, including sequents such as $A,B\types C$ allows us to describe certain operations in a type-theoretic style that would not otherwise be possible, such as a monoidal tensor product.
A type theorist speaks of this in terms of \emph{deciding on the judgmental structure first} (including ``structural rules'') and then defining the connectives to ``internalize'' various aspects of that structure.

From a categorical point of view, the move to (generalized) multicategories has the feature that \emph{it gives things universal properties}.
For instance, the tensor product in a monoidal category has no universal property, but the tensor product in a multicategory does.
In general, from a well-behaved 2-monad $T$ we can define a notion of ``$T$-multicategory''~\cite{burroni:t-cats,leinster:higher-opds,hermida:coh-univ,cs:multicats} in which $T$-algebra structure acquires a universal property (specifically, $T$ is replaced by a lax- or colax-idempotent 2-monad with the same algebras).
In type theoretic language, the move to $T$-multicategories corresponds to including the desired operations in the judgmental structure.
The fact that the $T$-operations then have universal properties is what enables us to write down the usual sort of type-theoretic left/right or introduction/elimination rules for them.

Making this correspondence explicit is helpful for many reasons.
Pedagogically, it can help the category theorist, who believes in universal properties, to understand why type theories are formulated the way they are.
It also makes the ``initiality theorems'' more modular: first we model the judgmental structure with a multicategory, and then we add more type formers corresponding to objects with various universal properties.
Finally, it provides a guide for new applications of categorical logic: when seeking a categorical structure to model a given type theory, we should look for a kind of multicategory corresponding to its judgments; while when seeking an internal logic for a categorical structure, we should represent it using universal properties in some kind of multicategory, from which we can extract an appropriate judgmental structure.

These facts about cut-elimination and multicategories have surely been known in some form to experts for a long time, but I am not aware of a clear presentation of them for the beginner coming from a category-theoretic background.\footnote{One reference that seems more related than it is is~\cite{dosen:cutelim-cats}.
  Like us, Do\v{s}en also emphasizes how cut-elimination presents free categorical structures without explicit composition.
  But his goal is to ``explain'' or ``justify'' categorical notions using cut-elimination, whereas our goal is rather the reverse; and
%  He also considers only sequent calculus, whereas we attempt to be even-handed between sequent calculus and natural deduction (although the latter is what yields set-like term calculi for internal reasoning, so we are somewhat biased towards it).
  says hardly anything about multicategories, which are central to our approach.
  % , appear in \textit{ibid.}\ only briefly in the third-to-last paragraph of the book.
  [TODO: Read Do\v{s}en and Petri\'{c}, \textit{Proof-Theoretical Coherence}, \url{http://www.mi.sanu.ac.rs/~kosta/publications.htm}]}
They are not strictly necessary if one wants simply to use type theory for internal reasoning about categories, and there are plenty of excellent introductions that take a geodesic route to that application.
However, we believe that they yield a deeper understanding of the type/category correspondence; and they are especially valuable when it comes to designing type theories that correspond to new categorical structures (or vice versa).

We will not assume that the reader has any acquaintance with type theory, or any interest in it apart from its uses for category theory.
However, because one of our goals is to help the reader become familiar with the lingo and concerns of type theorists, we sometimes include a little more detail than is strictly necessary for categorical applications.
The reader should feel free to skip over these brief digressions.



\chapter{Unary type theories}
\label{chap:unary}

We begin our study of type theories and their categorical counterparts with a class of very simple cases that we will call \emph{unary type theories}.
(This terminology is not standard in the literature.)
On the type-theoretic side the word ``unary'' indicates that there is only one type on each side of a sequent $A\types B$.
On the categorical side it means, roughly, that we deal with categories rather than any kind of multicategory.

In some ways the unary case is fairly trivial, but for that very reason it serves as a good place to become familiar with basic notions of type theory and how they correspond to category theory.
In later chapters we will generalize away from unarity in various ways.


\section{Posets}
\label{sec:poset}

We start with the simplest sort of categories: those in which each hom-set has at most one element.
These are well-known to be equivalent to \emph{preordered sets}, where the existence of an arrow $A\to B$ is regarded as the assertion that $A\le B$.
I will abusively call them \emph{posets}, although traditionally posets (partially ordered sets) also satisfy the antisymmetry axiom (if $A\le B$ and $B\le A$ then $A=B$); from a category-theoretic perspective, antisymmetry means asking a category to be skeletal, which is both unnatural and pointless.
Conveniently, posets also correspond to the simplest version of logic, namely \emph{propositional} logic.

From a category-theoretic perspective, the question we are concerned with is the following.
Suppose we have some objects in a poset, and some ordering relations between them.
For instance, we might have
\begin{mathpar}
  A\le B \and A\le C \and D\le A \and B \le E \and D\le C
\end{mathpar}
Now we ask, given two of these objects --- say, $D$ and $E$ --- is it necessarily the case that $D\le E$?
In other words, is it the case in \emph{any} poset containing objects $A,B,C,D,E$ satisfying the given relations that $D\le E$?
In this example, the answer is yes, because we have $D\le A$ and  $A\le B$ and $B\le E$, so by transitivity $D\le E$.
More generally, we would like a method to answer all possible questions of this sort.

There is an elegant categorical way to do this based on the notion of \emph{free structure}.
Namely, consider the category \bPoset of posets, and also the category \bRelGr of \emph{relational graphs}, by which I mean sets equipped with an arbitrary binary relation.
There is a forgetful functor $U:\bPoset \to \bRelGr$, which has a left adjoint $F$.

Now, the abstract information about ``five objects $A,B,C,D,E$ satisfying five given relations'' can be regarded as an object $\cG$ of \bRelGr, and to give five such objects satisfying those relations in a poset \cP is to give a map $\cG \to U\cP$ in \bRelGr.
By the adjunction, therefore, this is equivalent to giving a map $F\cG \to \cP$ in \bPoset.
Therefore, a given inequality such as $A\le E$ will hold in \emph{all} posets if and only if it holds in the \emph{particular, universal} poset $F\cG$ freely generated by the assumed data.

Thus, to answer all such questions at once, it suffices to give a concrete presentation of the free poset $F\cG$ generated by a relational graph \cG.
In this simple case, it is easy to give an explicit description of $F$: it is the reflexive-transitive closure.
But since soon we will be trying to generalize vastly, we want instead a general method to describe free objects.
From our current perspective, this is the role of type theory.

As noted in \cref{sec:intro}, when we move into type theory we use the symbol $\types$ instead of $\to$ or $\le$.
Type theory is concerned with \emph{(hypothetical) judgments}, which (roughly speaking) are syntactic gizmos of the form ``$\Gamma\types\Delta$'', where $\Gamma$ and $\Delta$ are syntactic gadgets whose specific nature is determined by the specific type theory under consideration (and, thus, by the particular kind of categories we care about).
We call $\Gamma$ the \emph{antecedent} or \emph{context}, and $\Delta$ the \emph{consequent} or \emph{co-context}.
In our simple case of posets, the judgments are simply
\[ A \types B \]
where $A$ and $B$ are objects of our (putative) poset; such a judgment represents the relation $A\le B$.
In general, the categorical view is that a hypothetical judgment represents a sort of \emph{morphism} (or, as we will see later, a sort of \emph{object}) in some sort of categorical structure.

In addition to a class of judgments, a type theory consists of a collection of \emph{rules} by which we can operate on such judgments.
Each rule can be thought of as a partial $n$-ary operation on the set of possible judgments for some $n$ (usually a finite natural number), taking in $n$ judgments (its \emph{premises}) that satisfy some compatibility conditions and producing an output judgment (its \emph{conclusion}).
We generally write a rule in the form
\begin{mathpar}
  \inferrule{\cJ_1 \\ \cJ_2 \\ \cdots \\ \cJ_n}{\cJ}
\end{mathpar}
with the premises above the line and the conclusion below.
A rule with $n=0$ is sometimes called an \emph{axiom}.
The categorical view is that we have a given ``starting'' set of judgments representing some objects and putative morphisms in the ``underlying data'' of a categorical structure, and the closure of this set under application of the rules yields the objects and morphisms in the \emph{free} structure it generates.

We will attempt to make all of this precise in \cref{chap:dedsys}, which the reader is free to consult now.
However, it is probably more illuminating at the moment to bring it back down to earth in our very simple example.
Since the properties distinguishing a poset are reflexivity and transitivity, we have two rules:
\begin{mathpar}
  \inferrule{ }{A\types A} \and
  \inferrule{A\types B \\ B\types C}{A\types C}
\end{mathpar}
in which $A,B,C$ represent arbitrary objects.
In other words, the first says that for any object $A$ we have a $0$-ary rule whose conclusion is $A\types A$, while the second says that for any objects $A,B,C$ we have a $2$-ary rule whose premises are $A\types B$ and $B\types C$ (that is, any two judgments of which the consequent of the first is the antecedent of the second) and whose conclusion is $A\types C$.
We will refer to the pair of these two rules as the \textbf{free type theory of posets}.

Hopefully it makes sense that we can construct the reflexive-transitive closure of a relational graph by expressing its relations in this funny syntax and then closing up under these two rules, since they are exactly reflexivity and transitivity.
Categorically, of course, that means identities and composition.
In type theory the composition/transitivity rule is often called \textbf{cut}, and plays a unique role, as we will see later.

In the example we started from,
\begin{mathpar}
  A\le B \and A\le C \and D\le A \and B \le E \and D\le C
\end{mathpar}
we have the two instances of the transitivity rule
\begin{mathpar}
  \inferrule{D\types A \\ A\types B}{D\types B}\and
  \inferrule{D\types B \\ B\types E}{D\types E}
\end{mathpar}
allowing us to conclude $D\types E$.
When applying multiple rules in sequence to reach a conclusion, it is customary to write them in a ``tree'' structure like so:
\begin{mathpar}
  \inferrule*{\inferrule*{D\types A \\ A\types B}{D\types B} \\ B\types E}{D\types E}
\end{mathpar}
Such a tree is called a \emph{derivation}.
The way to typeset rules and derivations in \LaTeX\ is with the \texttt{mathpartir} package; the above diagram was produced with
\begin{verbatim}
  \inferrule*{
    \inferrule*{D\types A \\ A\types B}{D\types B} \\
    B\types E
  }{
    D\types E
  }
\end{verbatim}
Note that \texttt{mathpartir} has only recently made it into standard distributions of \LaTeX, so if you have an older system you may need to download it manually.

Formally speaking, what we have observed is the following \emph{initiality theorem}.

\begin{thm}\label{thm:poset-initial-1}
  For any relational graph \cG, the free poset $\F{\bPoset}\cG$ that it generates is has the same objects and its morphisms are the judgments that are derivable from \cG in free type theory of posets.
\end{thm}
\begin{proof}
  In the preceding discussion we assumed it as known that the free poset on a relational graph is its reflexive-transitive closure, which makes this theorem more or less obvious.
  However, it is worth also presenting an explicit proof that does not assume this, since same pattern of proof will reappear many times for more complicated type theories where we don't know the answer in advance.

  Thus, let us define $\F{\bPoset}\cG$ as stated in the theorem.
  The reflexivity and transitivity rules imply that $\F{\bPoset}\cG$ is in fact a poset.
  Now suppose $\cA$ is any other poset and $P:\cG\to\cA$ is a map of relational graphs.
  The objects of $\F{\bPoset}\cG$ are the same as those of \cG, so $P$ extends uniquely to a map on underlying sets $\F{\bPoset}\cG\to\cA$.
  Thus it suffices to show that this map is order-preserving, i.e.\ that if $A\types B$ is derivable from \cG in the free type theory of posets, then $P(A)\le P(B)$.

  For this purpose we \emph{induct on the derivation of $A\types B$}.
  There are multiple ways to phrase such an induction.
  One is to define the \emph{height} of a derivation to be the number of rules appearing in it, and then induct on the height of the derivation of $A\types B$.
  \begin{enumerate}
  \item If there are no rules at all, then $A\types B$ must come from a relation $A\le B$ in \cG; hence $P(A)\le P(B)$ since $P$ is a map of relational graphs.
  \item If there are $n>0$ rules, then consider the last rule.
    \begin{enumerate}
    \item If it is the identity rule $A\types A$, then $P(A)\le P(A)$ in \cA since \cA is a poset and hence reflexive.
    \item Finally, if it is the transitivity rule, then each of its premises $A\types B$ and $B\types C$ must have a derivation with strictly smaller height, so by the (strong) inductive hypothesis we have $P(A)\le P(B)$ and $P(B)\le P(C)$.
      Since \cA is a poset and hence transitive, we have $P(A)\le P(C)$.\qedhere
    \end{enumerate}
  \end{enumerate}
\end{proof}

A different way to phrase such an induction, which is more flexible and more type-theoretic in character, uses what is called \emph{structural induction}.
This means that rather than introduce the auxiliary notion of ``height'' of a derivation, we apply a general principle that \emph{to prove that a property $P$ holds of all derivations, it suffices to show for each rule that if $P$ holds of the premises then it holds of the conclusion}.
We can also define operations on derivations by \emph{structural recursion}, meaning that it suffices to define what happens to the conclusion of each rule assuming that we have already defined what happens to the premises.
Structural induction and recursion can be justified formally by set-theoretic arguments --- see \cref{chap:dedsys} for some general statements.
However, intuitively they implicit in what is meant by saying that ``derivations are what we obtain by applying rules one by one,'' just as ordinary mathematical induction is implicit in saying that ``the natural numbers are what we obtain by starting with zero and constructing successors one by one'', and constructive type-theoretic foundations for mathematics often take them as axiomatic.
From now on we will use structural induction and recursion on derivations in all type theories without further comment.

However, it is proved, \cref{thm:poset-initial-1} enables us to reach conclusions about arbitrary posets by deriving judgments in type theory.
In our present trivial case this is not very useful, but as we will see it becomes more useful for more complicated structures.

Another way to express the initiality theorem is to incorporate \cG into the rules.
Given a relational graph \cG, we define the \textbf{type theory of posets under \cG} to be the free type theory of posets together with a 0-ary rule
\begin{mathpar}
  \inferrule{ }{A\types B}
\end{mathpar}
for any relation $A\le B$ in \cG.
Now a derivation can be written without any ``leaves'' at the top, such as
\begin{mathpar}
  \inferrule*{\inferrule*{\inferrule*{ }{D\types A} \\ \inferrule*{ }{A\types B}}{D\types B} \\ \inferrule*{ }{B\types E}}{D\types E}
\end{mathpar}
Clearly this produces the same judgments; thus the initiality theorem can also be expressed as follows.

\begin{thm}\label{thm:poset-initial-2}
  For any relational graph \cG, the free poset $\F{\bPoset}\cG$ that it generates is has the same objects and its morphisms are the derivable judgments in the type theory of posets under \cG.\qed
\end{thm}

We can extract from this our first general statement about categorical logic: it is \emph{a syntax for generating free categorical structures using derivations from rules}.
The reader may be forgiven at this time for wondering what the point is; but bear with us and things will get less trivial.


\section{Categories}
\label{sec:categories}

Let's now generalize from posets to categories.
The relevant adjunction is now between categories \bCat and \emph{directed graphs} \bGr; the latter are sets $\cG$ of ``vertices'' equipped with a set $\cG(A,B)$ of ``edges'' for each $x,y\in \cG$.
Thus, we hope to generate the free category $\F{\bCat}\cG$ on a directed graph \cG type-theoretically.

Our judgments $A\types B$ will still represent morphisms from $A$ to $B$, but now of course there can be more than one such morphism.
Thus, to specify a particular morphism, we need more information than the simple \emph{derivability} of a judgment $A\types B$.
Na\"ively, the first thing we might try is to identify this extra information with the \emph{derivation} of such a judgment, i.e.\ with the tree of rules that were applied to reach it.
This makes the most sense if we take the approach of \cref{thm:poset-initial-2} rather than \cref{thm:poset-initial-1}, so that distinct edges $f,g\in \cG(A,B)$ can be regarded as distinct \emph{rules}
\begin{mathpar}
  \inferrule*[right=$f$]{ }{A\types B} \and
  \inferrule*[right=$g$]{ }{A\types B} \and
\end{mathpar}
Thus, for instance, if we have also $h\in \cG(B,C)$, the distinct composites $h\circ g$ and $h\circ f$ will be represented by the distinct derivations
\begin{mathpar}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{B\types C} \\
    \inferrule*[right=$g$]{ }{A\types B}
  }{
    A\types C
  }\and
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{B\types C} \\
    \inferrule*[right=$f$]{ }{A\types B}
  }{
    A\types C
  }\and
\end{mathpar}
Note that when we have distinct rules with the same premises and conclusion, we have to label them so that we can tell which is being applied.
For consistency, we begin labeling the identity and composition rules too, with $\circ$ and $\idfunc$.

Of course, this na\"ive approach founders on the fact that composition in a category is supposed to be associative and unital, since the two composites $h\circ (g\circ f)$ and $(h\circ g)\circ f$, which ought to be equal, nevertheless correspond to distinct derivations:
\begin{equation}\label{eq:assoc}
  \begin{array}{c}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{C\types D} \\
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$g$]{ }{B\types C} \\
      \inferrule*[right=$f$]{ }{A\types B}
    }{
      A\types C
    }}{
    A\types D
  }\\\\
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$h$]{ }{C\types D} \\
      \inferrule*[right=$g$]{ }{B\types C}
    }{
      B\types D
    }\\
    \inferrule*[right=$f$]{ }{A\types B}
  }{
    A\types D
  }
  \end{array}
\end{equation}
Thus, with this type theory we don't get the free category on \cG, but rather some free category-like structure that lacks associativity and unitality.
There are two ways to deal with this problem; we consider them in turn.

\subsection{Explicit cuts}
\label{sec:category-cutful}

The first solution is to simply quotient by an equivalence relation.
Our equivalence relation will have to identify the two derivations in~\eqref{eq:assoc}, and also the similar pairs for identities:
\begin{mathpar}
  \inferrule{\inferrule*[right=$\idfunc$]{ }{A\types A}\\ {A\types B}}{A\types B}\;\circ
  \qquad\equiv\qquad A\types B
  \\
  \inferrule{A\types B \\ \inferrule*[right=$\idfunc$]{ }{B\types B}}{A\types B}\;\circ
  \qquad\equiv\qquad A\types B
\end{mathpar}
Our equivalence relation must also be a ``congruence for the tree-construction of derivations'', meaning that these identifications can be made anywhere in the middle of a long derivation, such as:
\begin{mathpar}
  \inferrule{\inferrule*{}{\sD_1\\\\\vdots} \\
    \inferrule*[right=$\circ$]{\inferrule*[right=$\idfunc$]{ }{A\types A}\\ \inferrule*{\sD_2\\\\\vdots}{A\types B}}{A\types B}
  }{\vdots\\\\\sD_3}
  \qquad\equiv\qquad
  \inferrule{\inferrule*{}{\sD_1\\\\\vdots} \\
    \inferrule*{\sD_2\\\\\vdots}{A\types B}
  }{\vdots\\\\\sD_3}
\end{mathpar}
We will also have to close it up under reflexivity, symmetry, and transitivity to make an equivalence relation.

Of course, it quickly becomes tedious to draw such derivations, so it is convenient to adopt a more succinct syntax for them.
We begin by labeling each judgment with a one-dimensional syntactic representation of its derivation tree, such as:
\begin{mathpar}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$g$]{ }{g:(B\types C)} \\
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$\idfunc$]{ }{\idfunc_B:(B\types B)} \\
      \inferrule*[right=$f$]{ }{f:(A\types B)}
    }{
      (\idfunc_B\circ f):(A\types B)
    }}{
    (g\circ (\idfunc_B\circ f)) : (A\types C)
  }
\end{mathpar}
These labels are called \emph{terms}.
Of course, in this case they are none other than the usual notation for composition and identities.
Formally, this means the rules are now:
\begin{mathpar}
  \inferrule{f\in\cG(A,B)}{f:(A\types B)}\and
  \inferrule{A\in\cG}{\idfunc_A : (A\types B)}\and
  \inferrule{\phi:(A\types B) \\ \psi:(B\types C)}{\psi\circ \phi:(A\types C)}
\end{mathpar}
% TODO: Mention somewhere the assumptions of "external" facts as premises
Here $\phi,\psi$ denote arbitrary terms, and if they contain $\circ$'s themselves then we put parentheses around them, as in the example above.
Now the generators of our equivalence relation look even more familiar:
\begin{align*}
  \phi \circ (\psi \circ \chi) &\equiv (\phi\circ\psi)\circ\chi\\
  \phi \circ \idfunc_A &\equiv \phi\\
  \idfunc_B \circ \phi &\equiv \phi
\end{align*}
Again $\phi,\psi,\chi$ denote arbitrary terms, corresponding to the fact that arbitrary derivations can appear at the top of our identified trees; and similarly these identifications can also happen anywhere inside another term, so that for instance
\[ k\circ (h\circ (g\circ f)) \equiv k\circ ((h\circ g)\circ f).  \]

Of course, we only impose these relations when they make sense.
We can describe the conditions under which this happens using rules for a secondary judgment $\phi\equiv \psi : (A\types B)$.
The rules for our generating equalities are
\begin{mathpar}
  \inferrule{\phi:(A\types B) \\ \psi:(B\types C) \\ \chi:(C\types D)}{(\phi \circ (\psi \circ \chi) \equiv (\phi\circ\psi)\circ\chi) : (A\types D)}\\
  \inferrule{\phi:(A\types B)}{(\phi \circ \idfunc_A \equiv \phi):(A\types B)}\and
  \inferrule{\phi:(A\types B)}{(\idfunc_B \circ \phi \equiv \phi):(A\types B)}
\end{mathpar}
and we must also have rules ensuring that we have an equivalence relation and a congruence:
\begin{mathpar}
  \inferrule{\phi:(A\types B)}{(\phi\equiv\phi):(A\types B)}\and
  \inferrule{(\phi\equiv\psi):(A\types B)}{(\psi\equiv\phi):(A\types B)}\and
  \inferrule{(\phi\equiv\psi):(A\types B)\\(\psi\equiv\chi):(A\types B)}{(\phi\equiv\chi):(A\types B)}\and
  \inferrule{(\phi_1\equiv\psi_1):(A\types B)\\(\phi_2\equiv\psi_2):(B\types C)}{(\phi_2\circ \phi_1 \equiv \psi_2\circ\psi_1):(A\types C)}
\end{mathpar}
The last of these is sufficient, in our simple case, to ensure we have a congruence; in general we would have to have one such equality rule for each basic rule of the theory (except for those with no premises, like $\idfunc$).

Many of our type theories will involve such an equality judgment, for which we always use the notation $\equivsym$, and the need for the equivalence relation and congruence rules is always the same.
Thus, we generally decline to mention them, stating only the ``interesting'' generating equalities for the theory.
A general framework for such equality judgments is described in \cref{sec:axioms}.

In our case, when the rules for $\circ$ and $\idfunc$ are augmented by these rules for $\equiv$, and we also add axioms for the edges of a given directed graph \cG, we call the result the \textbf{cut-ful type theory for categories under \cG}.
It may seem obvious that this produces the free category on \cG, but again we write it out carefully to help ourselves get used to the patterns.
In particular, we want to emphasize the role played by the following lemma:

\begin{lem}\label{thm:category-uniqderiv}
  If $\phi :(A\types B)$ is derivable in the cut-ful type theory for categories under \cG, then it has a unique derivation.
\end{lem}
\begin{proof}
  The point is that the terms produced by all the rules have disjoint forms.
  If $\phi$ is of the form ``$f$'' for some $f\in\cG(A,B)$, then it can only be derived by the first rule; if it is of the form ``$\idfunc_A$'' then it can only be derived by the identity rule, and if it is of the form ``$\psi\circ\phi$'' it can only be derived by the composition rule.
  In the third case, we apply induction to show that $\phi$ and $\psi$ also have unique derivations.
\end{proof}

In other words, the terms (before we impose the relation $\equiv$ on them) really are simply one-dimensional representations of derivations, as we intended.
This will not be true of all type theories; but those for which it fails tend to be much more complicated to analyze and prove the initiality theorem for.
We will always attempt to construct our type theories so that this property holds, and we will remark on it only when it fails.

\begin{rmk}
  Technically, there is either more or less happening here than may appear (depending on your point of view).
  A term as we write it on the page is really just a string of symbols, whereas in the proof of \cref{thm:category-uniqderiv} we have assumed that a term such as ``$f\circ (g\circ h)$'' can uniquely be read as $\circ$ applied to ``$f$'' and ``$g\circ h$''.
  This simple string of symbols could technically be regarded as $\circ$ applied to ``$f\circ (g$'' and ``$h)$'', but of course that would make no sense because those are not meaningful terms in their own right (in particular, they contain unbalanced parentheses).

  Thus, something \emph{more} must be happening, and that something else is called \emph{parsing} a term.
  Human mathematicians do it instinctively without thinking; electronic mathematicians have to be programmed to do it.
  In either case, the result of parsing a string of symbols is an ``internal'' representation (a mental idea for humans, a data structure for computers) that generally has the form of a tree, indicating the ``outermost'' operation as the root with its operands as branches, and so on, for instance:
  \[ f\circ (g\circ h) \qquad\leadsto\qquad \vcenter{\xymatrix@-1pc{ & \circ \ar@{-}[dl] \ar@{-}[dr]\\
      f && \circ \ar@{-}[dl]\ar@{-}[dr]\\
      & g && h }} \]

  Of course, this ``internal'' tree representation of a term is nothing but the corresponding derivation flipped upside-down.
  So in that sense \cref{thm:category-uniqderiv} is actually saying \emph{less} than one might think; the actual work is being done by the silent step of parsing.
  We will not say much more about parsing, however (though we discuss it a bit further in \cref{chap:dedsys}); we trust the human reader to do it on their own, and we trust programmers to have good algorithms for it.
  Accordingly, for type theories that satisfy the ``terms are derivations'' principle we will usually simply observe this fact, leaving it to the concerned reader to verify that the parse trees really do correspond to the derivation trees.
  (However, one small additional issue regarding this will arise in the next subsection.)
\end{rmk}

Now we can prove the initiality theorem.

\begin{thm}\label{thm:category-initial-1}
  The free category on a directed graph $\cG$ has the same objects as \cG, and its morphisms $A\to B$ are the terms $\phi$ such that $\phi :(A\types B)$ is derivable in the cut-ful type theory for categories under \cG, modulo the equivalence relation $\phi\equiv \psi:(A\types B)$.
\end{thm}
\begin{proof}
  Let $\F\bCat\cG$ be defined as described in the theorem; the identity and composition rules give it the structure necessary to be a category, and the transitivity and unitality relations make it a category.

  Now suppose \cA is any category and $P:\cG\to\cA$ is a map of directed graphs.
  Then $P$ extends uniquely to the objects of $\F\bCat\cG$, since they are the same as those of \cG.
  But unlike the case of posets, we have to define it on the morphisms of $\F\bCat\cG$ as well.

  If $\phi :(A\types B)$ is derivable, then by \cref{thm:category-uniqderiv} it has a unique derivation; thus we can define $P(\phi)$ by recursion on the derivation of $\phi$.
  Of course, if the derivation of $\phi$ ends with $f\in\cG(A,B)$, then we define $P(\phi)=P(f)$; if it ends with $\idfunc_A$ we define $P(\phi)=\idfunc_{P(A)}$; and if it ends with $\psi\circ\chi$ we define $P(\phi) = P(\psi)\circ P(\chi)$.

  We also have to show that this definition respects the equivalence relation $\equiv$.
  This is clear since $\cA$ is a category; formally it would be another induction on the derivations of $\equiv$ judgments.

  Finally, we have to show that this $P:\F\bCat\cG\to\cA$ is a functor.
  This follows by definition of the category structure of $\F\bCat\cG$ and the action of $P$ on its arrows.
\end{proof}

Of course, once again very little seems to be happening; we are just using a complicated funny syntax to build a free algebraic structure.
Therefore, it is the second way to deal with the problem of associativity that is more interesting.

\subsection{Cut admissibility}
\label{sec:category-cutadm}

In this case what we do is \emph{remove the composition rule $\circ$ entirely}; instead we ``build (post)composition into the axioms''.
That is, the only rule independent of \cG is identities:
\[ \inferrule{ }{A\types A}\,\idfunc \]
while for every edge $f\in \cG(A,B)$ we take the following rule:
\[ \inferrule{X\types A}{X\types B} \,f \]
for any $X$.
Informally, one might say that we represent $f$ by its ``image under the Yoneda embedding''.

Note that we have made a choice to build in \emph{postcomposition}; we could also have chosen to build in precomposition.
In the current context, either choice would work just as well; but later on we will see that there were reasons to choose postcomposition here.
We will call this the \textbf{cut-free type theory for categories under \cG}.

In this theory, if we have $f\in\cG(A,B)$, $g\in\cG(B,C)$, and $h\in \cG(C,D)$ there is \emph{only one way} to derive $A\types D$:
\begin{mathpar}
  \inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{A\types A}
      }{
        A\types B
      }
    }{
      A\types C
    }
  }{
    A\types D
  }
\end{mathpar}
Thus, we no longer have to worry about distinguishing between $h\circ (g\circ f)$ and $(h\circ g)\circ f$.
Of course, we have a new problem: if we are trying to build a category, then we \emph{do} need to be able to compose arrows!
So we need the following theorem:

\begin{thm}\label{thm:category-cutadm}
  If we have derivations of $A\types B$ and $B\types C$ in the cut-free type theory for categories under \cG, then we can construct a derivation of $A\types C$.
\end{thm}
\begin{proof}
  We induct on the derivation of $B\types C$.
  If it ends with $\idfunc$, then it must be that $B=C$; so our given derivation of $A\types B$ is also a derivation of $A\types C$.
  Otherwise, we must have some $f\in\cG(D,C)$ and our derivation of $B\types C$ ends like this:
  \begin{mathpar}
    \inferrule*[right=$f$]{\inferrule*{\sD\\\\\vdots}{B\types D}}{B\types C}
  \end{mathpar}
  In particular, it contains a derivation \sD of $B\types D$.
  Thus, by the inductive hypothesis we have a derivation, say $\sD'$, of $A\types D$.
  Now we can simply follow this with the rule for $f$:
  \begin{equation*}
    \inferrule*[right=$f$]{\inferrule*{\sD'\\\\\vdots}{A\types D}}{A\types C}\qedhere
  \end{equation*}
\end{proof}

In type-theoretic lingo, \cref{thm:category-cutadm} says that \textbf{the cut rule is admissible} in the cut-free type theory for categories under \cG.
In other words, although the cut/composition rule
\begin{mathpar}
  \inferrule*[right=$\circ$]{A\types B \\ B\types C}{A\types C}
\end{mathpar}
is not \emph{part of the type theory} as defined, it is nevertheless true that whenever we have derivations of the premises of this rule, we can construct a derivation of its conclusion.

\begin{rmk}\label{rmk:admissible-derivable-1}
  This is what it means in general for a rule to be \textbf{admissible}: it is not part of the theory as defined (that is, it is not one of the \textbf{primitive rules}), but nevertheless if it were added to the theory it would not change the set of derivable sequents.
  In between primitive and admissible rules there are \textbf{derivable rules}: those that can be expanded out directly into a fragment of a derivation in terms of the primitive rules.
  For instance, if we have $f\in\cG(A,B)$ and $g\in \cG(B,C)$, then the left-hand rule below is derivable:
  \begin{mathpar}
    \inferrule*{X\types A}{X\types C}\and
    \inferrule*[Right=$g$]{\inferrule*[Right=$f$]{X\types A}{X\types B}}{X\types C}
  \end{mathpar}
  because we can expand it out into the right-hand derivation in terms of the primitive rules.
  Any derivable rule is admissible: if we have a derivation of $X\types A$ we can follow it with the $f$ and $g$ rules to obtain a derivation of $X\types C$.
  Note the difference with the proof of cut-admissibility: here we do not need to modify the given derivation, we only apply further primitive rules to its conclusion.
  We will return to this distinction in \cref{rmk:admissible-derivable-2}.
\end{rmk}

Another way to say what is going on is that the morphisms in the free category on a directed graph \cG have an explicit description as \emph{finite strings of composable edges} in \cG.
We have just given an inductive definition of ``finite string of composable edges'': there is a finite string (of length 0) from $A$ to $A$; and if we have such a string from $X$ to $A$ and an edge $f\in\cG(A,B)$, we can construct a string from $X$ to $B$.

We could prove the initiality theorem by appealing to this known fact about free categories, but as before, we prefer to give a more explicit proof to illustrate the patterns of type theory.
For this purpose, we have to introduce terms, as we did in the previous section for the cut-ful theory.
We can do this with terms directly constructed so that their parse tree will mirror the derivation tree, for instance writing the rules as
\begin{mathpar}
  \inferrule{ }{\idfunc_A:(A\types A)}\,\idfunc\and
  \inferrule{\phi:(X\types A)}{\postc f\phi:(X\types B)} \,f
\end{mathpar}
Then a term derivation and corresponding parse tree would look like
\begin{mathpar}
\inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{\idfunc_A:(A\types A)}
      }{
        \postc f{\idfunc_A}:(A\types B)
      }
    }{
      \postc g{\postc f{\idfunc_A}}:(A\types C)
    }
  }{
    \postc h{\postc g{\postc f{\idfunc_A}}}:(A\types D)
  }
  \and\leadsto\and
  \vcenter{\xymatrix@-1pc{
      \postcsym h \ar@{-}[d] \\
      \postcsym g \ar@{-}[d] \\
      \postcsym f \ar@{-}[d] \\
      \idfunc_A
    }}
\end{mathpar}
However, now there is another option available to us, which begins to show more of the characteristic behavior of type-theoretic terms.
Rather than describing the entire judgment $A\types B$ with a term, the way we did for the cut-ful theory, we assign a \emph{formal variable} such as $x$ to the domain $A$, and then an expression containing $x$ to the codomain $B$.
For the theory of plain categories that we are working with here, the only possible expressions are repeated applications of function symbols to the variable, such as $h(g(f(x)))$.
We write this as
\[ x:A \types h(g(f(x))) : B\]
The identity rules and axiom rules can now be written as
\begin{mathpar}
  \inferrule{ }{x:A\types x:A}\,\idfunc \and
  \inferrule{x:X\types M:A}{x:X\types f(M):B} \,f
\end{mathpar}
Here $M$ denotes an arbitrary term, which will generally involve the variable $x$.
Thus, for instance, the composite of $h$, $g$, and $f$ would be written like so:
\begin{mathpar}
  \inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{x:A\types x:A}
      }{
        x:A\types f(x):B
      }
    }{
      x:A\types g(f(x)): C
    }
  }{
    x:A\types h(g(f(x))):D
  }
\end{mathpar}
Of course, the term $h(g(f(x)))$ has essentially the same parse tree as the term $\postc h{\postc g{\postc f{\idfunc_A}}}$ shown above, so it can clearly represent the same derivation.
The main difference is that instead of $\idfunc_A$ we have the variable $x$ representing the identity rule.

This is our first encounter with how type theory permits a ``set-like'' syntax when reasoning about arbitrary categorical structures.
It is also one reason why we chose to build in postcomposition rather than precomposition.
If we used precomposition instead, then the analogous syntax would be backwards: we would have to represent $f:A\to B$ as $f(u):A \types u:B$ rather than $x:A \types f(x):B$.
At a formal level, there would be little difference, but it feels much more familiar to apply functions to variables than to co-apply functions to co-variables.
(We can still dualize at the level of the categorical models; we already mentioned in \cref{sec:intro} that we could apply the type theory of categories with finite products to the opposite of the category of commutative rings.)

There is one further remark to make about the correspondence between terms and derivations in this theory.
As written on the page, the judgments $x:A \types f(x):B$ and $y:A \types f(y):B$ are distinct; but clearly they represent the same morphism.
So there is still an equivalence relation hanging around in the background, but it is a fairly harmless one; it just says that we can rename all the variables as long as we do it consistently.
(When we consider theories with bound variables later on, the meaning of ``consistently'' will become more complicated, but the principle is the same.)
\label{sec:alpha}%
In type-theoretic lingo, the equivalence relation of renaming variables is called \textbf{$\alpha$-equivalence}, and unless explicitly stated otherwise we \emph{always} consider judgments modulo $\alpha$-equivalence.\footnote{There is even a technical trick called ``de Bruijn indices'' that enables a computer to represent terms without having to choose variable names at all, avoiding the issue entirely.  But this notation is so difficult for humans to understand that (according to Conor McBride) Bob Atkey once described it as a reverse Turing test.}

Now that we have a nice notation to distinguish between derivations, we should observe that \cref{thm:category-cutadm} is not just a statement about derivability.
Rather, its proof constructs, recursively, an \emph{operation} on derivations, and hence on terms: given derivable term judgments $x:A\types M:B$ and $y:B\types N:C$, we can construct a derivable judgment $x:A\types P:C$.
For instance, suppose we start with $x:A \types f(x):B$ and $y:B\types h(g(y)):C$; then the construction proceeds in the following steps.
\begin{itemize}
\item The second derivation ends with an application of $h$, so we apply the inductive hypothesis to $x:A \types f(x):B$ and $y:B\types g(y):D$.
\item Now the second derivation begins with an application of $g$, so we recurse again on $x:A \types f(x):B$ and and $y:B\types y:B$.
\item This time the second derivation is just the identity rule, so the result is the first given derivation $x:A \types f(x):B$.
\item Backing out of the induction one step, we apply $g$ to this result to get $x:A\types g(f(x)):D$.
\item Finally, backing out one more time, we apply $h$ to the previous result to get $x:A\types h(g(f(x))):C$.
\end{itemize}
Intuitively, the result $h(g(f(x)))$ has been obtained by \emph{substituting} the term $f(x)$ for the variable $y$ in the term $h(g(y))$.
Thus, we refer to the operation on terms defined by \cref{thm:category-cutadm} as \textbf{substitution}.
In general, given $x:A\types M:B$ and $y:B\types N:C$ we denote the substitution of $M$ for $y$ in $N$ by $N[M/y]$ (although unfortunately one also finds other notations in the literature; including, quite confusingly, $[M/y]N$ and $N[y/M]$).
Note that this is ``meta-notation''; the square brackets are not part of the syntax of terms, instead they denote an operation \emph{on} terms.
The proof of \cref{thm:category-cutadm} \emph{defines} the notion of substitution recursively in the following way:
\begin{align*}
  y[M/y] &\coloneqq M\\
  f(N)[M/y] &\coloneqq f(N[M/y])
\end{align*}
When terms are regarded as syntax in their own right rather than as mere proxies for derivations, it is common to define substitution as an operation on terms first, and then to state (the analogue of) \cref{thm:category-cutadm} as ``if $x:A\types M:B$ and $y:B\types N:C$ are derivable, then so is $x:A\types M[N/y]:C$''.
From this point of view, one says that \textbf{substitution is admissible} in the cut-free type theory for categories under \cG.

Before proving the initiality theorem, let us first observe that substitution does, in fact, define a category:

\begin{lem}\label{thm:category-subassoc}
  Substitution is associative: given $x:A\types M:B$ and $y:B\types N:C$ and $z:C\types P:D$, we have $P[N/z][M/y] = P[N[M/y]/z]$.
  (This is a literal equality of terms or derivations.)
\end{lem}
\begin{proof}
  By induction on the derivation of $P$.
  If it ends with the identity, so that $P=z$, then
  \[P[N/z][M/y] = z[N/z][M/y] = N[M/y] = z[N[M/y]/z] = P[N[M/y]/z] \]
  If it ends with an application of a morphism $f$, so that $P = f(Q)$, then
  \begin{multline*}
    f(Q)[N/z][M/y] = f(Q[N/z])[M/y] = f(Q[N/z][M/y])\\
    = f(Q[N[M/y]/z]) = f(Q)[N[M/y]/z]
  \end{multline*}
  using the inductive hypothesis for $Q$ in the third step.
\end{proof}

\begin{thm}\label{thm:category-initial-2}
  The free category on a directed graph $\cG$ has the same objects as \cG, and its morphisms are the derivable term judgments $x:A\types M:B$ in the cut-free type theory for categories under \cG.
\end{thm}
\begin{proof}
  Let $\F\bCat\cG$ be defined as in the statement, with composition given by substitution constructed as in \cref{thm:category-cutadm}.
  By \cref{thm:category-subassoc}, composition is associative.
  For unitality, we have $y[M/y] = y$ by definition, while $N[x/x] = N$ is another easy induction on the structure of $N$.
  Thus, $\F\bCat\cG$ is a category.

  Now suppose \cA is any category and $P:\cG\to\cA$ is a map of directed graphs.
  We define $P:\F\bCat\cG \to\cA$ by recursion on the rules of the type theory: the identity $x:A\types x:A$ goes to $\idfunc_{P(A)}$, while $x:A\types f(M):B$ goes to $P(f) \circ P(M)$, with $P(M)$ defined recursively.
  Since $x:A\types f(M):B$ is the composite of $x:A\types M:C$ and $y:C\types f(y):B$ in $\F\bCat\cG$, this is the only possible definition that could make $P$ a functor.
  It remains to check that it actually \emph{is} a functor, i.e.\ that it preserves \emph{all} composites; that is, we must show that $P(N[M/y]) = P(N) \circ P(M)$.
  This follows by yet another induction on the derivation of $N$.
\end{proof}

Note that we did not have to impose any equivalence relation on the terms in this theory (apart from the fairly trivial $\alpha$-equivalence).
This suggests a second, more interesting, general statement about categorical logic: it is
{a syntax for generating free categorical structures using derivations from rules}
\emph{that yield elements in canonical form}, eliminating the need for quotients.
This statement is actually too narrow; as we will see later on, type theory is not \emph{just} about canonical forms.
However, canonical forms do play a very important role.

From the perspective of category theory, the reason for the importance of canonical forms is that we can easily decide whether two canonical forms are equal.
In the cut-free type theory for categories, two terms present the same morphism in a free category just when they are literally equal (modulo $\alpha$-conversion); whereas to check whether two terms are equal in the cut-ful theory we have to remove the identities and reassociate them all to the left or the right.

In fact, a good algorithm for checking equality of terms in the cut-ful theory is to \emph{interpret them into the cut-free theory}!
That is, we note that every rule of the cut-ful theory is admissible in the cut-free theory, so any term (i.e.\ derivation) in the cut-ful theory yields a derivation in the cut-free theory.
For instance, to translate the cut-ful term $h\circ ((\id_C\circ g) \circ f)$ into the cut-free theory, we first write it as a derivation
\begin{mathpar}
  \inferrule*[Right=$\circ$]{
    h:(C\types D)\\
    \inferrule*[Right=$\circ$]{
      \inferrule*[Right=$\circ$]{
        \id_C:(C\types C)\\
        g:(B\types C)}
      {(\id_C\circ g):(B\types C)}\\
      f:(A\types B)
    }{((\id_C\circ g) \circ f):(A\types C)}
  }{(h\circ ((\id_C\circ g) \circ f)):(A\types D)}
\end{mathpar}
and then annotate the same derivation by cut-free terms, using substitution for composition:
\begin{mathpar}
  \inferrule*[Right=$\circ$]{
    z:C\types h(z):D\\
    \inferrule*[Right=$\circ$]{
      \inferrule*[Right=$\circ$]{
        z:C\types z:C\\
        y:B\types g(y):C}
      {y:B\types g(y):C}\\
      x:A\types f(x):B
    }{x:A\types g(f(x)):C}
  }{x:A\types h(g(f(x))):D}
\end{mathpar}
Since, as we have proven, both the cut-ful and the cut-free theory present the same free structure, it follows that \emph{two terms in the cut-ful theory are equal modulo $\equiv$ exactly when their images in the cut-free theory are identical}.
Informally, we are just comparing two  ``removing all the identities and the parentheses''; but as we will see, in a more complicated theory much more can be going on.

In this sense, type theory can be considered to be about solving \emph{coherence problems} in category theory.
In general, the coherence problem for a categorical structure is to decide when two morphisms ``constructed from its basic data'' are equal (or isomorphic, etc.)
For instance, the classical coherence theorem of MacLane for monoidal categories says, informally, that two parallel morphisms constructed from the basic constraint isomorphisms of a monoidal category are \emph{always} equal; whereas the analogous theorem for braided monoidal categories says that they are equal if and only if they have the same underlying braid.
A type-theoretic calculus of canonical forms gives a way to answer this question, by translating a cut-ful theory into a cut-free one.
(And indeed, cut-elimination methods have frequently been used in the proof of coherence theorems; [TODO: some citations].)

A related remark is that categorical logic is about \emph{showing that two different categories have the same\footnote{Of course, technically, an object of one category is not generally also an object of another one.  So what we mean is that there is an easy way to transform the initial object of one category into the initial object of another.} initial object}.
The primitive rules of a type theory can be regarded as the ``operations'' of a certain algebraic theory, and the judgments that can be derived from these rules form the initial algebra for this theory, i.e.\ the initial object in a certain category.
(See \cref{chap:dedsys} for a precise statement along these lines.)
The initiality theorems we care about, however, show that these initial objects are \emph{also} initial in some other, quite different, category that is of more intrinsic categorical interest.

\begin{rmk}\label{rmk:admissible-derivable-2}
  This point of view sheds further light on the distinction between derivable and admissible rules mentioned in \cref{rmk:admissible-derivable-1}.
  A derivable rule automatically holds in any model of the ``algebraic theory'' version of a type theory, whereas an admissible rules holds \emph{only in the initial algebra} for this algebraic theory.
  In particular, an arbitrary model of the algebraic rules of the cut-free type theory for categories is not even a category, e.g.\ it may not satisfy the cut rule.
\end{rmk}


\subsection*{Exercises}

% [I think we should do this one in the text.]
% \begin{ex}
%   Prove \cref{thm:category-initial-2}.
%   You will have to start by proving that the composition defined in \cref{thm:category-cutadm} is in fact associative and unital, so that it forms a category.
% \end{ex}

\begin{ex}\label{ex:categories-over}
  Let \sM be a fixed category; then we have an induced adjunction between $\bCat/\sM$ and $\bGr/\sM$.
  Describe a cut-free type theory for presenting the free category-over-\sM on a directed-graph-over-\sM, and prove the initiality theorem (the analogue of \cref{thm:category-initial-2}).
  Note that you will have to prove that cut is admissible first.
  \textit{(Hint: index the judgments by arrows in \sM, so that for instance $A\types_\alpha B$ represents an arrow lying over a given arrow $\alpha$ in $\sM$.)}
\end{ex}


\section{Meet-semilattices}
\label{sec:mslat}

Moving gradually up the ladder of nontriviality, we now consider categories with finite products, or more precisely binary products and a terminal object.
In fact, let us revert back to the posetal world first and consider posets with binary meets and a top element, i.e.\ meet-semilattices.
We will make all this structure algebraic, so that our meet-semilattices are posets (which, recall, is not necessarily skeletal) \emph{equipped with} a chosen top element and an operation assigning to each pair of objects a meet.
We then have an adjunction relating the category \bmSLat of such meet-semilattices (and morphisms preserving all the structure strictly) with the category \bRelGr of relational graphs, and we want to describe the free meet-semilattice on a relational graph \cG.

One new feature this introduces is that the objects of $\F\bmSLat \cG$ will no longer be the same as those of \cG: we need to add a top element and freely apply the meet operation.
In order to describe this type-theoretically, we introduce a new judgment ``$\types A\type$'', meaning that $A$ will be one of the objects of the poset we are generating.
The rules for this judgment are
\begin{mathpar}
  \inferrule{ }{\types \top\type} \and
  \inferrule{\types A\type \\ \types B\type}{\types A\meet B\type}
\end{mathpar}
When talking about type theory under \cG, we additionally include ``axiom'' rules saying that each object of \cG is a type:
\begin{mathpar}
  \inferrule{A\in\cG}{\types A\type}
\end{mathpar}
Note that the premise $A\in \cG$ here is not a judgment; rather it is an ``external'' fact that serves as a precondition for application of thish rule.
Thus it might be more correct to write this rule as
\begin{mathpar}
  \inferrule{ }{\types A\type}\;\text{(if $A\in\cG$)}
\end{mathpar}
but we will generally write such conditions as premises for simplicity.

As an example of the application of these rules, if $A,B\in\cG$ we have a derivation
\begin{mathpar}
  \inferrule*{
    \inferrule*{A\in\cG}{\types A\type}\\
    \inferrule*{\inferrule*{ }{\types \top\type} \\ \inferrule*{B\in\cG}{\types B\type}}{\types \top\meet B\type}
  }{
    \types (A\meet (\top\meet B))\type
  }
\end{mathpar}
so that $A\meet (\top\meet B)$ will be one of the objects of $\F\bmSLat\cG$.

Now we need to describe the morphisms, i.e.\ the relation $\le$ in $\F\bmSLat\cG$.
The obvious thing to do is to assert the universal property of the meet and the top element:
\begin{mathpar}
  \inferrule{ }{A\types \top}\and
  \inferrule{ }{A\meet B \types A}\and
  \inferrule{ }{A\meet B \types B}\and
  \inferrule{A\types B \\ A\types C}{A\types B\meet C}
\end{mathpar}
This works, but it forces us to go back to asserting transitivity/cut.
For instance, if $A,B,C\in \cG$ we have the following derivation:
\begin{mathpar}
  \inferrule*{
    \inferrule*{ }{(A\meet B)\meet C \types A\meet B}\\
    \inferrule*{ }{A\meet B \types A}
  }{
    (A\meet B)\meet C \types A
  }
\end{mathpar}
but there is no way to deduce this without using the cut rule.
Thus, this \textbf{cut-ful type theory for meet-semilattices under \cG} works, but to have a better class of ``canonical forms'' for its relations we would also like a cut-free version.

What we need to do is to treat the ``projections'' $A\meet B \to A$ and $A\meet B\to B$ similarly to how we treated the edges of \cG in \cref{sec:categories}.
However, at this point we have to make a choice of whether to build in postcomposition or precomposition:
\[
\inferrule{A\types C}{A\meet B \types C} \qquad\text{or}\qquad
\inferrule{C\types A\meet B}{C\types A} \quad ?
\]
Both choices work (that is, they make cut admissible), and lead to different kinds of type theories with different properties.
The first leads to a kind of type theory called \textbf{sequent calculus}, and the second to a kind of type theory called \textbf{natural deduction}.
We consider each in turn.

\subsection{Sequent calculus for meet-semilattices}
\label{sec:seqcalc-mslat}

To be precise, for a relational graph \cG, the \textbf{unary sequent calculus for meet-semilattices under \cG} has the following rules (in addition to the rules for the judgment $\types A\type$ mentioned above).
We label each rule on the right to make them easier to refer to later on.
\begin{mathpar}
  \inferrule{A\in \cG}{A\types A}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ X\types A}{X\types B}\;fR\and
  \inferrule{\types A\type}{A\types \top}\;\top R\and
  \inferrule{A\types C \\ \types B\type}{A\meet B \types C}\;\meetL1\and
  \inferrule{B\types C \\ \types A\type}{A\meet B \types C}\;\meetL2\and
  \inferrule{A\types B \\ A\types C}{A\types B\meet C}\;\meetR
\end{mathpar}

There are several things to note about this.
The first is that we have included in the premises some judgments of the form $\types A\type$.
This ensures that whenever we can derive a sequent $A\types B$, that $A$ and $B$ are well-formed as types.
However, we don't need to assume explicitly as premises that \emph{all} types appearing in any sequent are well-formed, only those that are introduced without belonging to any previous sequents; this is sufficient for the following inductive proof.

\begin{thm}\label{thm:seqcalc-mslat-wftype}
  In the unary sequent calculus for meet-semilattices under \cG, if $A\types B$ is derivable, then so are $\types A\type$ and $\types B\type$.
\end{thm}
\begin{proof}
  By induction on the derivation of $A\types B$.
  \begin{itemize}
  \item If it is the $\idfunc$ rule, then $A\in\cG$ and so $\types A\type$.
  \item If it ends with the rule $f$ for some $f\in\cG(A,B)$, then $B\in \cG$ and so $\types A\type$, while $X\types A$ and so $\types X\type$ by the inductive hypothesis.
  \item If it ends with the rule $\top R$, then $\types A\type$ by assumption.  
  \item If it ends with the rule $\meetL1$, then $\types B\type$ by assumption, while $\types A\type$ and $\types C\type$ by the inductive hypothesis; thus also $\types A\meet B\type$.
  \item The cases for $\meetL2$ and $\meetR$ are similar.\qedhere
  \end{itemize}
\end{proof}

The second thing to note is that we only assert the identity rule $A\types A$ when $A$ is a \emph{generating object} (also called a \emph{base type}), i.e.\ an object of \cG.
This is sufficient because in the sequent calculus, we can derive the identity rule for any type:

\begin{thm}\label{thm:seqcalc-mslat-idadm}
  In the unary sequent calculus for meet-semilattices under \cG, if $A$ is a type (that is, if $\types A\type$ is derivable), then $A\types A$ is derivable.
\end{thm}
\begin{proof}
  We induct on the derivation of $\types A\type$.
  There are three cases:
  \begin{enumerate}
  \item $A$ is in \cG.  In this case $A\types A$ is an axiom.
  \item $A=\top$.  In this case $\top\types\top$ is a special case of the rule that anything $\types\top$.
  \item $A=B\meet C$ and we have derivations $\sD_B$ and $\sD_C$ of $\types B \type$ and $\types C\type$ respectively.
    Therefore we have, inductively, derivations $\sD_1$ and $\sD_2$ of $B\types B$ and $C\types C$, and we can put them together like this:
    \begin{equation*}
      \inferrule*{
        \inferrule*{
          \inferrule*{\sD_1\\\\\vdots}{B\types B} \\
          \inferrule*{\sD_C\\\\\vdots}{\types C\type}
        }{
          B\meet C \types B
        }\\
        \inferrule*{
          \inferrule*{\sD_2\\\\\vdots}{C\types C}\\
          \inferrule*{\sD_B\\\\\vdots}{\types B\type}
        }{
          B\meet C\types C
        }
      }{
        B\meet C\types B\meet C
      }\qedhere
    \end{equation*}
  \end{enumerate}
\end{proof}

In other words, the general identity rule
\[ \inferrule{\types A\type}{A\types A} \]
is also \emph{admissible}.
This is a general characteristic of sequent calculi.

Next we prove that the cut rule is admissible for this sequent calculus too.

\begin{thm}\label{thm:seqcalc-mslat-cutadm}
  In the unary sequent calculus for meet-semilattices under \cG, if $A\types B$ and $B\types C$ are derivable, then so is $A\types C$.
\end{thm}
\begin{proof}
  By induction on the derivation of $B\types C$.
  \begin{enumerate}
  \item If it is $\idfunc$, then $B=C$.
    Now $A\types C$ is just $A\types B$ and we are done.
  \item If it is $f\in\cG(C',C)$, then we have a derivation of $B\types C'$.
    So by the inductive hypothesis we can derive $A\types C'$, whence also $A\types C$ by the rule for $f$.
  \item If it ends with $\top R$, then $C=\top$.
    Since $A\types B$ is derivable, by \cref{thm:seqcalc-mslat-wftype} $\types A\type$ is also derivable; thus by $\top R$ we have $A\types \top$.
  \item If it ends with $\meetR$, then $C=C_1\meet C_2$ and we have derivations of $B\types C_1$ and $B\types C_2$.
    By the inductive hypothesis we can derive both $A\types C_1$ and $A\types C_2$, to which we can apply $\meetR$ to get $A\types C_1\meet C_2$.
  \item If it ends with $\meetL1$, then $B=B_1\meet B_2$ and we can derive $B_1\types C$.
    We now do a secondary induction on the derivation of $A\types B$.
    \begin{enumerate}
    \item It cannot end with $\idfunc$ or $f$ or $\top R$, since $B=B_1\meet B_2$ is not in $\cG$ and not equal to $\top$.
    \item If it ends with $\meetL1$, then $A=A_1\meet A_2$ and we can derive $A_1\types B$.
      By the inductive hypothesis, we can derive $A_1 \types C$, and hence by $\meetL1$ also $A \types C$.
      The case of $\meetL2$ is similar.
    \item Finally, if it ends with $\meetR$, then we can derive $A\types B_1$ and $A\types B_2$.
      Recall that we are also assuming a derivation of $B_1\types C$.
      Thus, by the inductive hypothesis on $A\types B_1$ and $B_1\types C$, we can derive $A\types C$.
      \label{item:mslat-principal-cut}\qedhere
    \end{enumerate}
  \end{enumerate}
\end{proof}

This simple proof already displays many of the characteristic features of a cut-admissibility argument.
The final case~\ref{item:mslat-principal-cut} is called the \textbf{principal case} for the operation $\meet$, when the type $B$ we are composing over (also called the \textbf{cut formula}) is obtained from $\meet$ and both sequents are also obtained from the $\meet$ rules.

\begin{rmk}
  It may seem somewhat odd that we can prove the admissibility of all cuts (compositions), but we have to assert identities as a primitive rule for base/generating types.
  This is essentially because we chose to ``build a cut'' into the rule $fR$ that represents the generating arrows.
  If we had not, then we would have to assert ``cuts over base types'' (that is, where the cut formula is an object of \cG) as primitive rules, the way we did in the cut-ful theory of \cref{sec:category-cutful}.
  Put differently, building a cut into $fR$ is essentially the ``morphism version'' of asserting identities primitively for base types.
\end{rmk}

Finally, we have the initiality theorem:

\begin{thm}\label{thm:seqcalc-mslat-initial}
  For any relational graph $\cG$, the free meet-semilattice $\F\bmSLat \cG$ it generates is described by the unary sequent calculus for meet-semilattices under \cG: its objects are the $A$ such that $\types A\type$ is derivable, with $A\le B$ just when $A\types B$ is derivable.
\end{thm}
\begin{proof}
  \cref{thm:seqcalc-mslat-idadm,thm:seqcalc-mslat-cutadm} show that this defines a poset; let us denote it $F\cG$.
  The rule $\top R$ implies that $\top$ is a top element, while the rules $\meetL1$, $\meetL2$, and $\meetR$ imply that $A\meet B$ is a binary meet.
  Therefore, we have a meet-semilattice.
  Moreover, the rules $\idfunc$ and $f$ yield a map of posets $\cG\to F\cG$.

  Now suppose $\cM$ is any other meet-semilattice with a map $P:\cG\to\cM$.
  Recall that a meet-semilattices is equipped with a chosen top element and meet function.
  We extend $P$ to a map from the objects of $F\cG$ by recursion on the construction of the latter, sending $\top$ to the chosen top element of \cM, and $A\meet B$ to the chosen meet in \cM of the (recursively defined) images of $A$ and $B$.
  This is clearly the only possible meet-semilattice map extending $P$, and it clearly preserves the chosen meets and top element, so it suffices to check that it is a poset map.
  This follows by a straightforward induction over the rules for deriving the judgment $A\types B$.
\end{proof}

To finish, we observe that this sequent calculus has another important property.
Inspecting the rules, we see that the operations $\meet$ and $\top$ only ever appear in the \emph{conclusions} of rules.
Each operation $\meet$ and $\top$ has zero or more rules allowing us to introduce it on the right of the conclusion, and likewise zero or more rules allowing us to introduce it on the left.
(Specifically, $\meet$ has two left rules and one right rule, while $\top$ has zero left rules and one right rule.)
This is convenient if we are given a sequent $A\types B$ and want to figure out whether it is derivable: we can choose rules to apply ``in reverse'' by breaking down $A$ and $B$ according to their construction out of $\meet$ and $\top$.

It also tells us nontrivial things about derivations.
For instance, all the primitive rules have the property that every type appearing in their premises also appears as a sub-expression of some type in their conclusion.
Thus, any (cut-free) \emph{derivation} of a sequent $A\types B$ must involves only types appearing as sub-expressions of $A$ and $B$.
This is called the \textbf{subformula property}.

The phrase \emph{sequent calculus}, like \emph{type theory}, is difficult to define precisely, but sequent calculi generally exhibit the properties we have observed in this subsection: admissibility of the identity rule (based on an axiom applying only to base types), admissibility of cut, type operations appearing only in the conclusions of rules, and the subformula property.

\subsection{Natural deduction for meet-semilattices}
\label{sec:natded-mslat}

Now suppose we make the other choice about how to treat projections.
We call this the \textbf{unary natural deduction for meet-semilattices under \cG}; its rules (in addition to those for $\types A\type$) are
\begin{mathpar}
  \inferrule{\types X\type}{X\types X}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ X\types A}{X\types B}\;fI\and
  \inferrule{\types X\type}{X\types \top}\;\top I\and
  \inferrule{X\types B\meet C}{X \types B}\;\meetE1\and
  \inferrule{X\types B\meet C}{X \types C}\;\meetE2\and
  \inferrule{X\types B \\ X\types C}{X\types B\meet C}\;\meetI
\end{mathpar}

We observe first that this theory has the same well-formedness property as the sequent calculus:

\begin{thm}\label{thm:natded-mslat-wftype}
  In the unary natural deduction for meet-semilattices under \cG, if $A\types B$ is derivable, then so are $\types A\type$ and $\types B\type$.\qed
\end{thm}

Unlike the sequent calculus, however, the general identity rule is not admissible: there is no way to derive $A\meet B \types A\meet B$ from $A\types A$ and $B\types B$ without it.
Thus, we assert the $\idfunc$ for all types, not just those coming from \cG.

Cut, however, is still admissible:

\begin{thm}\label{thm:natded-mslat-cutadm}
  In the unary natural deduction for meet-semilattices under \cG, if $A\types B$ and $B\types C$ are derivable, then so is $A\types C$.
\end{thm}
\begin{proof}
  We induct on the derivation of $B\types C$.
  \begin{enumerate}
  \item The cases when it ends with $\idfunc$, $f$, $\top I$, and $\meetI$ are just like those in \cref{thm:seqcalc-mslat-cutadm} for $\idfunc$, $f$, $\top R$, and $\meetR$.
  \item If it ends with $\meetE1$, then we have $B\types C\meet D$ for some $D$.
    Thus, $A\types C\meet D$ by the inductive hypothesis, so $A\types C$ by $\meetE1$.
    The case of $\meetE2$ is similar.\qedhere
  \end{enumerate}
\end{proof}

The proof is noticeably simpler than that of \cref{thm:seqcalc-mslat-cutadm}; we don't need the secondary inner induction.
This is essentially due to the fact that all the rules of this theory involve an \emph{arbitrary} type $X$ on the left (rather than one built using operations such as $\meet$).
Thus, instead of the rules of sequent calculus that introduce operations like $\meet$ and $\top$ on the left and right, we have rules like $\top I$ and $\meetI$ that introduce them on the right, and also rules that \emph{eliminate} them on the right like $\meetE1$ and $\meetE2$.
These properties are characteristic of \emph{natural deduction} theories.
(Later on we will be able to give a more convincing explanation of the origin of the phrase ``natural deduction''.)

\begin{rmk}
  Because the proof of cut-elimination for natural deduction theories is so much simpler than that for sequent calculus, some people say that the former is ``trivial''.
  Triviality is subjective; but what seems inarguable is that cut-elimination for natural deduction is \emph{saying something different} than cut-elimination for sequent calculus.
  The content of cut-elimination for sequent calculus corresponds more closely to $\beta$-reduction in natural deduction (see \cref{sec:beta-eta}), or to the cut-elimination (``hereditary substitution'') of a canonical/atomic natural deduction (see \cref{sec:atomcan}).
  This difference is also evident in the fact that we have to assert the full identity rule in natural deduction.
\end{rmk}

Of course, we should also prove the initiality theorem:

\begin{thm}\label{thm:natded-mslat-initial}
  For any relational graph $\cG$, the free meet-semilattice $\F\bmSLat \cG$ it generates is described by the unary natural deduction for meet-semilattices under \cG: its objects are the $A$ such that $\types A\type$ is derivable, with $A\le B$ just when $A\types B$ is derivable.
\end{thm}
\begin{proof}
  Almost exactly like \cref{thm:seqcalc-mslat-initial}.
\end{proof}

\subsection*{Exercises}

\begin{ex}\label{ex:mslat-idem}
  Using the unary sequent calculus for meet-semilattices, prove that $A\meet A \cong A$ for any object $A$ of any meet-semilattice.
  (Recall that meet-semilattices are categories with at most one morphism in each hom-set, so for two objects to be isomorphic it suffices to have a morphism in each direction.)
  Then prove the same thing using the natural deduction.
\end{ex}

\begin{ex}\label{ex:mslat-monoid}
  Using either the sequent calculus or the natural deduction for meet-semilattices (your choice), prove that in any meet-semilattice we have
  \begin{mathpar}
    A\meet \top \cong A\and
    \top \meet A \cong A\and
    A\meet B \cong B\meet A\and
    A\meet (B\meet C) \cong (A\meet B)\meet C\and
  \end{mathpar}
\end{ex}

\begin{ex}\label{ex:mslat-invertible}
  Prove that the rules $\top R$ and $\meetR$ in the unary sequent calculus for meet-semilattices are \emph{invertible}, in the sense that whenever we have a derivation of their conclusions, we also have a derivation of all their premises.
\end{ex}

\begin{ex}\label{ex:jslat}
  Describe a sequent calculus for \emph{join-semilattices} (posets with a bottom element and binary joins), and prove the initiality theorems for it (including the identity and cut admissibility theorems).
  The rules for $\bot$ and $\join$ should be exactly dual to the rules for $\top$ and $\meet$.
\end{ex}

\begin{ex}\label{ex:lattices}
  By putting together the rules for meet- and join-semilattices, describe a sequent calculus for \emph{lattices} (posets with a top and bottom element and binary meets and joins), and prove the initiality theorem.
\end{ex}

\begin{ex}\label{ex:lattices-invertible}
  Prove that in your sequent calculus for lattices from \cref{ex:lattices}, the rules $\top R$, $\meet R$, $\bot L$, and $\join L$ are all invertible in the sense of \cref{ex:mslat-invertible}.
\end{ex}

\begin{ex}\label{ex:seqcalc-poset-fib}
  A map of posets $P:\sA\to\sM$ is called a \emph{(cloven) fibration} if whenever $b\in\sA$ and $x\le P(b)$, there is a chosen $a\in \sA$ such that $P(a)=x$ and $a\le b$ and moreover for any $c\in\sA$, $c\le b$ and $P(c)\le x$ together imply $c\le a$.
  The object $a$ can be written as $x^*(b)$.
  \begin{enumerate}
  \item Given a fixed poset \sM, describe a sequent calculus for fibrations over \sM by adding rules governing the operations $x^*$ to the cut-free theory of \cref{ex:categories-over}.
  \item Prove the initiality theorem for this sequent calculus.
  \item Use this sequent calculus to prove that in any fibration $P:\sA\to\sM$, if we have $b\in \sA$ and $x\le y\le P(b)$, then $x^*(y^*(b))\cong x^*(b)$.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:natded-poset-fib}
  Now describe instead a natural deduction for fibrations over \sM, prove the initiality theorem, and re-prove that $x^*(y^*(b))\cong x^*(b)$ using this theory.
\end{ex}

\begin{ex}\label{ex:mslat-fib}
  Suppose we augment your sequent calculus for fibrations over \sM from \cref{ex:seqcalc-poset-fib} with the following additional rules for ``fiberwise meets''.
  Here $\types A\type_x$ is a judgment indicating that $A$ will be an object of our fibration in the fiber over $x\in\sM$.
  \begin{mathpar}
    \inferrule{ }{\types \top_x \type_x} \and
    \inferrule{\types A\type_x \\ \types B\type_x}{\types A\meet_x B\type_x} \and
    \inferrule{\types A\type_x}{A\types_{x\le y} \top_y}\and
    \inferrule{A\types_{x\le y} C \\ \types B\type_x}{A\meet_x B \types_{x\le y} C}\and
    \inferrule{B\types_{x\le y} C \\ \types A\type_x}{A\meet_x B \types_{x\le y} C}\and
    \inferrule{A\types_{x\le y} B \\ A\types_{x\le y} C}{A\types_{x\le y} B\meet_y C}\and
  \end{mathpar}
  Consider the sequents
  \begin{align*}
    x^*(A\meet_y B) &\types_{x\le x} x^*A \meet_x x^*B\\
    x^*A \meet_x x^*B &\types_{x\le x} x^*(A\meet_y B)
  \end{align*}
  for $x\le y$, $\types A\type_y$, and $\types B\type_y$.
  \begin{enumerate}
  \item Construct derivations of these sequents in the above sequent calculus.
  \item Write down an analoguous natural deduction and derive the above sequents therein.
  \item What categorical structure do you think these type theories construct the initial one of?
    If you feel energetic, prove the initiality theorem.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:poset-bifib}
  A map of posets $P:\sA\to\sM$ is called an \emph{opfibration} if $P\op:\sA\op\to\sM\op$ is a fibration.
  The analogous operation takes $a\in \sA$ and $P(a)\le y$ to a $b\in \sA$ with $P(b)=y$ and $a\le b$ and a universal property; we write this $b$ as $y_!(a)$.
  We say $P$ is a \emph{bifibration} if it is both a fibration and an opfibration.
  Describe a sequent calculus for bifibrations over a fixed \sM, and prove the initiality theorem.
\end{ex}

\begin{ex}\label{ex:poset-bifib-adj}
  Use your sequent calculus from \cref{ex:poset-bifib} to prove that in a bifibration of posets, if $x\le y$ in $\sM$, we have an adjunction $y_! \dashv x^*$.
\end{ex}

\begin{ex}\label{ex:poset-bifib-iso}
  Use your sequent calculus from \cref{ex:poset-bifib} to prove that in a bifibration of posets, if $x\cong y$ in $\sM$, we have an isomorphism $x_! \cong x^*$ (that is, for any $a$ in the fiber over $y$, we have $x_!(a) \cong x^*(a)$).
\end{ex}


\section{Categories with products}
\label{sec:catprod}\label{sec:beta-eta}

Now we move back from posets to categories.
Let \bPrCat be the category of categories with specified binary products and a specified terminal object, and functors preserving these strictly.
Then we have an adjunction relating \bPrCat to \bGr, and we want to describe the left adjoint with a type theory.

As in \cref{sec:natded-mslat}, we could take either the sequent calculus route or the natural deduction route.
Unfortunately, even if we build in enough composition to make cut admissible, in \emph{both} cases we need to impose a further equivalence relation on the derivations, as there are single morphisms that can be derived in multiple ways.
However, the ways in which this happens in the two cases are different.

On one hand, if we have an arrow $f:A\to C$ in a directed graph \cG, then there is a morphism $A\times B \to A \to A\times C$ in the free category-with-products on \cG.
In a sequent calculus, there are two distinct derivations of this morphism:
\begin{mathpar}
  \inferrule*{
    \inferrule*{A\types A}{A\times B \types A}\\
    \inferrule*{\inferrule*[Right=$f$]{A\types A}{A\types C}}{A\times B \types C}
  }{
    A\times B \types A\times C
  }\and
  \inferrule*{
    \inferrule*{A\types A \\ \inferrule*[Right=$f$]{A\types A}{A\types C}}{A\types A\times C}
  }{
    A\times B \types A\times C
  }\and
\end{mathpar}
whereas in a natural deduction there will be only one:
\begin{mathpar}
  \inferrule*{
    \inferrule*{A\times B\types A\times B}{A\times B \types A}\\
    \inferrule*[Right=$f$]{\inferrule*{A\times B\types A\times B}{A\times B\types A}}{A\times B \types C}
  }{
    A\times B \types A\times C
  }
\end{mathpar}
This sort of thing is true quite generally.
A sequent calculus includes both left and right rules, so to derive a given sequent we must choose whether a left or a right rule is to be applied last.
By contrast, both kinds of rules in a natural deduction (introduction and elimination) act on the right, so there is less choice about what rule to apply last.

On the other hand, if we have an arrow $A\to B$ in \cG, then in a natural deduction there are (at least) two derivations of the identity $A\to A$:
\begin{mathpar}
  \inferrule*{\inferrule*{A\types A \\ \inferrule*{A\types A}{A\types B}}{A\types A\times B}}{A\types A}\and
  \inferrule*{ }{A\types A}\and
\end{mathpar}
while in a sequent calculus there is only one:
\begin{mathpar}
  \inferrule*{ }{A\types A}
\end{mathpar}
This is also true quite generally.
A natural deduction includes both introduction and elimination rules, so we will always be able to introduce a type and then eliminate it, essentially ``doing nothing''.
By contrast, in a sequent calculus we have ``only introduction rules'' (of both left and right sorts), so this cannot happen.

In fact, in the simple case of categories with products, there are tricks enabling us to eliminate \emph{both} kinds of redundancy, and thereby do without any ``$\equiv$'' for both the sequent calculus and the natural deduction.
However, in even slightly more complicated theories the analogous tricks do not eliminate $\equiv$ completely, though they do reduce its complexity.
Thus, we focus in this section on theories with $\equiv$, relegating the tricks to the ``chapter appendices'' \cref{sec:atomcan,sec:focusing}.

It is convenient to describe the reduction rules using a term syntax for derivations, as introduced in \cref{sec:category-cutadm}.
If we think of $A\times B$ as the ``type of pairs consisting of one element of $A$ and one element of $B$'', then it is natural to represent the $\timesI/\timesR$ rule as a pair of terms, one in $A$ and one in $B$.
This idea leads us to write the natural deduction rules from \cref{sec:natded-mslat} as:
\begin{mathpar}
  \inferrule{\types X\type}{x:X\types x:X}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ x:X\types M:A}{x:X\types f(M):B}\;fI\and
  \inferrule{\types X\type}{x:X\types \ttt:\unit}\;\unit I\and
  \inferrule{x:X\types M:B\times C}{x:X \types \pi_1(M):B}\;\timesE1\and
  \inferrule{x:X\types M:B\times C}{x:X \types \pi_2(M):C}\;\timesE2\and
  \inferrule{x:X\types M:B \\ x:X\types N:C}{x:X\types \pair MN:B\times C}\;\timesI
\end{mathpar}
Note how well the natural-deduction choice of ``all rules acting on the right'' matches the use of abstract variables: in all cases we can think of ``applying functions to arguments'' in a familiar way.
It is possible to describe sequent calculus derivations using terms as well, but they are less pretty.
For this reason, we will henceforth use sequent calculus only for posetal theories, with a brief exception in \cref{sec:focusing}.

The need to impose the identity rule for all types (not just those coming from \cG) also makes perfect sense from the abstract variable standpoint: a variable in any type is also a term of that type.

Now the above equivalence between the two derivations of the identity $A\to A$ can be written as
\begin{equation}
  \pi_1(\pair M N) \equiv M\label{eq:beta-prodcat-1}
\end{equation}
and of course we should also have
\begin{equation}
  \pi_2(\pair M N) \equiv N\label{eq:beta-prodcat-2}
\end{equation}
Note that in these equalities we allow $M$ and $N$ to be arbitrary terms.
Categorically speaking, therefore, we are asserting that the maps $X\to A\times B$ induced by the universal property of the product (the $\timesI$ rule) do in fact have the desired composites with the projections.

The other half of the universal property is the uniqueness of maps into a product.
This corresponds to a dual family of simplifications: we want to identify the following derivations of $A\times B\to A\times B$.
\begin{mathpar}
  \inferrule*{
    \inferrule*{\inferrule*{ }{A\times B\types A\times B}}{A\times B \types A} \\
    \inferrule*{\inferrule*{ }{A\times B\types A\times B}}{A\times B \types B}
  }{
    A\times B \types A\times B
  }\and
  \inferrule*{ }{A\times B\types A\times B}
\end{mathpar}
In term syntax, this means that
\begin{equation}
  \pair{\pi_1(M)}{\pi_2(M)} \equiv M\label{eq:eta-prodcat}
\end{equation}
In type-theoretic lingo, the equalities~\eqref{eq:beta-prodcat-1} and~\eqref{eq:beta-prodcat-2} are called \textbf{$\beta$-conversion} while the equality~\eqref{eq:eta-prodcat} is called an \textbf{$\eta$-conversion}.
They generate an equivalence relation on terms, which we also require to be a congruence for everything else.

We can describe this more formally with an additional judgment ``$x:X\types M\equiv N :A$'', with rules shown in  \cref{fig:catprod-equality}.
Note that in addition to the $\beta$- and $\eta$-conversions, we assert reflexivity, symmetry, and transitivity, and also that all the previous rules preserve equality.
As remarked in \cref{sec:category-cutful}, all our equality judgments $\equiv$ will be equivalence relations with such a congruence property for all the primitive rules.
In general we will not bother to state these ``standard'' rules for $\equiv$, but since this is our first encounter with such a relation involving ``abstract variable'' term syntax we have included them explicitly.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{x:X\types M:A \\ x:X\types N:B}{x:X\types \pi_1(\pair M N) \equiv M :A}\and
    \inferrule{x:X\types M:A \\ x:X\types N:B}{x:X\types \pi_2(\pair M N) \equiv N :B}\and
    \inferrule{x:X\types M:A\times B}{x:X\types \pair{\pi_1(M)}{\pi_2(M)} \equiv M:A\times B}\and
    \inferrule{x:X\types M:A}{x:X \types M\equiv M:A}\and
    \inferrule{x:X\types M\equiv N:A}{x:X\types N\equiv M:A}\and
    \inferrule{x:X\types M\equiv N:A \\ x:X\types N\equiv P:A}{x:X\types M\equiv P:A}\and
    \inferrule{f\in \cG(A,B) \\ x:X\types M\equiv N:A}{x:X\types f(M)\equiv f(N):B}\and
    \inferrule{x:X\types M\equiv N:B\times C}{x:X \types \pi_1(M)\equiv \pi_1(N):B}\and
    \inferrule{x:X\types M\equiv N:B\times C}{x:X \types \pi_2(M)\equiv \pi_2(N):B}\and
    \inferrule{x:X\types M\equiv M':B \\ x:X\types N\equiv N':C}{x:X\types \pair MN\equiv \pair{M'}{N'}:B\times C}
  \end{mathpar}
  \caption{Equality rules for categories with products}
  \label{fig:catprod-equality}
\end{figure}

This completes the definition of the \textbf{unary type theory for categories with products under \cG}.

\begin{rmk}\label{rmk:beta-reduction}
  Note that unlike the equalities $h\circ (g\circ f) \equiv (h\circ g)\circ f$ from \cref{sec:category-cutful}, the $\beta$- and $\eta$-conversions are intutively ``directional'', with one side being ``simpler'' than the other.
  This suggests that we should be able to ``reduce'' an arbitrary term to a ``simplest possible form'' by successively applying $\beta$- and $\eta$-conversions.
  Such is indeed the case (although for technical reasons the $\eta$-conversion is usually applied in the less intuitive right-to-left direction and called an ``expansion'' rather than a ``reduction''.).
  This \emph{process of reduction} (and expansion) belongs to the ``computational'' side of type theory, which (though of course important in its own right) is somewhat tangential to our category-theoretic emphasis, so we will not discuss it in detail.
  (However, in \cref{sec:atomcan} we will directly characterize the \emph{result of reduction}, and thereby implicitly the process thereof.)
\end{rmk}

\begin{thm}\label{thm:catprod-subadm}
  Substitution is admissible in the unary type theory for categories with products under \cG.
  That is, if we have derivations of $x:A\types M:B$ and $y:B \types N:C$, then we have a derivation of $x:A \types N[M/y]:C$.
\end{thm}
\begin{proof}
  We induct on the derivation of $y:B \types N:C$.
  \begin{enumerate}
  \item If it ends with $\idfunc$, then we can use the given derivation $M$.
  \item If it ends swith $fI$ for $f\in \cG(C',C)$, then we have $y:B \types N':C'$, so by induction we have $x:A\types N'[M/y]:C'$ and hence $x:A\types f(N'[M/y]):C'$.
  \item If it ends with $\unit I$, then by $\unit I$ we have $x:A \types \ttt:\unit$ as well.
  \item If it ends with $\timesE1$, then we have $y:B\types N':C\times C'$, so by induction we have $x:A \types N'[M/y]:C\times C'$, hence $x:A \types \pi_1(N'[M/y]):C$ by $\timesE1$.
    The case for $\timesE2$ is similar.
  \item Finally, if it ends with $\timesI$, we have $y:B\types N_1:C_1$ and $y:B\types N_2:C_2$, so by induction we have $x:A \types N_1[M/y]:C_1$ and $x:A \types N_2[M/y]:C_2$, hence $x:A \types \pair{N_1[M/y]}{N_2[M/y]}:C_1\times C_2$.\qedhere
  \end{enumerate}
\end{proof}

As with \cref{thm:category-cutadm}, this proof can be regarded as \emph{defining} recursively what it means to ``substitute'' $M$ for $y$ in $N$.
The defining clauses are
\begin{align*}
  N[M/y] &= M\\
  (f(N'))[M/y] &= f(N'[M/y])\\
  \ttt[M/y] &= \ttt\\
  (\pi_1(N'))[M/y] &= \pi_1(N'[M/y])\\
  (\pi_2(N'))[M/y] &= \pi_2(N'[M/y])\\
  \pair{N_1}{N_2}[M/y] &= \pair{N_1[M/y]}{N_2[M/y]}
\end{align*}

Similarly, we also have:

\begin{thm}\label{thm:catprod-subcong}
  The relation $\equiv$ is a congruence for substitution in the unary type theory for categories with products under \cG.
  In other words, if we have derivations of $x:X \types M\equiv M':B$ and $y:B \types N\equiv N':C$, then we can derive $x:A \types N[M/y] \equiv N'[M'/y]:C$.\qed
\end{thm}

\begin{thm}\label{thm:catprod-subassoc}
  Substitution is associative in the unary type theory for categories with products under \cG: we have $P[N/z][M/y] = P[N[M/y]/z]$.\qed
\end{thm}

\begin{thm}\label{thm:catprod-initial}
  For any directed graph \cG, the free category-with-products $\F\bPrCat \cG$ it generates is described by the unary type theory for categories with products under \cG: its objects are the $A$ such that $\types A\type$ is derivable, and its morphisms $A\to B$ are the terms $M$ such that $x:A \types M:B$ is derivable, modulo the equivalence relation $\equiv$.\qed
\end{thm}

The proofs of these theorems are left to the reader in \cref{ex:catprod-subcong,ex:catprod-subassoc,ex:catprod-initial}.

\subsection*{Exercises}

\begin{ex}\label{ex:catprod-functor-uniq}
  Suppose we have
  \begin{mathpar}
    f\in\cG(A,B)\and
    g\in\cG(A,C)\and
    h\in\cG(B,D)\and
    k\in\cG(C,E)
  \end{mathpar}
  Consider the following two derivations of $A\types D\times E$.
  Note that both use the admissible cut/substitution rule.
  \begin{mathpar}
    \let\mytimes\times
    \def\times{\mathord{\mytimes}}
    \inferrule*[Right=$\timesI$]{
      \inferrule*[right=cut]{\inferrule*[Right=$f$]{\inferrule*{ }{A\types A}}{A\types B}\\
        \inferrule*[Right=$h$]{\inferrule*{ }{B\types B}}{B\types D}}{A\types D}\\
      \inferrule*[Right=cut]{\inferrule*[Right=$g$]{\inferrule*{ }{A\types A}}{A\types C}\\
        \inferrule*[Right=$k$]{\inferrule*{ }{C\types C}}{C\types E}}{A\types E}
    }{A\types D\times E}\and
    \inferrule*[Right=cut]{
      \inferrule*[right=$\timesI$]{
        \inferrule*[Right=$f$]{\inferrule*{ }{A\types A}}{A\types B}\\
        \inferrule*[Right=$g$]{\inferrule*{ }{A\types A}}{A\types C}
      }{A\types B\times C}\\
      \inferrule*[right=$\timesI$]{
        \inferrule*[Right=$h$]{\inferrule*[Right=$\timesE1$]{
            \inferrule*{ }{B\times C\types B\times C}
          }{B\times C\types B
          }}{B\times C\types D}\\
        \inferrule*[Right=$k$]{\inferrule*[Right=$\timesE2$]{
            \inferrule*{ }{B\times C\types B\times C}
          }{B\times C\types C
          }}{B\times C\types E}
      }{B\times C\types D\times E}
    }{A\types D\times E}\and
  \end{mathpar}
  Write down the terms corresponding to these two derivations and show directly that they are related by $\equiv$.
\end{ex}

\begin{ex}
  Use the type theory for categories with products to prove that in any category with products we have
  \begin{mathpar}
    A\times B \cong B\times A\and
    A\times (B\times C) \cong (A\times B)\times C\and
    A\times \unit \cong A\and
    \unit \times A \cong A.
  \end{mathpar}
  Note that since we are in categories now rather than posets, to show that two types $A$ and $B$ are isomorphic we must derive $x:A\types M:B$ and $y:B\types N:A$ and also show that their substitutions in both orders are equal (modulo $\equiv$) to identities.
\end{ex}

\begin{ex}\label{ex:catprod-subcong}
  Prove \cref{thm:catprod-subcong} ($\equiv$ is a congruence in the unary type theory for categories with products).
\end{ex}

\begin{ex}\label{ex:catprod-subassoc}
  Prove \cref{thm:catprod-subassoc} (substitution is associative in the unary type theory for categories with products).
\end{ex}

\begin{ex}\label{ex:catprod-initial}
  Prove \cref{thm:catprod-initial} (initiality for the unary type theory for categories with products).
\end{ex}

\begin{ex}\label{ex:catfib}
  A functor $P:\sA\to \sM$ is called a \textbf{fibration} if for any $b\in \sA$ and $f:x\to P(b)$, there exists a morphism $\phi:a\to b$ in \sA such that $P(\phi)=f$ and $\phi$ is \emph{cartesian}, meaning that for any $\psi:c\to b$ and $g:P(c)\to x$ such that $P(\psi)=fg$, there exists a unique $\chi:c\to a$ such that $P(\chi)=g$ and $\phi\chi=\psi$.
  The object $c$ is denoted $f^*(b)$.
  \begin{enumerate}
  \item Generalize your natural deduction for fibrations of posets from \cref{ex:natded-poset-fib} to a type theory for fibrations of categories over a fixed base category \sM, with $\beta$- and $\eta$-conversion $\equiv$ rules.
  \item Prove the initiality theorem for this type theory.
  \item Use this type theory to prove that in any fibration $P:\sA\to\sM$:
    \begin{enumerate}
    \item For any $f:x\to y$ in \sM, $f^*$ is a functor from the fiber over $y$ to the fiber over $x$.
    \item For any $B\in \sA$ and $x\xto{f} y \xto{g} P(B)$ in \sM, we have $f^*(g^*(B)) \cong (gf)^*(M)$.
    \end{enumerate}
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:catprod-fib}
  Generalize \cref{ex:mslat-fib} from posets to categories, combining your type theory from \cref{ex:catfib} with the one for categories with products from \cref{sec:beta-eta}.
\end{ex}


\section{Categories with coproducts}
\label{sec:catcoprod}

% [TODO: I'm unsure whether this should be here at all.
% It seems a natural thing to expect, but on the other hand it involves variable binding in terms, which might be easier to introduce first with simply-typed $\lambda$-calculus in \cref{chap:simple}.
% Note that we also need variable binding for monoidal categories, so if we want to do those before cartesian monoidal categories (as also seems natural, since multicategories are simpler than cartesian multicategories) we need to discuss variable binding sooner than $\lambda$-calculus.]

In \cref{ex:jslat}, you obtained a \emph{sequent calculus} for join-semilattices by dualizing the sequent calculus for meet-semilattices.
However, \emph{natural deductions} don't dualize as straightforwardly, due to the insistence that all rules act only on the right.
(Of course, we could dualize them to ``co-natural deductions'' in which all rules act only on the \emph{left}, but that would destroy the familiar behavior of terms on variables, as well as make it tricky to combine left and right universal properties, such as for lattices.)
To describe joins in a natural deduction, we need to ``build an extra cut'' into their universal property:
\begin{mathpar}
  \inferrule{X\types A}{X\types A\join B}\;\joinI1\and
  \inferrule{X\types B}{X\types A\join B}\;\joinI2\and
  \inferrule{A\types C \\ B\types C \\ X\types A\join B}{X\types C}\;\joinE
\end{mathpar}
Note that $\joinE$ is precisely the result of cutting $\joinL$ with an arbitrary sequent:
\begin{mathpar}
  \inferrule*[Right=cut]{X\types A\join B \\ \inferrule*[Right=$\joinL$]{A\types C \\ B\types C}{A\join B\types C}}{X\types C}
\end{mathpar}
We treat the bottom element similarly:
\begin{mathpar}
  \inferrule{X\types \bot \\\types C\type}{X\types C}
\end{mathpar}

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\types X\type}{x:X\types x:X}\;\idfunc\and
    \inferrule{f\in \cG(A,B) \\ x:X\types M:A}{x:X\types f(M):B}\;fI\and
    \inferrule{x:X \types M:\zero \\ \types C\type}{x:X \types \abort(M):C}\;\zeroE\\
    \inferrule{x:X\types M:A}{x:X\types \inl(M):A + B}\;\plusI1\and
    \inferrule{x:X\types N:B}{X\types \inr(N):A + B}\;\plusI2\and
    \inferrule{u:A\types P:C \\ v:B\types Q:C \\ x:X\types M:A + B}{x:X\types \case(M,u.P,v.Q):C}\;\plusE
  \end{mathpar}
  \caption{Unary type theory for categories with coproducts}
  \label{fig:catcoprod}
\end{figure}

Rather than take the time to analyze a natural deduction for join-semilattices, we skip directly to the analogous \textbf{unary type theory for categories with coproducts} in which we annotate the judgments with terms, shown in \cref{fig:catcoprod}.
The two injections $A\to A+B$ and $B\to A+B$ are named $\inl$ and $\inr$ (``in-left'' and ``in-right''), while the unique morphism $\zero \to C$ is called $\abort$.
The term $\case(M,u.P,v.Q)$ appearing in the rule $\plusE$, however, probably requires some explanation.

We should start by saying that this term is parsed as $\case(M,(u.P),(v.Q))$: that is, the periods bind more tightly than the commas.
Now the idea is that $\case(y,f,g)$ denotes the image of $y:A+B$ under the morphism $A+B\to C$ determined by $f:A\to C$ and $g:B\to C$ (the ``coparing'', sometimes denoted $[f,g]$).
The rule mentions an arbitrary term $M:A+B$ rather than a variable $y$ simply because it has to build in a cut.
The morphisms $f$ and $g$, however, are specified (like all morphisms in unary type theory) using term syntax, $u:A\types P:C$ and $v:A\types Q:C$ --- and, as always, the specific variables $u$ and $v$ used are irrelevant (judgments only have meaning up to $\alpha$-equivalence).
Thus we write $u.P$ and $v.Q$ in the $\case$ term, rather than simply $P$ and $Q$, to indicate what variable is being used.
These variables $u$ and $v$ are ``bound'', like the integration variable in a definite integral: $\case(M,u.f(u),v.g(v))$ has the same meaning as $\case(M,w.f(w),z.g(z))$.

\begin{rmk}
  In unary type theory it is not strictly necessary to add the prefixes ``$u.$'' and ``$v.$'' --- as long as variables are a syntactic group disjoint from all other symbols, the variable $u$ is determined as the only variable occurring in $P$, and similarly for $v$ in $Q$ (and if no variables occur in $P$, it doesn't matter what variable $u$ we mean).
  However, starting in \cref{chap:simple} it will be important to keep track of which variable is being bound, so we establish good habits now.
\end{rmk}

Now that we understand the $\case$ syntax, we can write down the $\beta$-conversion rules.
Dualizing the $\beta$-conversion rules for products, the $\beta$-conversion rules for coproducts should say that the map $A+B\to C$ induced by $f:A\to C$ and $g:B\to C$ yields $f$ and $g$ when composed with the coproduct injections.
Recalling that composition is given by substitution, this leads us to write down
\begin{align*}
  \case(\inl(y),u.P,v.Q) &\equiv P[y/u]\\
  \case(\inr(z),u.P,v.Q) &\equiv Q[z/v]
\end{align*}
Similarly, the $\eta$-conversion rule\footnote{In fact, for types such as coproducts with a left universal property, there is no consensus on exactly what equality ``$\eta$'' refers to.\label{fn:weak-eta}
  From a categorical point of view this equality is the most natural, since like the $\eta$-conversion rule for products it expresses the uniqueness aspect of the universal property.
  But sometimes $\eta$ is used to refer only to the special case $\case(y,u.\inl(u),v.\inr(v))\equiv y$, which is also analogous to the $\eta$ rule for products in that it says that elements \emph{of the type in question} have a canonical form (a pair in a product or a case-split in a coproduct).
  The relationship between these two, which involves ``commuting conversions'', is discussed further in \cref{sec:atomcan}.} should say that morphisms out of a coproduct are determined uniquely by their composites with the projections:
\begin{equation}
  \case(y,u.P[\inl(u)/y],v.P[\inr(v)/y]) \equiv P\label{eq:catcoprod-eta}
\end{equation}
In fact, to ensure that $\equiv$ is a congruence, we should ``build in a cut'' to all of these rules, so that the antecedent of the conclusion is an arbitrary type.
Thus the actual generating $\equiv$ rules are
\begin{mathpar}
  \inferrule{u:A\types P:C \\ v:B\types Q:C \\ x:X\types M:A}{x:X \types \case(\inl(M),u.P,v.Q) \equiv P[M/u] : C}
  \and
  \inferrule{u:A\types P:C \\ v:B\types Q:C \\ x:X\types N:B}{x:X \types \case(\inr(N),u.P,v.Q) \equiv Q[N/v] : C}
  \and
  \inferrule{x:X \types M:A+B \\ y:A+B \types P:C}{x:X \types \case(M,u.P[\inl(u)/y],v.P[\inr(v)/y]) \equiv P[M/y] : C}
\end{mathpar}
As usual, we also require $\equiv$ to be an equivalence relation and a congruence for substitution.
This completes the definition of our \textbf{unary type theory for categories with coproducts under \cG}.

\begin{lem}\label{thm:catcoprod-subadm}
  Substitution is admissible in the unary type theory for categories with coproducts under \cG.
  That is, if we have derivations of $x:A\types M:B$ and $y:B\types N:C$, we can construct a derivation of $x:A\types N[M/y]:C$.
\end{lem}
\begin{proof}
  By induction over derivations.
  As usual for natural deduction theories, there is not much happening: for each rule that might appear last in the derivation of $y:B\types N:C$, we apply the inductive hypothesis to its premises and then re-apply the final rule.
\end{proof}

\begin{thm}\label{thm:catcoprod-initial}
  For any directed graph \cG, the free category with coproducts generated by \cG can be described by the unary type theory for categories with coproducts under \cG: its objects are the $A$ such that $\types A\type$ is derivable, and its morphisms $A\to B$ are the derivable judgments $x:A\types M:B$, modulo the equivalence relation $\equiv$.
\end{thm}
\begin{proof}
  \cref{thm:catcoprod-subadm} defines substitution, and the same sort of induction proves it associative and unital; thus we have a category $\F\bCoprCat\cG$.
  The rules are defined just so as to give this category the structure of finite coproducts.

  Now if \cM is a category with (specified) finite coproducts and $P:\cG\to\cM$ is a map of graphs, we extend it to the objects of $\F\bCoprCat\cG$ in the obviously unique way (by induction on derivations of $\types A\type$), and to its morphisms by induction on derivations of $x:A\types M:B$.
  All the non-identity rules are defined by a cut with a generator or one of the coproduct structure maps; thus their images in \cM are uniquely determined by the need for the extension to be a functor and to preserve the specified coproducts.
  Finally, we show by a further induction that this extension is actually a functor (preserves all composites).
\end{proof}

This is the end of the main material in \cref{chap:unary}; the reader is free (after doing some exercises) to move on directly to \cref{chap:simple}.
However, if you want to understand $\equiv$ a little more deeply, you may also continue with the ``chapter appendices'' \cref{sec:atomcan,sec:focusing}.


\subsection*{Exercises}

\begin{ex}\label{ex:catcoprod-functor-uniq}
  This is the dual of \cref{ex:catprod-functor-uniq}, though of course its proof is not dual.
  Suppose we have
  \begin{mathpar}
    f\in\cG(A,C)\and
    g\in\cG(B,D)\and
    h\in\cG(C,E)\and
    k\in\cG(D,E)
  \end{mathpar}
  Here is one (cut-free) derivation of $A+B\types E$.
  \begin{mathpar}
    \inferrule*[Right=$\plusE$]{
      \inferrule*{ }{A+B\types A+B}\\
      \inferrule*[Right=$h$]{\inferrule*[Right=$f$]{\inferrule*{ }{A\types A}}{A\types C}}{A\types E}\\
      \inferrule*[Right=$k$]{\inferrule*[Right=$g$]{\inferrule*{ }{B\types B}}{B\types D}}{B\types E}
    }{A+B\types E}\and
  \end{mathpar}
  Write down another derivation of $A+B\types E$ that ends with the following cut:
  \begin{mathpar}
    \inferrule*[Right=cut]{
      \inferrule*{\vdots}{A+B\types C+D}\\
      \inferrule*{\vdots}{C+D\types E}
    }{A+B\types E}\and
  \end{mathpar}
  Then write down the terms corresponding to the two derivations and show directly that they are related by $\equiv$.
\end{ex}

\begin{ex}\label{ex:cat-prod-coprod}
  Combine the type theories of \cref{sec:catprod,sec:catcoprod} to obtain a unary type theory for categories with both products and coproducts.
  Prove the initiality theorem.
  (Note in particular that no compatibility between the products and coproducts is required or ensured.)
\end{ex}

\begin{ex}\label{ex:cat-opfib}
  A functor $P:\sA\to\sB$ is called an \textbf{opfibration} if $P\op:\sA\op\to\sM\op$ is a fibration (as in \cref{ex:catfib}).
  The dual of $f^*(b)$ is written $f_!(a)$.
  \begin{enumerate}
  \item Write down a type theory for opfibrations and prove the initiality theorem.
    (Remember that we always use natural deduction style when dealing with categories rather than posets, so you can't just dualize \cref{ex:catfib} or categorify \cref{ex:poset-bifib}.
    You will probably want a term syntax such as ``$\match_!$''.)
  \item Use this type theory to prove that $f_!$ is always a functor.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:cat-prod-coprod-uniq}
  Suppose we have
  \begin{mathpar}
    f\in\cG(A,C)\and
    g\in\cG(A,D)\and
    h\in\cG(B,C)\and
    k\in\cG(B,D)
  \end{mathpar}
  Write down two different derivations of $A+B\types C\times D$ in the type theory of \cref{ex:cat-prod-coprod}, one that ends with $\timesI$ and one that ends with $\plusE$.
  Then write down the corresponding terms and show directly that they are identified by $\equiv$.
\end{ex}

\begin{ex}\label{ex:cat-bifib}
  A functor $P:\sA\to\sB$ is called a \textbf{bifibration} if it is both a fibration and an opfibration.
  \begin{enumerate}
  \item Combine the theories of \cref{ex:catfib,ex:cat-opfib} to obtain a type theory for bifibrations.
  \item If you aren't tired of proving initiality theorems yet, do it for this type theory.
  \item Use this type theory to prove that in any bifibration, $f_!$ is left adjoint to $f^*$.
  \end{enumerate}
\end{ex}


\begin{subappendices}
\section{Canonical and atomic}
\label{sec:atomcan}

As observed in \cref{rmk:beta-reduction}, the $\beta$-reduction rule can be seen as a ``directed simplification'': clearly $M$ is simpler than $\pi_1(\pair M N)$.
Put differently, the derivation leading to the term $\pi_1(\pair M N)$ is ``redundant'' since it represents a morphism that already had a representative, namely $M$.
We now present a theory that prevents such redundancy from occurring.

The idea is to prevent application of elimination rules (such as $\pi_1$) to terms resulting from introduction rules (such as $\pair M N$).
We do, of course, have to be able to apply elimination rules to some terms, in order to derive say $x:A\times B \types \pi_1(x):A$.
To distinguish between the ``terms that can be eliminated'' and those that can't, we replace the single judgment $x:A\types M:B$ with two:
\begin{mathpar}
  x:A \types M\atom B\and
  x:A \types M\can B.
\end{mathpar}
We read $M\atom B$ as ``$M$ is an \textbf{atomic} term of type $B$'' and $M\can B$ as ``$M$ is a \textbf{canonical} term of type $B$''.
Intuitively, the canonical terms are a subset of the ordinary ones with the property of being ``canonical representatives'' of a morphism in a free category with products.
If we succeed at proving an initiality theorem without needing to impose any equivalence relation on terms, then it will follow that in fact, every $\equiv$-equivalence class of ordinary terms has \emph{exactly one} representative that is ``canonical'' in the current sense.
However, we will see below that this is not always possible.

\subsection{Categories with products}
\label{sec:atomcan-catprod}

We consider first the case of categories with products only.
We assert the same natural deduction rules as before, but now with annotations describing which terms are canonical and which are atomic.
\begin{mathpar}
  \inferrule{\types X\type}{x:X\types x\atom X}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ x:X\types M\can A}{x:X\types f(M)\atom B}\;fI\and
  \inferrule{\types X\type}{x:X\types \ttt\can \unit}\;\unit I\and
  \inferrule{x:X\types M\atom B\times C}{x:X \types \pi_1(M)\atom B}\;\timesE1\and
  \inferrule{x:X\types M\atom B\times C}{x:X \types \pi_2(M)\atom C}\;\timesE2\and
  \inferrule{x:X\types M\can B \\ x:X\types N\can C}{x:X\types \pair MN\can B\times C}\;\timesI
\end{mathpar}
Note particularly that in the $\timesE$ rules, $M$ must be atomic; whereas in the $\timesI$ rule, $\pair MN$ is only canonical; thus $\pi_1(\pair MN)$ cannot be formed.
We allow $M$ and $N$ in the $\timesI$ rule to also be canonical, so that we can form iterated pairs such as $\pair M{\pair NP}$; while similarly we say that $\pi_1(M)$ and $\pi_2(M)$ are again atomic, so that we can form iterated projections such as $\pi_1(\pi_2(M))$.
All variables are atomic, so that we can form $x:A\times B \types \pi_1(x):A$.

Of course, this division into atomic and canonical is an artifact of the syntax; it has no natural counterpart in category theory.
Thus, to extract a description of a free category with products, we need some way to combine the two kinds of terms to describe one kind of morphism.
It turns out that the best way to do this is to assert that any atomic term \emph{belonging to a base type} (i.e.\ an object of \cG) is also canonical:
\begin{mathpar}
  \inferrule{B\in\cG \\ x:A \types M\atom B}{x:A\types M\can B}\;\atomcan
\end{mathpar}
and then regard the canonical terms as the morphisms.
This completes the definition of our \textbf{unary canonical/atomic calculus for categories with products}.

At this point it is a little difficult to explain why we regard a term of the form $f(M)$ to be atomic but allow the argument $M$ to be canonical.
It doesn't make much difference what choice we make regarding this rule in our current very simple theory, since both $M$ and $f(M)$ always belong to a base type, and for base types there is no difference between atomic and canonical terms (the only way to make a canonical term of a base type is to apply $\atomcan$).
So we just ask the reader to take our word for it that this choice is the best one; it will make more sense later [TODO].

The restriction $B\in\cG$ in $\atomcan$ is what removes the redundancy expressed by the \emph{other} generator of $\equiv$, namely the $\eta$-conversion $\pair{\pi_1(M)}{\pi_2(M)}\equiv M$.
However, it deals with it in a surprising way: it is the seemingly ``more complicated'' term $\pair{\pi_1(M)}{\pi_2(M)}$ that we regard as canonical!
For example, when $x=M$ we have the following derivation in our new system:
\begin{mathpar}
  \inferrule*[Right=\timesI]{
    \inferrule*[Right=\atomcan]{
      \inferrule*[Right=\timesE]{\inferrule*{ }{x:A\times B\types x\atom A\times B}}{x:A\times B \types \pi_1(x)\atom A}
    }{x:A\times B \types \pi_1(x)\can A} \\
    \inferrule*[Right=\atomcan]{
      \inferrule*[Right=\timesE]{\inferrule*{ }{x:A\times B\types x\atom A\times B}}{x:A\times B \types \pi_2(x)\atom B}
    }{x:A\times B \types \pi_2(x)\can B}
  }{
    x:A\times B \types \pair{\pi_1(x)}{\pi_2(x)}\can A\times B
  }
\end{mathpar}
but while we have $x:A\times B\types x\atom A\times B$, we cannot apply $\atomcan$ to deduce $x:A\times B\types x\can A\times B$ since $A\times B$ is not a base type.
Put differently, when transforming an arbitrary term into a canonical one, we regard the $\beta$-conversion as a reduction, but the $\eta$-conversion as an \emph{expansion}.

There are various reasons for this choice, foremost among which is that it works well.
Philosophically, it can be justified by saying that the type $A\times B$ is supposed to be a ``type whose elements are pairs'', and therefore the \emph{canonical form} of any element of it (even a variable) should \emph{be a pair}.
A category theorist can gain some intuition for it by thinking of $\beta$-reduction as analogous to the multiplication transformation $T^2\to T$ of a monad, while the $\eta$-expansion is analogous to the unit transformation $\mathsf{Id}\to T$.
In an example such as the free monoid monad, the multiplication ``simplifies'' a list $((a,b),(),(c,d,e))$ by removing redundant parentheses to get $(a,b,c,d,e)$, while the unit makes an element $a$ into a list $(a)$ by \emph{adding} parentheses.
Na\"ively one might say that $(a)$ is more complicated than $a$, but it has the virtue of being in the same ``canonical form'' as every other list.

\begin{rmk}
  Recall that we generally want to regard terms as simply a different syntactic representation of derivations, with a parse tree that inverts the derivation tree.
  This is almost true for our current system: the only rule that breaks it is $\atomcan$, which uses the same term $M$ to represent two different derivations (one of $M\atom B$, and one of $M\can B$ obtained by following the former with $\atomcan$).
  However, this can be regarded as simply a convenient abuse of notation: since otherwise the syntaxes producing atomic and canonical terms are disjoint, if when parsing $M\can B$ we see that $M$ looks like an atomic term, it must be that it was deduced from $\atomcan$ and so we can insert an appropriate node into the tree.
\end{rmk}

The central lemma that justifies this theory is, as usual, the admissibility of cut/substitution.

\begin{lem}\label{thm:catprod-atomcan-subadm}
  The following two cut rules are admissible in the unary canonical/atomic calculus for categories with products.
  \begin{enumerate}
  \item If $x:A\types M\can B$ and $y:B\types N\atom C$ are derivable, then either $x:A\types P\atom C$ or $x:A\types P\can C$ is derivable for some $P$.\label{item:catprod-atomcan-subadm-1}
  \item If $x:A\types M\can B$ and $y:B\types N\can C$ are derivable, then $x:A\types P\can C$ is derivable for some $P$.\label{item:catprod-atomcan-subadm-2}
  \end{enumerate}
\end{lem}
For purposes of composition in a free category with products, we are mainly interested in~\ref{item:catprod-atomcan-subadm-2}.
However, for the induction to go through we must also prove~\ref{item:catprod-atomcan-subadm-1} by a simultaneous induction on derivations of both.
\begin{proof}
  Here are the inductive cases for the derivation of $y:B\types N\atom C$:
  \begin{itemize}
  \item If it ends with $\idfunc$, then $B=C$ and $N=y$, whence $x:A\types M\can B$ is our desired result.
  \item If it ends with $fI$ for some $f\in\cG(D,C)$, then $N=f(N')$ and $y:B\types N'\can D$.
    Thus, by the inductive hypothesis of~\ref{item:catprod-atomcan-subadm-2}, we have $x:A\types P'\can D$.
    Applying $fI$ again, we have $x:A\types f(P')\atom C$.
  \item If it ends with $\timesE1$, then we have $N=\pi_1(N')$ and $y:B\types N'\atom C\times D$.
    Thus, inductively we have $x:A\types P'\can C\times D$.
    This is where the interesting thing happens: since $P'$ is canonical rather than atomic, we can't apply $\timesE1$ directly to obtain $\pi_1(P')$.
    Instead we observe that \emph{any canonical term of $C\times D$ must be of the form $\pair PQ$}, simply because the only rule that builds canonical terms of a product type is $\timesI$.
    Thus, we must have $P'=\pair PQ$ where $x:A\types P\can C$ and $x:A\types Q\can D$.
    But $x:A\types P\can C$ is exactly what we wanted.
    (Of course, the case of $\timesE2$ is symmetric.)
  \end{itemize}
  And here are the inductive cases for the derivation of $y:B\types N\can C$:
  \begin{itemize}
  \item If it ends with $\unit I$, then $C=\unit$ and so $x:A\types \ttt\can \unit$.
  \item If it ends with $\timesI$, then $N=\pair{N_1}{N_2}$ with $y:B\types N_1\can C_1$ and $y:B\types N_2\can C_2$.
    Thus, inductively we have $x:A\types P_1:C_1$ and $x:A\types P_2:C_2$, whence $x:A\types \pair{P_1}{P_2}\can C$.
  \item If it ends with $\atomcan$, then we have $y:B\types N\atom C$, and hence by the inductive hypothesis of~\ref{item:catprod-atomcan-subadm-1} we have either $x:A\types P\atom C$ or $x:A\types P\can C$.
    In the second case we are done.
    In the first case, since we already applied $\atomcan$ at type $C$, $C$ must be a base type, so we can apply $\atomcan$ again to conclude $x:A\types P\can C$.\qedhere
  \end{itemize}
\end{proof}

Like \cref{thm:category-cutadm,thm:catprod-subadm}, the proof of \cref{thm:catprod-atomcan-subadm} defines a ``substitution'' operation (well, technically, two operations) on terms, which we write as $N\hsub{M/y}$.
However, compared to ordinary substitution, this operation is more powerful: it also performs ``$\beta$-simplification'' as it goes.
For instance, we have
\[ \pi_1(y)\hsub{\pair M N/y} = M \]
whereas ordinary substitution would give $\pi_1(\pair M N)$.
In type-theoretic lingo, this ``reducing substitution'' is called \textbf{hereditary substitution}.
Its recursive defining clauses are
\begin{align*}
  y\hsub{M/y} &= M\\
  f(N)\hsub{M/y} &= f(N\hsub{M/y})\\
  \ttt\hsub{M/y} &= \ttt\\
  \pi_1(N)\hsub{M/y} &= P \qquad\text{where }N\hsub{M/y}=\pair P Q\\
  \pi_2(N)\hsub{M/y} &= Q \qquad\text{where }N\hsub{M/y}=\pair P Q\\
  \pair P Q \hsub{M/y} &= \pair{P\hsub{M/y}}{Q\hsub{M/y}}.
\end{align*}

In particular, hereditary substitution implies that the usual natural deduction rules are also \emph{admissible} for canonical terms.
For instance, given a canonical term $x:X \types M\can A\times B$, we can construct a canonical term $x:X\types M' \can A$, even though $\pi_1$ cannot be applied directly to canonical terms; we can take $M' = \pi_1(y)\hsub{M/y}$.
(It may be tempting to write this as ``$\pi_1(M)$'', but that would probably be confusing since it hides the fact that a meta-operation on syntax is occurring, as we can't directly apply $\pi_1$ to $M$.)

This gives our promised way to ``reduce'' terms in the type theory from \cref{sec:beta-eta}.
Like we did in \cref{sec:category-cutadm} for the cut-ful type theory of \cref{sec:category-cutful}, since all the rules of \cref{sec:beta-eta} are admissible in our canonical/atomic calculus, we can interpret any derivation --- and hence any term --- in that type theory into our current one.
With projections defined in terms of hereditary substitution as in the previous paragraph, the net effect of this is to automatically reduce all applications of a projection to a pair.

Now recall that in ordinary natural deduction, we don't need to (and, indeed, can't) prove that identity rule $A\types A$ is also admissible; hence we assume the ``use a variable'' rule $x:A\types x:A$.
However, in the canonical/atomic calculus, we have only assumed that variables are \emph{atomic}; thus we need to also prove that there are \emph{canonical} identities.

\begin{lem}\label{thm:catprod-atomcan-idadm}
  If $\types A\type$ is derivable, then there is a term $\hid{A}(x)$ such that $x:A \types \hid{A}(x)\can A$.
\end{lem}
\begin{proof}
  We induct on the derivation of $\types A\type$.
  \begin{itemize}
  \item If $A\in\cG$ is a base type, then we can apply $\atomcan$ to derive $x:A \types x\can A$.
  \item If $A=\unit$, then $x:\unit \types \ttt\can \unit$.
  \item If $A=A_1\times A_2$, then by induction we have $y:A_1 \types \hid{A_1}(y)\can A_1$ and $z:A_2 \types \hid{A_2}(z)\can A_2$.
    Thus, $x:A \types \hid{A_1}(y)\hsub{\pi_1(x)/y}\can A_1$ and $x:A \types \hid{A_2}(z)\hsub{\pi_2(x)/z}\can A_2$, so we have
    \[ x:A \types \pair{\hid{A_1}(y)\hsub{\pi_1(x)/y}}{\hid{A_2}(z)\hsub{\pi_2(x)/z}} \can A.\qedhere\]
  \end{itemize}
\end{proof}

As a recursively defined operation, the clauses of $\hid{A}$ are thus
\begin{align*}
  \hid{A}(x) &= x \qquad\text{if }{A\in\cG}\\
  \hid{\unit}(x) &= \ttt\\
  \hid{A\times B}(x) &= \pair{\hid{A}(y)\hsub{\pi_1(x)/y}}{\hid{B}(z)\hsub{\pi_2(x)/z}}.
\end{align*}
Note that the hereditary substitution in the last clause is not doing any $\beta$-reduction, since the identity terms being substituted into are built only out of variables and pairs.
For instance, we have
\[ \hid{A\times (B\times C)}(x) = \pair{\pi_1(x)}{\pair{\pi_1(\pi_2(x))}{\pi_2(\pi_2(x))}} \]

\begin{lem}\label{thm:atomcan-catprod-subassoc}
  Hereditary substitution and identities form a category:
  \begin{align*}
    P\hsub{N/z}\hsub{M/y} &= P\hsub{N\hsub{M/y}/z}\\
    \hid{A}(y)\hsub{M/y} &= M\\
    N\hsub{\hid{A}(x)/y} &= N[x/y].
  \end{align*}
\end{lem}
\begin{proof}
  Left to the reader as \cref{ex:atomcan-catprod-subassoc}.
\end{proof}

\begin{thm}\label{thm:atomcan-catprod-initial}
  For any directed graph \cG, the free category with finite products generated by \cG can be described by the unary canonical/atomic calculus for categories with products under \cG, without any quotienting: its objects are the types $A$ such that $\types A\type$ is derivable, and its morphisms $A\to B$ are the terms $M$ such that $x:A \types M\can B$ is derivable.
\end{thm}
\begin{proof}
  \cref{thm:atomcan-catprod-subassoc} gives us a category, which we denote $\F\bPrCat\cG$.
  The interesting thing is the proof that $\F\bPrCat\cG$ has finite products.
  The product of $A$ and $B$ is, of course, supposesd to be $A\times B$, with projections
  \begin{align*}
    z:A\times B &\types \pi_1(w)\hsub{z/w}\can A\\
    z:A\times B &\types \pi_2(w)\hsub{z/w}\can B
  \end{align*}
  (We can't write $\pi_1(z)$ because $A$ may not be a base type.)
  Now given any morphisms $x:X \types M\can A$ and $x:X \types N\can B$, we can certainly form $x:X \types \pair M N \can A\times B$, and its composite with $\pi_1$ is
  \[ \pi_1(w)\hsub{z/w}\hsub{\pair M N/z} = \pi_1(w)\hsub{z\hsub{\pair M N/z}/w} = \pi_1(w)\hsub{\pair M N/w} = M \]
  using the associativity of hereditary substitution and the definition of hereditary substitution on $\pi_1$.
  Of course, the case of $\pi_2$ is similar.
  In the other direction, given any morphism $x:X \types P\can A\times B$, as observed in the proof of \cref{thm:catprod-atomcan-subadm} $P$ must be a pair $\pair M N$ where $x:X\types M\can A$ and $x:X\types N\can B$, and for the same reasons $M$ and $N$ are the composites of $P$ with $\pi_1$ and $\pi_2$ respectively.
  Thus, $A\times B$ has the universal property of a product with respect to \emph{syntactic equality} of terms/derivations.

  Now if \cM is a category with finite products and $P:\cG\to\cM$ a graph morphism, we extend $P$ to $\F\bPrCat\cG$ just as before by recursion on derivations.
  We have to define $P$ on both atomic and canonical terms for the recursion to go through (since the atomic and canonical terms are defined by mutual induction); since there is no separation between ``atomic and canonical morphisms'' in \cM we just send both kinds of term to ordinary morphisms in \cM.
  As usual, the defining clauses in this recursion are forced by functoriality and product-preservation, and we can then prove by induction on derivations that the resulting operation actually is a functor.
\end{proof}

\subsection{Categories with coproducts}
\label{sec:atomcan-catcoprod}

Now we consider a canonical/atomic calculus for categories with coproducts.
In general, objects with left (``mapping out'') universal properties are trickier to deal with in natural deduction systems, because our terms only appear on the \emph{right} of the $\types$.
We begin by rewriting the theory of \cref{sec:catcoprod} with atomic and canonical annotations; see \cref{fig:catcoprod-atomcan}.
\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\types X\type}{x:X\types x\atom X}\;\idfunc\and
    \inferrule{f\in \cG(A,B) \\ x:X\types M\can A}{x:X\types f(M)\atom B}\;fI\and
    \inferrule{x:X \types M\atom \zero \\ \types C\type}{x:X \types \abort(M)\can C}\;\zeroE\\
    \inferrule{x:X\types M\can A}{x:X\types \inl(M)\can A + B}\;\plusI1\and
    \inferrule{x:X\types N\can B}{X\types \inr(N)\can A + B}\;\plusI2\and
    \inferrule{u:A\types P\can C \\ v:B\types Q\can C \\ x:X\types M\atom A + B}{x:X\types \case(M,u.P,v.Q)\can C}\;\plusE\and
    \inferrule{B\in\cG\\ x:A \types M\atom B}{x:A\types M\can B}\;\atomcan
  \end{mathpar}
  \caption{Unary canonical/atomic calculus for categories with coproducts}
  \label{fig:catcoprod-atomcan}
\end{figure}
The principle is the same: we prevent ourselves from eliminating on the results of introduction rules by requiring elimination rules to act only on atomic terms.
The only perhaps surprising thing is that the ``branches'' $P,Q$ of $\case$, and the $\case$ term itself, are canonical, though the ``branch term'' $M$ must be atomic.

The proof of admissibility of (hereditary) substitution for this theory is more complicated because of the presence of the rules $\zeroE$ and $\plusE$.
In \cref{sec:atomcan-catprod} we could see by inspection that canonical terms of a certain type must have a certain form, but $\zeroE$ and $\plusE$ can produce canonical terms of any type at all.

\begin{thm}\label{thm:atomcan-catcoprod-subadm}
  The following rules are admissible in the unary canonical/atomic calculus for categories with coproducts under \cG:
  \begin{enumerate}
  \item Hereditary substitution into atomic terms: if $x:A\types M\can B$ and $y:B\types N\atom C$ are derivable, then either $x:A\types P\atom C$ or $x:A\types P\can C$ is derivable for some $P$.\label{item:catcoprod-atomcan-subadm-1}
  \item Hereditary substitution into canonical terms: if $x:A\types M\can B$ and $y:B\types N\can C$ are derivable, then $x:A\types P\can C$ is derivable for some $P$.\label{item:catcoprod-atomcan-subadm-2}
  \end{enumerate}
\end{thm}
\begin{proof}
  By mutual induction.
  We outline the cases for each statement as follows.
  \begin{enumerate}
  \item A derivation of $y:B\types N\atom C$ can only end with $\idfunc$ and $fI$.
    Both cases are just like those in \cref{thm:catprod-atomcan-subadm};
    recall that the second case uses the inductive hypothesis of~\ref{item:catcoprod-atomcan-subadm-2}.
  \item A derivation of $y:B\types N\can C$ can end with $\plusI1$, $\plusI2$, $\zeroE$, or $\plusE$.
    \begin{itemize}
    \item If $N=\inl(N')$ with $y:B\types N'\can C_1$, then by the inductive hypothesis $x:A\types N'\hsub{M/y}\can C_1$ and so $x:A\types \inl(N'\hsub{M/y})\can C$.
      The case of $\inr$ is dual.
    \item If $N=\case(N',u.P,v.Q)$ with $y:B\types N'\atom D+E$ and $u:D\types P\can C$ and $v:E\types Q\can C$, then since the only rule that can produce an atomic term of type $D+E$ is $\idfunc$, we must have $B=D+E$ and $N'=y$.
      % In a more general case, we would inspect the last rule in the derivation; the other ways to produce an atomic term could then be substituted by the inductive hypothesis of~\ref{item:catcoprod-atomcan-subadm-1} to give an atomic $N'\hsub{M/y}\atom D+E$ that we can apply $\case$ to directly.
      Now we do a secondary inner induction on the derivation of $x:A\types M\can B$, which can likewise end with $\plusI1$, $\plusI2$, or $\zeroE$ or $\plusE$.
      \begin{itemize}
      \item If $M=\inl(M')$, then by the inductive hypothesis we have $x:A\types P\hsub{M'/u}\can C$.
      \item Dually, if $M=\inr(M')$, we have $x:A\types Q\hsub{M'/v}\can C$.
      \item If $M=\abort(M')$, then $x:A\types \abort(M')\can C$.
      \item If $M=\case(M',w.S,z.T)$ with $x:A\types M'\atom F+G$ and $w:F\types S\can D+E$ and $z:G\types T\can D+E$, then
        by the inner inductive hypothesis we have $w:F\types S'\can C$ and $z:G\types T'\can C$.
        Thus, applying $\plusE$ we have $x:A\types \case(M',w.S',z.T')\can C$.
      \end{itemize}
    \item The case when $N=\abort(N')$ with $y:B\types N'\atom\zero$ is similar, but simpler.
      We must have $B=\zero$ and $N'=y$, and we induct on the derivation of $x:A\types M\can B$, which must end with $\zeroE$ or $\plusE$.
      \begin{itemize}
      \item If $M=\abort(M')$, then $x:A\types \abort(M')\can C$.
      \item If $M=\case(M',w.S,z.T)$ with $x:A\types M'\atom F+G$ and $w:F\types S\can \zero$ and $z:G\types T\can \zero$, then
        by the inner inductive hypothesis we have $w:F\types S'\can C$ and $z:G\types T'\can C$, whence $x:A \types \case(M',w.S',z.T')\can C$.\qedhere
      \end{itemize}
    \end{itemize}
  \end{enumerate}
\end{proof}

\begin{figure}
  \centering
  \begin{align*}
    y\hsub{M/y} &= M\\
    f(N)\hsub{M/y} &= f(N\hsub{M/y})\\
    \inl(N)\hsub{M/y} &= \inl(N\hsub{M/y})\\
    \inr(N)\hsub{M/y} &= \inr(N\hsub{M/y})\\
    \abort(y)\hsub{M/y} &= \abort'(M)\\
    \case(y,u.P,v.Q)\hsub{M/y} &= \case'(M,u.P,v.Q)\\
    \\
    \abort'(\abort(M)) &= \abort(M)\\
    \abort'(\case(M,u.P,v.Q)) &= \case(M,u.\abort'(P),v.\abort'(Q))\\
    \\
    \case'(\inl(M),u.P,v.Q) &= P\hsub{M/u}\\
    \case'(\inr(M),u.P,v.Q) &= Q\hsub{M/v}\\
    \case'(\abort(M),u.P,v.Q) &= \abort(M)\\
    \case'(\case(M,w.S,z.T),u.P,v.Q) &= \case(M,w.\case'(S,u.P,v.Q),z.\case'(T,u.P,v.Q))
  \end{align*}
  \caption{Hereditary substitution for categories with coproducts}
  \label{fig:catcoprod-hsub}
\end{figure}

The resulting definition of hereditary substitution is shown in \cref{fig:catcoprod-atomcan}.
We have introduced notation $\abort'$ and $\case'$ for the operations defined by the inner recursions in the proof.
Note that these essentially constitute proofs that the unrestricted $\zeroE$ and $\plusE$ rules are admissible, just as the hereditary substitution from \cref{sec:atomcan-catprod} yielded an admissible unrestricted $\timesE$.

The identity rule is much easier:

\begin{thm}\label{thm:atomcan-catcoprod-idadm}
  The canonincal identity rule is admissible: if $\types A\type$, then $x:A \types \hid{A}(x)\can A$.
\end{thm}
\begin{proof}
  The defining clauses are
  \begin{align*}
    \hid{A}(x) &= x \quad\text{if }A\in \cG\\
    \hid{\zero}(x) &= \abort(x)\\
    \hid{A+B}(x) &= \case(x,u.\hid{A}(u),v.\hid{B}(v))\qedhere
  \end{align*}
\end{proof}

Like the admissible $\pi_1$ from \cref{sec:atomcan-catprod}, $\match'$ satisfies $\beta$-conversion as a syntactic equality (this is its first two defining clauses).
Unfortunately, this theory still does not validate the full $\eta$ rule for coproducts as a syntactic equality.
We do have the following weaker property:

\begin{lem}\label{thm:atomcan-catcoprod-weaketa}
  If $x:X \types M\can A+B$, then $M = \case'(M,u.\inl(u),v.\inr(v))$.
  Similarly, if $x:X\types M\can\zero$, then $M=\abort'(M)$.
\end{lem}
\begin{proof}
  An easy induction over the defining clauses of $\case'$ and $\abort'$.
\end{proof}

In other words, it is still true (see footnote~\ref{fn:weak-eta}) that every term \emph{of type $A+B$} has a canonical form.
However, unlike the case of products this is insufficient to ensure the full uniqueness aspect of the universal property, since the ``mapping out'' universal property of $A+B$ constructs terms not of type $A+B$ itself but of some other type.
For instance, if we have
\begin{mathpar}
  f\in\cG(A,C)\and
  g\in\cG(B,C)\and
  h\in\cG(C,D)\and
\end{mathpar}
then the induced map $A+B\to D$ is represented by both of the terms
\begin{align*}
  x:A+B &\types \case(x,u.h(f(u)),v.h(g(v)))\can D\\
  x:A+B &\types h(\case(x,u.f(u),v.g(v)))\can D
\end{align*}
and these two terms are (syntactically) different, despite both yielding $h(f(u))$ and $h(g(v))$ under the hereditary substitutions $\hsub{\inl(u)/x}$ and $\hsub{\inr(v)/x}$.

Thus, the terms/derivations in this type theory don't present a category with finite coproducts unless we impose some equivalence relation on them.
The above example also points the way to the correct equivalence relation.
We no longer need $\beta$-conversion, nor do we need the weak form of $\eta$-conversion expressed by \cref{thm:atomcan-catcoprod-weaketa}; instead we need the relations shown in \cref{fig:catcoprod-commconv}.
\begin{figure}
  \centering
  \begin{align*}
    f(\case(M,u.P,v.Q)) &\equiv \case(M,u.f(P),v.f(Q))\\
    \inl(\case(M,u.P,v.Q)) &\equiv \case(M,u.\inl(P),v.\inl(Q))\\
    \inr(\case(M,u.P,v.Q)) &\equiv \case(M,u.\inr(P),v.\inr(Q))\\
    \case(\case(M,u.P,v.Q),w.R,z.S) &\equiv \case(M,u.\case(P,w.R,z.S),v.\case(Q,w.R,z.S))\\
    \abort(\case(M,u.P,v.Q)) &\equiv \case(M,u.\abort(P),v.\abort(Q))
  \end{align*}
  \caption{Commuting conversions for coproducts}
  \label{fig:catcoprod-commconv}
\end{figure}
These are called \textbf{commuting conversions}.
They are a consequence of the strong form of $\eta$-conversion~\eqref{eq:catcoprod-eta}:
\begin{equation}
  \inferrule{x:X \types M\atom A+B \\ y:A+B \types P\can C}{x:X \types \case(M,u.P[\inl(u)/y],v.P[\inr(v)/y]) \equiv P[M/y] \can C}\label{eq:catcoprod-eta-again}
\end{equation}
(take $P$ to be the left-hand side of each commuting conversion with $y$ in place of $M$).
Conversely, when combined with the weak $\eta$-conversion, the commuting conversions imply the strong $\eta$-conversion:

\begin{thm}
  If we augment the canonical/atomic calculus for categories with coproducts by an $\equiv$ on canonical terms generated by the relations in \cref{fig:catcoprod-commconv} (plus the usual congruence properties), then the strong $\eta$-rule~\eqref{eq:catcoprod-eta-again} is admissible.
\end{thm}
\begin{proof}
  Induction on the derivation of $P$; each commuting conversion handles exactly one of the cases.
\end{proof}




\subsection*{Exercises}

\begin{ex}\label{ex:atomcan-catprod-subassoc}
  Prove \cref{thm:atomcan-catprod-subassoc} (hereditary substitution is associative).
\end{ex}

\begin{ex}\label{ex:atomcan-catfib}
  Formulate a canonical/atomic calculus for fibrations of categories and prove its initiality theorem.
  Then do the same for fibrations of categories with products, as in \cref{ex:catprod-fib}.
\end{ex}


\section{Focusing}
\label{sec:focusing}

For completeness, we now describe how the sequent calculus for meet-semilattices can also be modified to present free categories with products without needing an equivalence relation on terms.
This version is not as commonly used in categorical logic, but it is important in other applications of type theory, such as automated proof search; it is called \emph{focusing}.

If we inspect the derivations of canonical terms in the type theory of \cref{sec:atomcan}, we see that they have the following form: first they apply some number of projections to variables, then they apply $\atomcan$, they apply some number of generating arrows between base types, and finally they apply some number of pairings.
For instance, if $f\in \cG(C,D)$ we have
\[x:(A\times B)\times C \types \pair{f(\pi_2(x))}{\pi_2(\pi_1(x))}\can D\times B\]
with derivation
\begin{mathpar}
\let\mytimes\times
\def\times{\mathord{\mytimes}}
  \inferrule*{
    \inferrule*{\inferrule*{\inferrule*{
          x:(A\times B)\times C \types x\atom (A\times B)\times C
        }{x:(A\times B)\times C \types \pi_2(x) \atom C
        }}{x:(A\times B)\times C \types \pi_2(x) \can C
      }}{x:(A\times B)\times C \types f(\pi_2(x)) \can D}
    \\
    \inferrule*{\inferrule*{\inferrule*{
          x:(A\times B)\times C \types x\atom (A\times B)\times C
        }{x:(A\times B)\times C \types \pi_1(x)\atom A\times B
        }}{x:(A\times B)\times C \types \pi_2(\pi_1(x))\atom B}
    }{x:(A\times B)\times C \types \pi_2(\pi_1(x))\can B}
  }{x:(A\times B)\times C \types \pair{f(\pi_2(x))}{\pi_2(\pi_1(x))}\can D\times B}
\end{mathpar}
This suggests that we ought to consider as ``canonical'' the sequent calculus derivation that is closest to the above.
Roughly speaking, this means first applying left rules, then generators, then right rules\footnote{To be honest, we should point out that the fact that the canonical derivations can be described so simply is an artifact of the relative triviality of our current theory, with essentially only one operation $\times$.
One can still formulate canonical/atomic and focused versions of more general theories, but the canonical forms don't have as simple a description.}, as in the following derivation:
\begin{mathpar}
  \inferrule*{
    \inferrule*{\inferrule*{C\types C
      }{(A\times B)\times C \types C
      }}{(A\times B)\times C \types D}
      \\
    \inferrule*{\inferrule*{B \types B
      }{A\times B \types B
      }}{(A\times B)\times C \types B}
  }{(A\times B)\times C \types D\times B}
\end{mathpar}
As in the natural deduction case, we can rule out the undesired sequent calculus derivations by using two judgments to indicate whether we are in the ``left phase'' or the ``right phase''.
We denote these by
\[ \focus{A} \types B
\qquad\text{and}\qquad
%A\types\focus{B}
A\types B
\]
respectively.
The brackets in the first case indicate that we are ``focused'' on the left side of the sequent.
You might think that in the second case we should write $A\types \focus B$ to indicate that we are focused on the right side, but in fact in the latter case we are not really ``focused'' at all; in more complicated situations there is also a ``right-focused'' judgment $A\types \focus B$ that is different from both $\focus A \types B$ and $A\types B$.
The asymmetry we see here is due to the fact that all universal properties we are considering are of the ``mapping in'' sort.

We now modify the sequent calculus rules from \cref{sec:seqcalc-mslat} by stipulating that
the right rules only apply when we are in the right phase,
% \begin{mathpar}
%   \inferrule{A\types \focus{B} \\ A\types \focus{C}}{A\types \focus{B\times C}}\and
%   \inferrule{f\in \cG(A,B) \\ X\types \focus{A}}{X\types \focus{B}}\and
%   \inferrule{\types A\type}{A\types \focus{\top}}\and
% \end{mathpar}
while the left rules apply only in the left phase;
% \begin{mathpar}
%   \inferrule{A\in \cG}{\focus A\types A}\and
%   \inferrule{\focus A\types C \\ \types B\type}{\focus {A\times B} \types C}\and
%   \inferrule{\focus B\types C \\ \types A\type}{\focus{A\times B} \types C}\and
% \end{mathpar}
and we can transition from the left to the right phase, but not vice versa.
% \[ \inferrule{\focus A \types B}{A\types\focus B}\]
Since we are interested in distinguishing between derivations, we also annotate our sequents with terms; and
because the left rules of sequent calculus are not well-adapted to the use of formal variables, we go back to annotating the entire sequent with a term.
The rules with annotations for this \textbf{unary focused sequent calculus for categories with products} are shown in \cref{fig:catprod-focused-seqcalc}.

\begin{figure}
  \centering
  \begin{mathpar}
    % \inferrule{\phi:(A\types \focus{B}) \\ \psi:(A\types \focus{C})}{\pair{\phi}{\psi}:(A\types \focus{B\times C})}\and
    % \inferrule{f\in \cG(A,B) \\ \phi:(X\types \focus{A})}{\postc f\phi:(X\types \focus{B})}\and
    % \inferrule{\types A\type}{\ttt:(A\types \focus{\top})}\and
    \inferrule{\phi:(A\types {B}) \\ \psi:(A\types {C})}{\pair{\phi}{\psi}:(A\types {B\times C})}\and
    \inferrule{f\in \cG(A,B) \\ \phi:(X\types {A})}{\postc f\phi:(X\types {B})}\and
    \inferrule{\types A\type}{\ttt:(A\types {\top})}\and
    \inferrule{A\in \cG}{\idfunc_A:(\focus A\types A)}\and
    \inferrule{\phi:(\focus A\types C) \\ \types B\type}{\prec{\phi}{\pi_1}:(\focus {A\times B} \types C)}\and
    \inferrule{\phi:(\focus B\types C) \\ \types A\type}{\prec{\phi}{\pi_2}:(\focus{A\times B} \types C)}\and
    \inferrule{\phi:(\focus A \types B)}{\phi:(A\types B)}
  \end{mathpar}
  \caption{Unary focused sequent calculus for categories with products}
  \label{fig:catprod-focused-seqcalc}
\end{figure}

For instance, the above derivation of $(A\times B)\times C \types D\times B$ can be given in the focused sequent calculus with term annotations as follows:
\begin{mathpar}
  \let\mytimes\times
  \def\times{\mathord{\mytimes}}
  \inferrule*{
    \inferrule*{\inferrule*{\inferrule*{\idfunc_C:(\focus C\types C)
        }{\prec{\idfunc_C}{\pi_2}:(\focus{(A\times B)\times C} \types C)
        }}{\prec{\idfunc_C}{\pi_2}:((A\times B)\times C \types C)
      }}{\postc{f}{\prec{\idfunc_C}{\pi_2}}:((A\times B)\times C \types D)}
    \\
    \inferrule*{\inferrule*{\inferrule*{\idfunc_B:(\focus B \types B)
        }{\prec{\idfunc_B}{\pi_2}:(\focus{A\times B} \types B)
        }}{\prec{\prec{\idfunc_B}{\pi_2}}{\pi_1}:(\focus{(A\times B)\times C} \types B)
      }}{\prec{\prec{\idfunc_B}{\pi_2}}{\pi_1}:((A\times B)\times C \types B)}
  }{\pair{\postc{f}{\prec{\idfunc_C}{\pi_2}}}{\prec{\prec{\idfunc_B}{\pi_2}}{\pi_1}}
    :((A\times B)\times C \types {D\times B})}
  % \inferrule*{
  %   \inferrule*{\inferrule*{\inferrule*{\idfunc_C:(\focus C\types C)
  %       }{\prec{\idfunc_C}{\pi_2}:(\focus{(A\times B)\times C} \types C)
  %       }}{\prec{\idfunc_C}{\pi_2}:((A\times B)\times C \types \focus C)
  %     }}{\postc{f}{\prec{\idfunc_C}{\pi_2}}:((A\times B)\times C \types \focus D)}
  %   \\
  %   \inferrule*{\inferrule*{\inferrule*{\idfunc_B:(\focus B \types B)
  %       }{\prec{\idfunc_B}{\pi_2}:(\focus{A\times B} \types B)
  %       }}{\prec{\prec{\idfunc_B}{\pi_2}}{\pi_1}:(\focus{(A\times B)\times C} \types B)
  %     }}{\prec{\prec{\idfunc_B}{\pi_2}}{\pi_1}:((A\times B)\times C \types \focus B)}
  % }{\pair{\postc{f}{\prec{\idfunc_C}{\pi_2}}}{\prec{\prec{\idfunc_B}{\pi_2}}{\pi_1}}
  %   :((A\times B)\times C \types \focus{D\times B})}
\end{mathpar}

As usual, we need to prove the admissibility of identity and cut.
The proof of cut is very similar to that of \cref{thm:catprod-atomcan-subadm}.

\begin{lem}\label{thm:focused-catprod-cutadm}
  The following two cut rules are admissible in the unary focused sequent calculus for categories with products.
  \begin{enumerate}
    \item If $A\types B$ and $\focus B\types C$, then $A\types C$.\label{item:focused-catprod-cutadm-1}
    \item If $A\types B$ and $B\types C$, then $A\types C$.\label{item:focused-catprod-cutadm-2}
  \end{enumerate}
\end{lem}
\begin{proof}
  First we prove~\ref{item:focused-catprod-cutadm-1} by induction on the derivation of $\focus B\types C$.
  \begin{itemize}
  \item If it is $\idfunc_B$, then $A\types B$ is $A\types C$.
  \item If it ends with $\pi_1$, then $B$ is $B_1\times B_2$ and we have $\focus B_1\types C$.
    Now consider the derivation of $A\types B$, i.e.\ $A\types B_1\times B_2$.
    \textit{A priori} this could end either with the ``unfocus'' rule or with the $\times R$ rule.
    But in fact, the unfocus rule is impossible, because it is impossible to derive $\focus A\types B$ unless $B$ is a base type (this is an easy induction).
    Thus, we must have $A\types B_1$ and $A\types B_2$, so we can apply the inductive hypothesis on $B_1$ to get $A\types C$.
  \end{itemize}
  Now we prove~\ref{item:focused-catprod-cutadm-2}, by induction on the derivation of $B\types C$.
  \begin{itemize}
  \item If it ends with the right rule for $\times$ or $\unit$, or the generator rule for $f$, we can simply apply the inductive hypothesis and then the same rule, as usual.
  \item If it ends with the unfocus rule, then we apply part~\ref{item:focused-catprod-cutadm-1}.\qedhere
  \end{itemize}
\end{proof}

However, if we try to prove admissibility of identity directly, we run into problems.
We need the following lemma first.

\begin{lem}\label{thm:focused-catprod-leftadm}
  The unfocused left rules for $\times$ are admissible in the unary focused sequent calculus for categories with products.
  That is, if we have a derivation of $A\types C$ and $\types B\type$, we can construct a derivation of $A\times B\types C$, and dually.
\end{lem}
\begin{proof}
  We induct on the derivation of $A\types C$.
  \begin{itemize}
  \item If it ends with the ``unfocus'' rule, whose premise is $\focus A\types C$, then we can apply the focused left rule to this to conclude $\focus{A\times B}\types C$, and then unfocus it to obtain $A\times B\types C$.
  \item If it ends with the right rule for $\times$, then $C=C_1\times C_2$ and we have derivations of $A\types C_1$ and $A\types C_2$.
    By the inductive hypothesis we get $A\times B\types C_1$ and $A\times B\types C_2$, whence $A\times B\types C$ by the right rule for $\times$.
  \item If it ends with the generator rule for $f\in \cG(D,C)$, then we have a derivation of $A\types D$.
    By the inductive hypothesis we get $A\times B\types D$, whence $A\times B\types C$.
  \item Finally, if it ends with the right rule for $\unit$, then $C=\unit$, and so the same rule yields $A\times B\types C$.\qedhere
  \end{itemize}
\end{proof}

\begin{lem}\label{thm:focused-catprod-idadm}
  The unfocused identity rule is admissible in the unary focused sequent calculus for categories with products.
  That is, if $\types A\type$, then $A\types A$.
\end{lem}
\begin{proof}
  Given \cref{thm:focused-catprod-leftadm}, we can use the same proof as \cref{thm:seqcalc-mslat-idadm}.
\end{proof}

Of course, although the inductive argument for \cref{thm:focused-catprod-idadm} written in English is the same as that for \cref{thm:seqcalc-mslat-idadm}, the end result is different because we have the inductive construction of \cref{thm:focused-catprod-leftadm} frobnicating the results at each inductive step rather than simply applying the primitive left rule.
For instance, on $(A\times B)\times C$ the unmodified proof of \cref{thm:seqcalc-mslat-idadm} would give the derivation
\begin{mathpar}
  \inferrule*{
    \inferrule*{\inferrule*{\inferrule*{A\types A}{A\times B\types A}\\
        \inferrule*{B\types B}{A\times B\types B}
      }{A\times B\types A\times B}
    }{(A\times B)\times C \types A\times B}\\
    \inferrule*{C\types C
    }{(A\times B)\times C \types C}
  }{(A\times B)\times C \types (A\times B)\times C}
\end{mathpar}
which cannot be ``focalized'' since it applies a left rule after a right rule.
Instead, from \cref{thm:focused-catprod-idadm} we obtain
\begin{mathpar}
  \inferrule*{
    \inferrule*{
      \inferrule*{\inferrule*{\inferrule*{\focus A\types A}{\focus{A\times B}\types A}
        }{\focus{(A\times B)\times C} \types A}
      }{(A\times B)\times C \types A}\\
      \inferrule*{\inferrule*{\inferrule*{\focus B\types B}{\focus{A\times B}\types B}
        }{\focus{(A\times B)\times C} \types B}
      }{(A\times B)\times C \types B}
    }{(A\times B)\times C \types A\times B}\\
    \inferrule*{\inferrule*{\focus C\types C
      }{\focus {(A\times B)\times C} \types C}
    }{(A\times B)\times C \types C}
  }{(A\times B)\times C \types (A\times B)\times C}
\end{mathpar}
The resulting term is
\[ \pair{\pair{\prec{\prec{\idfunc_A}{\pi_1}}{\pi_1}}{\prec{\prec{\idfunc_B}{\pi_2}}{\pi_1}}}{\prec{\idfunc_C}{\pi_2}} :
({(A\times B)\times C \types (A\times B)\times C})\]

We let the reader complete the argument (\cref{ex:focused-catprod-cutassoc,ex:focused-catprod-initial}).

\begin{lem}\label{thm:focused-catprod-cutassoc}
  Cut is associative and unital in the unary focused sequent calculus for categories with products.\qed
\end{lem}

\begin{thm}\label{thm:focused-catprod-initial}
  For any directed graph \cG, the free category with finite products generated by \cG can be described by the unary focused sequent calculus for categories with products under \cG, without any quotienting: its objects are the types $A$ such that $\types A\type$ is derivable, and its morphisms $A\to B$ are the terms $M$ such that $M:(A \types B)$ is derivable.\qed
\end{thm}


[TODO: Does focusing continue to identify unique canonical derivations in more complicated theories (with both positives and negatives)?  Right now I think the answer is no, that we still need commuting conversions.  How does focusing compare to canonical/atomic in theories with both positives and negatives?  In theories with just positives, focusing should work just as well as in theories with just negatives, although canonical/atomic fails to give $\eta$.]


\subsection*{Exercises}

\begin{ex}\label{ex:focused-catprod-cutassoc}
  Prove \cref{thm:focused-catprod-cutassoc} (cut is associative and unital in the unary focused sequent calculus for categories with products).
\end{ex}

\begin{ex}\label{ex:focused-catprod-initial}
  Prove \cref{thm:focused-catprod-initial} (initiality for the unary focused sequent calculus for categories with products).
\end{ex}


\end{subappendices}


\chapter{Simple type theories}
\label{chap:simple}

At this point we have done about all we can with \emph{unary} type theories, where the antecedent and consequent of each sequent consist only of a single type.
(In fact, most introductions to type theory skip over the unary case altogether, but I find it clarifying to start with cases that are as simple as possible.)
The most common type theories allow finite \emph{lists} of types as the antecedent.
These are the object of study in this chapter; we call them \emph{simple type theories}.
This term is more common in the literature than ``unary'', but perhaps not with the exact meaning we are giving it.
(Sometimes one allows lists in the consequent as well; we will consider this possibility in \cref{chap:polycats}.)

\section{Towards multicategories}
\label{sec:why-multicats}

As motivation for the generalization away from unary type theories, we consider a few problems with unary type theory, from a categorical perspective, that all turn out to have this as their solution.
Let's begin by stating some general principles of type theory.
Looking back at the rules of all our type theories, we see that they can be divided into two groups.
On the one hand, there are rules that don't refer specifically to any operation, such as the identity rule $x:X \types x:X$ and the cut rule.
On the other hand, there are rules that introduce or eliminate a particular operation on types, such as product, coproduct, and so on --- and each such rule refers to only \emph{one} operation  (such as $\times,+,f^*$, etc.).

This ``independence'' between the rules for distinct operations is important for the good behavior of type theory.
For instance, notice that many of the exercises have involved combining the rules for multiple previously-studied operations.
If you did some of these exercises, you hopefully got a sense for how these operations tend to coexist ``without interacting'' in the metatheory: e.g.\
when proving the cut-admissibility theorems we essentially just commute the rules for different operations past each other.
This ``modularity'' means that we are always free to add new structure to a theory without spoiling the structure we already had.
We formulate this as a general principle:
\begin{equation}\label{princ:independence}
  \text{Each rule in a type theory should refer to only one operation}.\tag{$\ast$}
\end{equation}

Note that despite~\eqref{princ:independence}, we can often obtain nontrivial results about the interaction of operations.
For instance, in \cref{ex:mslat-monoid} you showed that $A\meet \top\cong A$, even though $\meet$ and $\top$ are distinct operations with apparently unrelated rules.
Similarly, in \cref{ex:mslat-fib,ex:catprod-fib} you showed that $f^*$ preserves $\meet$ and $\times$.
In general, this tends to happen when relating operations whose universal properties all have the same ``handedness''.
For instance, all the operations $\meet,\top,\times,\unit,f^*$ have ``mapping in'' or ``from the right'' universal properties.
Thus, we can expect to compare two objects built using more than one of them by showing that they have the same universal property, and this is essentially what type theory does.

We also observe that in all cases we were able to extract the rules for a given operation from the universal property of the objects it was intended to represent in category theory.
The left and right rules in a sequent calculus, or the introduction and elimination rules in a natural deduction, always expressed the ``two sides'' of a universal property: one of them ``structures the object'' and the other says that it is universal with this structure.
The ``principal case'' of cut admissibility for a sequent calculus, and the $\beta$-conversion equality rule for a natural deduction, both express the fact that morphisms defined by the universal property ``compose with the structure'' to the inputs, e.g.\ a map $X\to A\times B$ defined from $f:X\to A$ and $g:X\to B$ gives back $f$ and $g$ when composed with the product projections.
Similarly, the proof of identity admissibility for a sequent calculus, and the $\eta$-conversion rule for a natural deduction, express the uniqueness half of the universal property.
This leads us to formulate another general principle:
\begin{equation}\label{princ:ump}
  \parbox{3.5in}{\centering The operations in a type theory should correspond categorically to objects with universal properties.}\tag{$\dagger$}
\end{equation}

The point is that from the perspective of unary type theory, these two principles \emph{seem} overly restrictive.
For instance, we remarked above that by expressing universal properties in type theory we can compare operations whose universal properties have the same handedness; but often we are interested in categorical structures satisfying nontrivial relations between objects with universal properties of different handedness.
For instance, in any category with both products and coproducts, there is a canonical map $(A\times B)+(A\times C) \to A\times (B+C)$, and the category is said to be \emph{distributive} if this map is always an isomorphism.
(When the category is a poset, we call it a \emph{distributive lattice}.)
However, as you saw in \cref{ex:lattices,ex:cat-prod-coprod}, if we simply combine the unary type theoretic rules for $\times$ and $+$, we get a type theory for categories with products and coproducts, but no interaction between them.
So unary type theory cannot deal with distributive categories while adhering to~\eqref{princ:independence} and~\eqref{princ:ump}.

Perhaps surprisingly, there \emph{is} a way to present a type theory for distributive categories.
The idea is to move into a world where the product $A\times B$ \emph{also} has a ``mapping out'' universal property, so that we can compare $(A\times B)+(A\times C)$ and $A\times (B+C)$ by saying they have the same universal property.
As we will see, this requires moving to a type theory with multiple antecedents.

This is one motivation.
Another is that we might want to talk about operations whose universal property can't be expressed in unary type theory while adhering to~\eqref{princ:independence}.
For instance, a \emph{cartesian closed category} has exponential objects such that morphisms $X\to Z^Y$ correspond to morphisms $X\times Y\to Z$; but how can we express this without referring to $\times$?
It turns out that the solution is the same.

We might also want to talk about operations that \emph{have} no obvious universal property, obviously violating~\eqref{princ:ump}.
For instance, what about monoidal categories?
In the usual presentation of a monoidal category, the tensor product $A\otimes B$ has no universal property.
It turns out that there is a way to give it a universal property, and this also leads us to higher-ary antecedents.

Finally, there is also another motivation not having anything to do with~\eqref{princ:independence} and~\eqref{princ:ump}: we may want to generalize the ``input data'' \cG from which we generate our free objects.
The unary type theory for categories with products allows us to prove theorems of the form ``in any category with products, for any morphisms $f:A\to B$, $g:C\to D$ \dots'', but it doesn't permit these quantifications to include ``for any $f:A\times B\to C$''.
This is because the type theory generates a free category-with-products from a directed graph \cG whose vertices and edges are the objects and morphisms we hypothesize; but there is no way to express $f:A\times B\to C$ as an edge of \cG, since \cG has no products of objects yet.
This problem turns out to have the same solution as well.

As already mentioned, on the type-theoretic side what we will do in this chapter is allow multiple types in the antecedent of a judgment (but still, for now, only one type in the consequent); we call these \emph{simple type theories}.
In a simple type theory the antecedent is often called the \emph{context}.

On the categorical side, what we will study are \emph{multicategories} of various sorts.
An ordinary multicategory is like a category but whose morphisms can have any finite list of objects as their domain, say $f:(A_1,\dots,A_n) \to B$, with a straightforward composition law.
There are many possible variations on this definition: in a symmetric multicategory the finite lists can be permuted, in a cartesian multicategory we can add unrelated objects and collapse equal ones, and so on.
All of these categorical structures are known as \emph{generalized multicategories}.
There is an abstract theory of generalized multicategories [TODO: cite] that includes these examples and many others, but (at least in the current version of this chapter) we will simply work with concrete examples.

Our approach to the semantics of simple type theory can be summed up in the following additional principle:
\begin{equation}\label{princ:structural}
  \parbox{4in}{\centering The shape of the context and the structural rules in a simple type theory, including admissible rules such as cut, should mirror the categorical structure of a generalized multicategory.}\tag{$\ddagger$}
\end{equation}
The \emph{structural rules} are the rules that don't refer to any operation on types, such as identity and cut.
(Later on we will meet other structural rules, such as exchange, contraction, and weakening.)
Principle~\eqref{princ:ump} then tells us that the \emph{non-structural} rules (which are sometimes called \emph{logical} rules) should all correspond to objects with universal properties in a generalized multicategory.

In sum, we have the following table of correspondences:
\begin{center}
  \begin{tabular}{c|c}
    Type theory & Category theory\\\hline
    Structural rules & Generalized multicategory\\
    Logical rules & Independent universal properties
  \end{tabular}
\end{center}
Here by ``independent universal properties'' I mean that the universal property of each object can be defined on its own without reference to any other objects defined by universal properties (unlike, for instance, the exponential in a cartesian closed category).

This all been very abstract, so I recommend the reader come back to this section again at the end of this chapter.
However, for completeness let me point out now that this general correspondence is particularly useful when designing new type theories and when looking for categorical semantics of existing type theories.
On one hand, any type theory that adheres to~\eqref{princ:independence} should have semantics in a kind of generalized multicategory that can be ``read off'' from the shape of its contexts and its structural rules.
On the other hand, to construct a type theory for a given categorical structure, we should seek to represent that structure as a generalized multicategory in which all the relevant objects have independent universal properties; then we can ``read off'' from the domains of morphisms in those multicategories the shape of the contexts and the structural rules of our desired type theory.

We will not attempt to make this correspondence precise in any general way, and in practice it has many tweaks and variations that would probably be exceptions to any putative general theorem; but it is a very useful heuristic.


\section{Introduction to multicategories}
\label{sec:multicats-catth}

From a categorical point of view, a multicategory can be regarded as an answer to the question ``in what kind of structure does a tensor product have a universal property?''
The classical tensor product of vector spaces (or, more generally, modules over a commutative ring) does have a universal property: it is the target of a universal bilinear map.
That is, there is a function $m:V\times W \to V\tensor W$ that is bilinear (i.e.\ $m(x,-)$ and $m(-,y)$ are linear maps for all $x\in V$ and $y\in W$), and any other bilinear map $V\times W \to U$ factors uniquely through $m$ by a linear map $V\tensor W \to U$.
Put differently, $V\tensor W$ is a representing object for the functor $\mathrm{Bilin}(V,W;-) : \bVect \to \bSet$.

Of course, this property determines the tensor product up to isomorphism (though of course one still needs some more or less explicit construction to ensure that such a representing object exists).
However, unlike many universal properties, it is not quite sufficient on its own to show that the tensor product behaves as desired.
In particular, to show that the tensor product is associative, we would naturally like to show that $V\tensor (W\tensor U)$ and $(V\tensor W)\tensor U$ are both representing objects for the functor of \emph{trilinear} maps $\mathrm{Trilin}(V,W,U;-)$; but this is not an abstract consequence of the fact that each binary tensor product represents bilinear maps.
What we need is a sort of ``relative representability'' $\mathrm{Trilin}(V,W,U;-) \cong \mathrm{Bilin}(V\tensor W,U;-)$, and similarly.

Finally, when we come to prove that these associativity isomorphisms satisfy the pentagon axiom of a monoidal category, we need analogous facts about quadrilinear maps, at which point it is clear that we should be talking about a general $n$.
A multicategory is the categorical context in which to do this: in addition to ordinary morphisms like an ordinary category (e.g.\ linear maps) it also contains $n$-ary maps for all $n\in\dN$ (e.g.\ multilinear maps).
Formally, just as a category is a directed graph with composition and identities, a multicategory is a \emph{multigraph} with composition and identities.

\begin{defn}\label{defn:multigraph}
  A \textbf{multigraph} \cG consists of a set of \emph{objects}, together with for every object $B$ and every finite list of objects $(A_1,\dots,A_n)$ a set of \emph{arrows} $\cG(A_1,\dots,A_n;B)$.
\end{defn}

Note that $n$ can be $0$.
We say that an arrow in $\cG(A_1,\dots,A_n;B)$ is \textbf{$n$-ary}; the special cases $n=0,1,2$ are \emph{nullary}, \emph{unary}, and \emph{binary}.

\begin{defn}
  A \textbf{multicategory} \cM is a multigraph together with the following structure and properties.
  \begin{itemize}
  \item For each object $A$, an identity arrow $\idfunc_A\in\cM(A;A)$.
  \item For any object $C$ and lists of objects $(B_1,\dots,B_m)$ and $(A_{i1},\dots,A_{in_i})$ for $1\le i\le m$, a composition operation
    \begin{align*}
      \cM(B_1,\dots,B_m;C) \times \prod_{i=1}^m \cM(A_{i1},\dots,A_{in_i};B_i) &\too \cM(A_{11},\dots,A_{mn_m};C)\\
      (g,(f_1,\dots,f_m)) &\mapsto g\circ (f_1,\dots,f_m)
    \end{align*}
    [TODO: Picture]
  \item For any $f\in\cG(A_1,\dots,A_n;B)$ we have
    \begin{mathpar}
      \idfunc_B \circ (f) = f\and
      f\circ (\idfunc_{A_1},\dots,\idfunc_{A_n}) = f.
    \end{mathpar}
  \item For any $h,g_i,f_{ij}$ we have
    \begin{mathpar}
      (h\circ (g_1,\dots,g_m))\circ (f_{11},\dots,f_{mn_m}) =
      h \circ (g_1\circ (f_{11},\dots,f_{1n_1}), \dots, g_m \circ (f_{m1},\dots,f_{mn_m}))
    \end{mathpar}
  \end{itemize}
\end{defn}

The objects and unary arrows in a multicategory form a category; indeed, a multicategory with only unary arrows is exactly a category.
Vector spaces and multilinear maps, as discussed above, are a good example to build intuition.

While the above definition is the natural one from a certain categorical perspective, there is another equivalent way to define multicategories.
If in the ``multi-composition'' $g\circ (f_1,\dots,f_m)$ all the $f_j$'s for $j\neq i$ are identities, we write it as $g \circ_i f_i$.
We may also write it as $g\circ_{B_i} f_i$ if there is no danger of ambiguity (e.g.\ if none of the other $B_j$'s are equal to $B_i$).
Thus we have operations
\begin{multline*}
  \circ_i : \cM(B_1,\dots,B_n;C) \times \cM(A_1,\dots,A_m;B_i) \\
  \too \cM(B_1,\dots,B_{i-1},A_1,\dots,A_m,B_{i+1},\dots,B_n;C)
\end{multline*}
that satisfy the following properties:
\begin{itemize}
\item $\idfunc_B \circ_1 f = f$ (since $\idfunc_B$ is unary, $\circ_1$ is the only possible composition here).
\item $f\circ_i \idfunc_{B_i} = f$ for any $i$.
\item $(h \circ_i g) \circ_{j} f$ is equal either to $h\circ_i (g\circ_k f)$ for some $k$ or to $(h\circ_k f)\circ_\ell g$ for some $k,\ell$, according to whether $j$ is such that $f$ is being plugged into one of the inputs of $g$ or into one of the remaining inputs of $h$.
  [TODO: Picture]
  From this idea one may deduce formulas for $k,\ell$ in terms of $i,j$.
\end{itemize}
Conversely, given the $\circ_i$ operations one may define
\[ g\circ (f_1,\dots,f_m) = (\cdots((g \circ_m f_m) \circ_{m-1} f_{m-1}) \cdots \circ_2 f_2) \circ_1 f_1 \]
to recover the original definition of multicategory.
We leave the details to the interested reader (\cref{ex:multicat-defns})

With multicategories in hand, we can give an abstract version of the characterization of the tensor product of vector spaces using multilinear maps.
\begin{defn}
  Given objects $A_1,\dots,A_n$ in a multicategory \cM, a \textbf{tensor product} of them is an object $\bigtensor_i A_i$ with a morphism $\chi:(A_1,\dots,A_n) \to \bigtensor_i A_i$ such that all the maps $(-\circ_i \chi)$ are bijections.
\end{defn}

When $n=2$ we write a binary tensor product as $A_1\tensor A_2$.
When $n=0$ we call a nullary tensor product a \textbf{unit object} and write $\one$.
When $n=1$ a unary tensor product is just an object isomorphic to $A$.

\begin{thm}\label{thm:multicat-repr}
  If in some multicategory \cM every pair of objects has a specified tensor product, and there is a unit object, then the underlying category of \cM inherits a monoidal structure.
\end{thm}
\begin{proof}
  It is easy to show that $(A_1\tensor A_2)\tensor A_3$ and $A_1 \tensor (A_2\tensor A_3)$, if they both exist, are both a ternary tensor product $\bigtensor_{i=1}^3 A_i$, and hence canonically isomorphic.
  Similarly, $A\tensor \one$ and $\one\tensor A$ are unary tensor products, hence canonically isomorphic to $A$.
  The coherence axioms follow similarly.
\end{proof}

A multicategory with the property of \cref{thm:multicat-repr} is said to be \textbf{representable}.
Conversely, given a \emph{monoidal} category \cM, we can make it a multicategory with $\cM(A_1,\dots,A_n;B) = \cM(\bigtensor_i A_i;B)$.
This yields an equivalence between monoidal categories and representable multicategories --- with the caveat that functors of multicategories (maps of multigraphs preserving identities and composition) correspond to \emph{lax} monoidal functors.
The strong monoidal functors correspond to the multicategory functors that ``preserve composites and units'' in the usual sense that functors preserve universal properties: if $\chi:(A,B)\to A\tensor B$ is a tensor product then so is its image $F\chi :(FA,FB)\to F(A\tensor B)$ and so on.
And if the domain and codomain are equipped with chosen tensor products and units, then the strict monoidal functors are those that preserve these chosen ones exactly.

There are plenty of good references on multicategories, so we will not belabor them further here.
See for instance~\cite{hermida:multicats,leinster:higher-opds}.


\subsection*{Exercises}

\begin{ex}\label{ex:multicat-defns}
  Write down the axioms for $\circ_i$ more explicitly, and prove that the two definitions of multicategory are equivalent.
\end{ex}


\section{Multiposets and monoidal posets}
\label{sec:multiposets-monpos}

\subsection{Multiposets}
\label{sec:multiposets}

We begin our study of type theory for multicategories with the posetal case.
A \textbf{multiposet} is a multicategory in which each set $\cM(A_1,\dots,A_n;B)$ has at most one element.
We consider the adjunction between the category \bMPos of multiposets and the category \bRelMGr of \emph{relational multigraphs}, i.e.\ sets of objects equipped with an $n$-ary relation ``$(a_1,\dots,a_{n-1})\le b$'' for all integers $n\ge 1$.
We would like to construct the free multiposet on a relational multigraph \cG using a type theory.

Its objects, of course, will be those of \cG, so we do not yet need a type judgment.
We represent its relations using a judgment written ``$A_1,A_2,\dots,A_n \types B$''.
As is customary, we use capital Greek letters such as $\Gamma$ and $\Delta$ to stand for finite lists (possibly empty) of types.
We write ``$\Gamma_1,\Gamma_2,\dots,\Gamma_n$'' for the concatenation of such lists, and we also write for instance ``$\Gamma,A,\Delta$'' to indicate a list containing the type $A$ somewhere the middle.

At the moment, the only rules for this judgment will be identities and those coming from \cG.
Based on the lessons we learned from unary type theory, we represent the latter in Yoneda-style.
\begin{mathpar}
  \inferrule{ }{A\types A}\and
  \inferrule{(A_1,\dots,A_n \le B)\in\cG \\ \Gamma_1\types A_1 \\ \dots \\ \Gamma_n \types A_n}{\Gamma_1,\dots,\Gamma_n\types B}
\end{mathpar}
We call this the \textbf{cut-free type theory for multiposets under \cG}.
Note that we have chosen to use the ``multi-composition'' in Yoneda-ifying the relations in \cG; this will be necessary for admissibility of cut.
By contrast, it is traditional to formulate the cut rule in terms of the $\circ_i$ compositions.

\begin{thm}\label{thm:multiposet-cutadm}
  In the cut-free type theory for multiposets under \cG, the following cut rule is admissible: if we have derivations of $\Gamma\types A$ and of $\Delta,A,\Psi\types B$, then we can construct a derivation of $\Delta,\Gamma,\Psi\types B$.
\end{thm}
\begin{proof}
  We induct on the derivation of $\Delta,A,\Psi\types B$.
  If it is the identity rule, then $A=B$ and $\Delta$ and $\Psi$ are empty, so our given derivation of $\Gamma\types A$ is all we need.
  Otherwise, it comes from some relation $A_1,\dots,A_n \le B$ in \cG, where we have derivations of $\Gamma_i \types A_i$, and there is an $i$ such that $\Gamma_i = \Gamma_i',A,\Gamma_i''$, while $\Delta = \Gamma_1,\dots,\Gamma_{i-1},\Gamma_i'$ and $\Psi = \Gamma_i'',\Gamma_{i+1},\dots,\Gamma_n$.
  Now by the inductive hypothesis, we can construct a derivation of $\Gamma_i',\Gamma,\Gamma_i''\types A_i$.
  Applying the rule for $A_1,\dots,A_n \le B$ again, with this derivation in place of the original $\Gamma_i \types A_i$, gives the desired result.
\end{proof}

\begin{thm}\label{thm:multiposet-initial}
  For any relational multigraph \cG, the free multiposet it generates has the same objects, and the relation $(A_1,\dots,A_n)\le B$ holds just when the sequent $A_1,\dots,A_n\types B$ is derivable in the cut-free type theory for multiposets under \cG.
\end{thm}
\begin{proof}
  \cref{thm:multiposet-cutadm}, together with the identity rule, tells us that this defines a multiposet $\F\bMPos\cG$.
  If $\cM$ is any other multiposet with a map $P:\cG\to\cM$ of relational multigraphs, then since the objects of $\F\bMPos\cG$ are those of \cG, there is at most one extension of $P$ to $\F\bMPos\cG$.
  It suffices to check that the relations in $\F\bMPos\cG$ hold in $\cM$; but this is clear since $\cM$ is a multiposet and the only rules are an identity and a particular multi-transitivity.
\end{proof}

Now we augment the type theory for multiposets with operations representing a tensor product.
Since the tensor product now has a universal property, this is essentially straightforward.
First of all, we need a type judgment $\types A\type$, with unsurprising rules:
\begin{mathpar}
  \inferrule{A\in\cG}{\types A\type}\and
  \inferrule{ }{\types \one\type}\and
  \inferrule{\types A\type \\ \types B\type}{\types A\tensor B\type}
\end{mathpar}

Second, in addition to the rules from \cref{sec:multiposets}, we have rules for $\tensor$.
Once again we need to make a choice between sequent calculus and natural deduction.

\subsection{Sequent calculus for monoidal posets}
\label{sec:seqcalc-monpos}

In sequent calculus the left rule expresses the universal property; while
the right rule should be the universal relation $A,B\types A\tensor B$, but we have to Yoneda-ify it using the multicomposition.
The rules for $\one$ are similar.
\begin{mathpar}
  \inferrule{\Gamma,A,B,\Delta\types C}{\Gamma,A\tensor B,\Delta\types C}\;\tensorL\and
  \inferrule{\Gamma\types A \\ \Delta\types B}{\Gamma,\Delta\types A\tensor B}\;\tensorR\and
  \inferrule{\Gamma,\Delta\types A}{\Gamma,\one,\Delta\types A}\;\one L\and
  \inferrule{ }{\types \one}\;\one R
\end{mathpar}
This defines the \textbf{sequent calculus for monoidal posets under \cG}.
Note the presence of the additional contexts $\Gamma$ and $\Delta$ in $\tensorL$ and $\one L$, which corresponds to the strong universal property of a tensor product in a multicategory referring to $n$-ary arrows for all $n$.

\begin{thm}\label{thm:monpos-identity}
  The general identity rule is admissible in the sequent calculus for monoidal posets under \cG: if $\types A\type$ is derivable, then so is $A\types A$.
\end{thm}
\begin{proof}
  By induction on the derivation of $\types A\type$.
  If $A\in \cG$, then $A\types A$ is an axiom.
  If $A=\one$, then $\one\types \one$ has the following derivation:
  \begin{mathpar}
    \inferrule*[Right=$\one L$]{\inferrule*[Right=$\one R$]{ }{\types \one}}{\one\types \one}
  \end{mathpar}
  And if $A=B\tensor C$, by the inductive hypothesis we have derivations $\sD_B$ and $\sD_C$ of $B\types B$ and $C\types C$, which we can put together like so:
  \begin{equation*}
    \inferrule*[Right=$\tensorL$]{
      \inferrule*[Right=$\tensorR$]{
        \inferrule*{\sD_B\\\\\vdots}{B\types B}\\
        \inferrule*{\sD_C\\\\\vdots}{C\types C}
      }{
        B,C\types B\tensor C
      }
    }{
      B\tensor C\types B\tensor C
    }\qedhere
  \end{equation*}
\end{proof}

\begin{thm}\label{thm:monpos-cutadm}
  Cut is admissible in the sequent calculus for monoidal posets under \cG: if we have derivations of $\Gamma\types A$ and of $\Delta,A,\Psi\types B$, then we can construct a derivation of $\Delta,\Gamma,\Psi\types B$.
\end{thm}
\begin{proof}
  As always, we induct on the derivation of $\Delta,A,\Psi\types B$.
  The cases of the identity and of relations from \cG are just as in \cref{thm:multiposet-cutadm}
  It cannot end with a $\one R$.
  If it ends with a $\tensorR$, we use the inductive hypotheses on its premises and apply $\tensorR$ again.

  The cases when it ends with a left rule introduce a new feature that we have not seen before in cut-admissibility proofs.
  Suppose it ends with a $\one L$.
  If $A$ is the $\one$ that was introduced by this rule, then we proceed basically as before: if $\Gamma\types A$ is $\one R$, so that $\Gamma$ is empty, then we are in the principal case and we can simply use the given derivation of $\Delta,\Psi\types B$; while if it is a left rule then we can apply a secondary induction.
  But it might also happen that $A$ is a different type, appearing in $\Delta$ or $\Psi$.
  However, this case is also easily dealt with by applying the inductive hypothesis to $\Gamma\types A$ and the given $\Delta,\Psi\types B$ (with $A$ appearing somewhere in its antecedents).

  The case of $\tensorL$ is similar.
  In the interesting (principal) case, when the sequent $\Delta,A_1\tensor A_2,\Psi\types B$ is derived from $\Delta,A_1,A_2,\Psi\types B$, while $\Gamma\types A_1\tensor A_2$ is derived using $\tensorR$ from $\Gamma_1\types A_1$ and $\Gamma_2\types A_2$ (so that necessarily $\Gamma = \Gamma_1,\Gamma_2$), then we can apply the inductive hypothesis twice as follows:
  \begin{equation*}
    \inferrule*[Right=cut]{
      \Gamma_2\types A_2\\
      \inferrule*[Right=cut]{
        \Gamma_1\types A_1\\
        \Delta,A_1,A_2,\Psi\types B
      }{
        \Delta,\Gamma_1,A_2,\Psi\types B
      }
    }{
      \Delta,\Gamma_1,\Gamma_2,\Psi\types B
    }
  \end{equation*}
  We have to apply it twice because we are formulating our cut rule using $\circ_i$ composition.
  If we were using multicomposition then we would only need to apply it once; but the notation for the whole argument would become much less wieldy that way.
\end{proof}

We are ready to prove the initiality theorem, relating to an adjunction between the categoris \bRelMGr of relational multigraphs and \bMonPos of monoidal posets.
As always, the morphisms in our categories will be completely strict: so in particular the morphisms in \bMonPos are \emph{strict} monoidal functors.

\begin{thm}\label{thm:monpos-initial}
  For any relational multigraph \cG, the free monoidal poset generated by \cG is described by the sequent calculus for monoidal posets under \cG: its objects are the $A$ such that $\types A\type$ is derivable, while the relation $(A_1,\dots,A_n)\le B$ holds just when the sequent $A_1,\dots,A_n\types B$ is derivable.
\end{thm}
\begin{proof}
  As before, \cref{thm:monpos-identity,thm:monpos-cutadm} show that this defines a multiposet $\F\bMonPos\cG$.
  Moreover, the rules for $\tensor$ and $\one$ tell us that it is representable, hence monoidal.

  Now suppose $P:\cG\to\cM$ is a map into the underlying multiposet of any other monoidal poset.
  We can extend $P$ uniquely to a function from the objects of $\F\bMonPos\cG$ to those of $\cM$ preserving $\tensor$ and $\one$ on objects, so it remains to check that this is a map of multiposets and preserves the universal properties of $\tensor$ and $\one$.
  However, $\tensorR$ and $\one R$ are preserved by the universal data of $\tensor$ and $\one$ in \cM, while the universal properties of these data mean that $\tensorL$ and $\one L$ are also preserved.
\end{proof}

\subsection{Natural deduction for monoidal posets}
\label{sec:natded-monpos}

In natural deduction, the introduction rules $\tensorI$ and $\one I$ will coincide with the right rules $\tensorR$ and $\one R$ from the sequent calculus, but now we need elimination rules.
Since $\tensor$ and $\one$ in a multicategory have a ``mapping out'' universal property, these elimination rules will be reminiscent of the ``case analysis'' rules from \cref{sec:catcoprod}.
Formally speaking, they can be obtained by simply cutting $\tensorL$ and $\one L$ with an arbitrary sequent:
\begin{mathpar}
  \inferrule*[Right=cut]{
    \Psi \types A\tensor B \\
    \inferrule*[Right=$\tensorL$]{\Gamma,A,B,\Delta\types C}{\Gamma,A\tensor B,\Delta\types C}
  }{
    \Gamma,\Psi,\Delta \types C
  }\and
  \inferrule*[Right=cut]{
    \Psi\types \one \\
    \inferrule*[Right=$\one L$]{\Gamma,\Delta\types A}{\Gamma,\one,\Delta\types A}
  }{
    \Gamma,\Psi,\Delta\types C
  }
\end{mathpar}
As usual in a natural deduction, we also need to assert the identity rule for all types.
Thus our complete \textbf{natural deduction for monoidal posets under \cG} consists of (the rules for $\types A\type$ and):
\begin{mathpar}
  \inferrule{\types A\type}{A\types A}\and
  \inferrule{(A_1,\dots,A_n \le B)\in\cG \\ \Gamma_1\types A_1 \\ \dots \\ \Gamma_n \types A_n}{\Gamma_1,\dots,\Gamma_n\types B}\and
  \inferrule{\Gamma\types A \\ \Delta\types B}{\Gamma,\Delta\types A\tensor B}\;\tensorI\and
  \inferrule{
    \Psi \types A\tensor B \\
    \Gamma,A,B,\Delta\types C
  }{
    \Gamma,\Psi,\Delta \types C
  }\;\tensorE\\
  \inferrule{ }{\types \one}\;\one I\and
  \inferrule{
    \Psi\types \one \\
    \Gamma,\Delta\types A
  }{
    \Gamma,\Psi,\Delta\types C
  }\;\one E
\end{mathpar}
We leave the metatheory of this as an exercise (\cref{ex:natded-monpos}).

\subsection*{Exercises}

\begin{ex}\label{ex:natded-monpos}
  Prove the well-formedness, cut-admissibility, and initiality theorems for the natural deduction for monoidal posets.
\end{ex}

\begin{ex}\label{ex:monpos-mslat}
  Write down either a sequent calculus or a natural deduction for monoidal posets that are also meet-semilattices, and prove its initiality theorem.
\end{ex}

\begin{ex}\label{ex:monpos-jslat}
  Let us augment the sequent calculus for monoidal posets by the following versions of the rules for join-semilattices:
  \begin{mathpar}
    \inferrule{\types A\type \\ \types B\type}{\types A\join B\type}\and
    \inferrule{ }{\types \bot\type}\and
    \inferrule{\Gamma \types A}{\Gamma\types A\join B}\and
    \inferrule{\Gamma \types B}{\Gamma\types A\join B}\and
    \inferrule{\Gamma,A,\Delta \types C\\\Gamma,B,\Delta \types C}{\Gamma,A\join B,\Delta\types C}\and
    \inferrule{ }{\Gamma,\bot,\Delta\types C}
  \end{mathpar}
  \begin{enumerate}
  \item Construct derivations in this calculus of the following sequents:
    \begin{align*}
      (A\tensor B)\join (A\tensor C) &\types  A\tensor (B\join C)\\
      A\tensor (B\join C) &\types (A\tensor B)\join (A\tensor C)
    \end{align*}
  \item What categorical structure do you think models this type theory?
  \end{enumerate}
\end{ex}


\section{Multicategories and monoidal categories}
\label{sec:multicat-moncat}

Now we are ready to move back up to categories, distinguishing between derivations by introducing a term calculus.

\subsection{Multicategories}
\label{sec:multicats}

Categorically, we begin with the adjunction between the category $\bMCat$ of multicategories and the category $\bMGr$ of multigraphs.
Let \cG be a multigraph; we now augment the cut-free theory of \cref{sec:multiposets} with terms that exactly represent the structure of derivations, as we did in \cref{sec:categories,sec:catprod,sec:catcoprod}.
\begin{mathpar}
  \inferrule{A\in\cG}{x:A\types x:A}\and
  \inferrule{f\in \cG(A_1,\dots,A_n;B) \\ \Gamma_1\types M_1:A_1 \\ \dots \\ \Gamma_n \types M_n:A_n}{\Gamma_1,\dots,\Gamma_n\types f(M_1,\dots,M_n):B}\and
\end{mathpar}
We call this the \textbf{cut-free type theory for multicategories under \cG}.
Note that it has the following property.

\begin{lem}\label{thm:multicat-linear}
  If $\Gamma\types M:B$ is derivable, then every variable in $\Gamma$ appears exactly once in $M$.
\end{lem}
\begin{proof}
  By induction on the derivation.
  The identity rule $x:A\types x:A$ clearly has this property.
  And in the conclusion of the generator rule each variable appears in exactly one $\Gamma_i$, hence can only appear in one of the $M_i$'s, and by induction it appears exactly once there; hence it appears exactly once in $f(M_1,\dots,M_n)$.
\end{proof}

In type-theoretic lingo, \cref{thm:multicat-linear} says that this type theory is \textbf{linear} (just like a linear polynomial uses each variable exactly once, a linear type theory uses each variable exactly once).
Note that linearity is a property of a system that we \emph{prove}, not a requirement that we impose from outside.

\begin{thm}\label{thm:multicat-subadm}
  Substitution is admissible in the cut-free type theory for multicategories under \cG: given derivations of $\Gamma\types M:A$ and of $\Delta,x:A,\Psi\types N:B$, we can construct a derivation of $\Delta,\Gamma,\Psi\types M[N/x]:B$.
\end{thm}
\begin{proof}
  Just like \cref{thm:multiposet-cutadm}.
\end{proof}

As before, note that we can regard this as defining substitution; its inductive clauses are
\begin{align*}
  x[M/x] &= M\\
  f(N_1,\dots,N_n)[M/x] &= f(N_1,\dots,N_{i-1},N_i[M/x],N_{i+1},\dots,N_n)
\end{align*}
where $i$ is the unique index such that $x$ occurs in $N_i$ (which exists by \cref{thm:multicat-linear}).

\begin{thm}\label{thm:multicat-subassoc}
  Substitution in the cut-free type theory for multicategories satisfies the associativity/interchange rules of the $\circ_i$ operations:
  \begin{enumerate}
  \item If $\Gamma\types M:A$ and $\Delta,x:A,\Delta' \types N:B$ and $\Psi,y:B,\Psi'\types P:C$, then\label{item:multicat-subassoc-1}
    \[ P[N/y][M/x] = P[N[M/x]/y] \]
  \item If $\Gamma\types M:A$ and $\Delta \types N:B$ and $\Psi,x:A,\Psi',y:B,\Psi''\types P:C$, then\label{item:multicat-subassoc-2}
    \[ P[N/y][M/x] = P[M/x][N/y] \]
  \end{enumerate}
\end{thm}
\begin{proof}
  In both cases we induct on the derivation of $P$.
  For~\ref{item:multicat-subassoc-1}, if $P=y$ then both sides are $N[M/x]$.
  If $P=f(P_1,\dots,P_n)$, suppose $y$ occurs in $P_i$.
  Then $P[N/y] = f(P_1,\dots,P_i[N/y],\dots,P_n)$ and $x$ occurs in $P_i[N/y]$, so
  $P[N/y][M/x] = f(P_1,\dots,P_i[N/y][M/x],\dots,P_n)$ and the inductive hypothesis applies.

  For~\ref{item:multicat-subassoc-2}, we can't have $P$ being a single variable since there are two distinct variables in its context.
  Thus it must be $f(P_1,\dots,P_n)$, with $x$ and $y$ appearing in $P_i$ and $P_j$ respectively.
  If $i=j$, then we simply apply the inductive hypothesis to $P_i$; while if $i\neq j$ then
  \begin{equation*}
    P[N/y][M/x] = f(P_1,\dots,P_i[M/x],\dots,P_j[N/y],\dots,P_n) = P[M/x][N/y]\qedhere
  \end{equation*}
\end{proof}

\begin{thm}\label{thm:multicat-initial}
  For any multigraph \cG, the free multicategory generated by \cG can be described by the cut-free type theory for multicategories under \cG: its objects are those of \cG, and its morphisms $\Gamma\to B$ are the derivable judgments $\Gamma\types M:B$.
\end{thm}
\begin{proof}
  \cref{thm:multicat-subadm} tells us how to compose multimorphisms in the $\circ_i$ style, and \cref{thm:multicat-subassoc} verifies the associativity/interchange axiom for these.
  (If you did \cref{ex:multicat-defns}, feel free to verify this by translating the conclusions of \cref{thm:multicat-subassoc} back into numerical indices.
  Note how much more comprehensible it is when expressed using abstract variables.)
  The two identity axioms are $x[M/x]=M$ (one of the defining clauses of substitution) and ``$M[y/x] = M$'', which looks false or nonsensical but is actually just an instance of $\alpha$-equivalence (recall that we always consider type theories up to $\alpha$-equivalence, i.e.\ renaming of variables).

  Thus, we have a multicategory $\F\bMCat\cG$.
  Let \cM be any multicategory and $P:\cG\to\cM$ a map of multigraphs; as usual we extend $P$ to the morphisms of $\F\bMCat\cG$ by induction on derivations, and such an extension is forced since the rules are all instances of functoriality.
  Finally we verify by induction on derivations that this extension is in fact functorial on all composites.
\end{proof}


\subsection{Monoidal categories}
\label{sec:moncat}

We proceed to annotate the natural deduction for monoidal posets from \cref{sec:natded-monpos} with terms, obtaining the \textbf{simple type theory for monoidal categories under \cG} as shown in \cref{fig:moncat}.

As noted in \cref{sec:natded-monpos}, since the tensor product has a ``mapping out'' universal property like that of a coproduct, its elimination rule is a sort of ``case analysis'' that decomposes an element of $A\tensor B$ into an element of $A$ and an element of $B$.
Just as the rule $\plusE$ says that ``to do something with an element of $A+B$, it suffices to assume that it is either $\inl(u)$ or $\inr(v)$'', the rule $\tensorE$ says that ``to do something with an element of $A\tensor B$, it suffices to assume that it is $\pair{x}{y}$.''
And just as the term syntax $\case(M,u.P,v.Q)$ for coproducts binds the variables $u$ in $P$ and $v$ in $Q$, the term syntax $\match_\tensor(M,xy.N)$ binds the variables $x$ and $y$ in $N$.

The case of the unit is similar but simpler: to do something with an element of $\one$, it suffices to assume that it is $\ttt$ --- hence gives no extra information, i.e.\ no new variables are bound.
Thus, the term syntax $\match_\one(M,N)$ binds nothing in $N$, it simply allows us to ignore $M$.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\types A\type}{x:A\types x:A}\and
    \inferrule{f\in \cG(A_1,\dots,A_n;B) \\ \Gamma_1\types M_1:A_1 \\ \dots \\ \Gamma_n \types M_n:A_n}{\Gamma_1,\dots,\Gamma_n\types f(M_1,\dots,M_n):B}\and
    \inferrule{\Gamma\types M:A \\ \Delta\types N:B}{\Gamma,\Delta\types \pair{M}{N}:A\tensor B}\;\tensorI\and
    \inferrule{
      \Psi \types M:A\tensor B \\
      \Gamma,x:A,y:B,\Delta\types N:C
    }{
      \Gamma,\Psi,\Delta \types \match_\tensor(M,xy.N):C
    }\;\tensorE\\
    \inferrule{ }{\types \ttt:\one}\;\one I\and
    \inferrule{
      \Psi\types M:\one \\
      \Gamma,\Delta\types N:A
    }{
      \Gamma,\Psi,\Delta\types \match_\one(M,N):C
    }\;\one E
  \end{mathpar}
  \caption{Simple type theory for monoidal categories}
  \label{fig:moncat}
\end{figure}

The $\beta$- and $\eta$-conversion rules implement the universal properties:
\begin{mathpar}
  \match_\tensor(\pair M N,xy.P) \equiv P[M/x,N/y]\and
  \match_\tensor(M,xy.N[\pair xy/u]) \equiv N[M/u]\\
  \match_\one(\ttt,N) \equiv N\and
  \match_\one(M,N[\ttt/u]) \equiv N[M/u]
\end{mathpar}
[TODO: Maybe say more here, depending on what we said about coproducts.]

\begin{lem}\label{thm:moncat-linear}
  If $\Gamma\types M:B$ is derivable in the simple type theory for monoidal categories under \cG, then every variable in $\Gamma$ appears exactly once in $M$.
\end{lem}
\begin{proof}
  By induction on the derivation.
  The cases of variables and generators are as in \cref{thm:multicat-linear}.
  For a pair $\pair M N$ coming from $\tensorI$, each variable in $\Gamma,\Delta$ appears in exactly one of $\Gamma$ and $\Delta$, hence in exactly one of $M$ and $N$; we then apply the inductive hypotheses to $M$ or $N$ respectively.
  Similarly, for $\match_\tensor(M,xy.N)$ coming from $\tensorE$, each variable in $\Gamma,\Psi,\Delta$ must appear in exactly one of $\Gamma$, $\Psi$, or $\Delta$; by induction then in the first and third cases it must appear exactly once in $N$, and in the second case it must appear exactly once in $M$.
  The case of $\one E$ is similar, while there are no variables at all in $\ttt$.
\end{proof}

\begin{lem}\label{thm:moncat-subadm}
  Substitution is admissible in the simple type theory for monoidal categories under \cG, in the same sense as \cref{thm:multicat-subadm}.
  Moreover, it is associative and interchanging in the same sense as \cref{thm:multicat-subassoc}.
\end{lem}
\begin{proof}
  We sketch the proof by giving the defining clauses for the substitution operation:
  \begin{alignat*}{2}
    x[M/x] &= M\\
    f(N_1,\dots,N_n)[M/x] &= f(N_1,\dots,N_i[M/x],\dots,N_n) &&\quad\text{if $x$ occurs in $N_i$}\\
    \pair P Q[M/x] &= \pair{P[M/x]}{Q} &&\quad\text{if $x$ occurs in $P$}\\
    \pair P Q[M/x] &= \pair{P}{Q[M/x]} &&\quad\text{if $x$ occurs in $Q$}\\
    \match_\tensor(N,uv.P)[M/x] &= \match_\tensor(N[M/x],uv.P) &&\quad\text{if $x$ occurs in $N$}\\
    \match_\tensor(N,uv.P)[M/x] &= \match_\tensor(N,uv.P[M/x]) &&\quad\text{if $x$ occurs in $P$}\\
    \ttt[M/x] &&&\quad\text{cannot occur}\\
    \match_\one(N,P)[M/x] &= \match_\one(N[M/x],P) &&\quad\text{if $x$ occurs in $N$}\\
    \match_\one(N,P)[M/x] &= \match_\one(N,P[M/x]) &&\quad\text{if $x$ occurs in $P$}
  \end{alignat*}
  To summarize, we simply check which of the input terms the variable being substituted for appears in, and substitute recursively in that term.
  As in \cref{sec:catcoprod} [TODO], we are assuming that the bound variables $u,v$ are distinct from the variable $x$ being substituted for; if they are not then we have to rename them first.

  The proof of associativity and interchange is essentially the same as before: all the other rules behave just like the generator rules, except for $\ttt$ where the claim is trivial.
\end{proof}

\begin{thm}\label{thm:moncat-initial}
  The free monoidal category generated by a multigraph \cG (or, more precisely, its underlying multicategory) can be described by the simple type theory for monoidal categories under \cG: its objects are the $A$ such that $\types A\type$, and its morphisms are the term judgments $\Gamma\types M:A$ modulo the congruence $\equiv$.
\end{thm}
\begin{proof}
  \cref{thm:moncat-subadm} shows that we obtain a multicategory $\F\bMonCat\cG$ this way, just as in \cref{thm:multicat-initial}.
  The rules for $\tensor$ and $\one$, together with the $\beta$- and $\eta$-rules for $\equiv$, tell us that it is representable, and hence a monoidal category.
  Now if \cM is a monoidal category and $P:\cG\to\cM$ a map of multigraphs, we extend it to $\F\bMonCat\cG$ by induction on derivations (of objects and morphisms and equalities) using the fact that \cM is a representable multicategory, observe that this definition is forced by functoriality and (strict) preservation of the monoidal structure, and then prove by induction that it is indeed a functor.
\end{proof}


\section{Symmetric monoidal categories}
\label{sec:symmoncat}

[Symmetric multicategories and their type theory, exchange rule, structural rules in general.  How exactly do the structural rules work with initiality?  Should we use finite sets (e.g.\ of variables) instead of natural numbers to index the domains of morphisms in our symmetric multicategories?]

We can make exchange admissible by incorporating a shuffle into the generator rule (and presumably doing similarly with the other rules that manipulate contexts).
I've never seen that done before, but I think it's more standard that we can make exchange, contraction, and weakening all admissible by allowing the variable rule to extract from anywhere in the context.

Can we also make sense of ``multisets''?


\section{Introduction to cartesian multicategories}
\label{sec:cartmulti}

[Define cartesian multicategories, cartesian multiposets, representability, products have both universal properties]


\section{Cartesian monoidal categories}
\label{sec:cartmoncat}

[Not sure exactly where the section boundaries should be here, or how to mix introducing the categorical structures with the type theories.
Where to talk about distributive categories and distributive lattices?]


\section{Algebraic theories and operads}
\label{sec:algthy-opd}

[A multigraph is also known as a signature of function symbols.  An arbitrary set of equations between terms can be added to $\equiv$, thereby describing a ``presented'' rather than a free categorical structure.  For cartesian monoidal categories, this is a (multi-sorted) algebraic theory.  For (symmetric) monoidal categories, it is a syntactic approach to (colored) operads.]


\section{Closed monoidal categories}
\label{sec:clmoncat}


\section{Intuitionistic propositional logic}
\label{sec:int-logic}


\chapter{Classical type theories}
\label{chap:polycats}



\section{Polycategories and linear logic}
\label{sec:cllin}

[Mention linearly distributive categories and $\ast$-autonomous categories.
But don't belabor them, and perhaps just cite references like~\cite{cs:wkdistrib} for their universal characterizations and initiality theorems.]


\section{Classical logic}
\label{sec:classical}

[Cartesian polycategories]


\section{Posetal props and symmetric monoidal posets}
\label{sec:proppos-smpos}

Now let us consider sequents $\Gamma\types\Delta$ where the commas on both sides are intended to represent the \emph{same} tensor product $\tensor$.
For simplicity, we assume this tensor product is symmetric, so that we have an exchange rule on both sides.
This leads us to consider different identity and cut rules (in fact, two identity rules):
\begin{mathpar}
  \inferrule*{ }{()\types()}\and
  \inferrule*{\Gamma\types\Delta}{\Gamma,A\types \Delta,A}\and
  \inferrule*[Right=cut]{\Gamma\types\Xi,\Psi \\ \Psi,\Phi \types \Delta}{\Gamma,\Phi \types \Delta,\Xi}
\end{mathpar}
As always, of course, we intend for the cut rule to be admissible, but writing down at this point what it should be helps us build it into other rules appropriately.

To prove an initiality theorem, we need an appropriate categorical structure.
We define a \textbf{prop} to be a symmetric polygraph \cP together with the following data:
\begin{enumerate}
\item A morphism $\idfunc_{()}:()\to ()$.
\item For every $A\in\cP$, a morphism $\idfunc_A (A)\to (A)$.
  % For every $f:\Gamma\to\Delta$ and object $A$, a morphism $(f,\idfunc_A):(\Gamma,A) \to (\Delta,A)$.
  % By induction from this and the previous, we have $\idfunc_{\Gamma} :\Gamma\to\Gamma$ for any $\Gamma$.
\item For every $f:\Gamma\to (\Xi,\Psi)$ and $g:(\Psi,\Phi)\to \Delta$, a composite $g\circ_\Psi f : (\Gamma,\Phi) \to (\Xi,\Delta)$.
% \item Identities are invariant under permutation: $\sigma\idfunc_{\Gamma}\sigma = \idfunc_{\sigma\Gamma}$.
\item Composition is invariant under permutation: $\tau(g\circ_\Psi f)\sigma = (\tau g)\circ_\Psi (f\sigma)$ and $g\sigma \circ_\Psi f = g\circ_{\sigma \Psi} \sigma f$.
\item Composition is unital:
  % $g\circ_\Psi \idfunc_\Psi = g$ and $\idfunc_\Psi\circ_\Psi f = f$.
  $g\circ_A \idfunc_A = g$ and $\idfunc_A\circ_A f = f$.
\item Composition is associative: given $f:\Gamma\to (\Xi,\Theta,\Psi)$ and $g:(\Psi,\Phi)\to (\Delta,\Upsilon)$ and $h:(\Upsilon,\Theta,\Lambda)\to \Omega$, we have
  \[h \circ_{\Upsilon,\Theta} (g\circ_\Psi f) = (h\circ_\Upsilon g) \circ_{\Theta,\Psi} f \qquad \text{(as morphisms} (\Gamma,\Phi,\Lambda) \to (\Xi,\Delta,\Omega)). \]
\item Composition is interchanging: $g\circ_{()}f = f\circ_{()}g$ (modulo a symmetry action).
\end{enumerate}
By composing along empty lists, we get a ``tensor product'' of morphisms: if $f:\Gamma\to\Xi$ and $g:\Phi\to\Delta$, then $g\circ_{()}f : (\Gamma,\Phi) \to (\Xi,\Delta)$.
(This sort of operation is what distinguishes a prop from a ``properad''.)
In particular, we can produce identity morphisms for lists: $\idfunc_{(A,B)} = \idfunc_A \circ_{()} \idfunc_B$ and so on.

Similarly we have $f\circ_{()}g : (\Phi,\Gamma) \to (\Delta,\Xi)$, and the interchange axiom means that these two morphisms are related by the appropriate symmetric group actions.
In the special case when $\Gamma=\Phi=\Xi=\Delta=()$, the Eckmann--Hilton argument implies that the morphisms $()\to ()$ form a commutative monoid; thus our props are the original ones of Adams--MacLane~\cite{maclane:natural-assoc,maclane:cat-alg} rather than the ``graphical'' ones of Batanin--Berger~\cite{bb:htapm}.

It may not be obvious that the above axioms do actually give the same notion of prop.
Eventually we will prove this; but in this section we restrict to the posetal case, in which case the axioms are irrelevant.
By a \textbf{posetal prop} we mean a prop in which there is at most one morphism $\Gamma\to\Delta$ for any $\Gamma,\Delta$.
Given a relational polygraph \cG, we define the \textbf{type theory for posetal props under \cG} to have the two identity rules and a Yoneda-ified generator rule:
\begin{mathpar}
  \inferrule{ }{()\types()}\and
  \inferrule{\Gamma\types\Delta}{\Gamma,A\types \Delta,A}\and
  \inferrule{\Gamma\types\Xi,\Psi \\ f\in\cG(\Psi,\Phi; \Delta)}{\Gamma,\Phi \types \Delta,\Xi}
\end{mathpar}

\begin{thm}\label{thm:prop-cutadm}
  For any polygraph \cG, the above cut rule is admissible in the type theory for posetal props under \cG.
\end{thm}
\begin{proof}
  As always, we induct on the derivation of $\Psi,\Phi \types \Delta$.
  \begin{enumerate}
  \item If it is the empty identity rule $()\types()$, then $\Gamma\types\Xi,\Psi$ is just $\Gamma\types\Xi$ and is also the desired conclusion.
  \item If it ends with the other identity rule, then there are two cases.
    \begin{enumerate}
    \item If $A$ is in $\Phi$, then we have $\Phi=\Phi',A$ and $\Delta=\Delta',A$ and a derivation of $\Psi,\Phi'\types\Delta'$.
      Applying the inductive hypothesis to this we get $\Gamma,\Phi'\types \Xi,\Delta'$, and then applying the identity rule again gives the desired conclusion.
    \item If $A$ is in $\Psi$, then we have $\Psi=\Psi',A$ and $\Delta=\Delta',A$ and a derivation of $\Psi',\Phi\types\Delta'$.
      Now we can inductively cut this with the given $\Gamma\types \Xi,A,\Psi'$ along $\Psi'$ to get $\Gamma,\Phi\types \Xi,A,\Delta'$ which is the desired conclusion.
    \end{enumerate}
  \item Finally, suppose $\Psi,\Phi \types \Delta$ ends with the rule for an $f$.
    Thus, we have $\Delta=\Delta_1,\Delta_2$ and $\Psi=\Psi_1,\Psi_2$ and $\Phi=\Phi_1,\Phi_2$, where $f\in\cG(\Psi_2,\Phi_2,\Upsilon;\Delta_1)$ and a given derivation of $\Psi_1,\Phi_1\types \Delta_2,\Upsilon$.
    We first inductively cut the latter with our given $\Gamma\types \Xi,\Psi_1,\Psi_2$ along $\Psi_1$ to get $\Gamma,\Phi_1\types \Xi,\Psi_2,\Delta_2,\Upsilon$, then apply the $f$ rule on $\Psi_2,\Upsilon$ to get the desired $\Gamma,\Phi_1,\Phi_2\types \Xi,\Delta_1,\Delta_2$ as desired.\qedhere
  \end{enumerate}
\end{proof}

\begin{thm}\label{thm:posprop-initial}
  For any relational polygraph \cG, the free posetal prop generated by \cG is described by the type theory for posetal props under \cG: its objects are those of \cG, and we have $\Gamma\le\Delta$ just when $\Gamma\types\Delta$ is derivable.
\end{thm}
\begin{proof}
  \cref{thm:prop-cutadm} implies that these definitions give us a posetal prop $\F\bSMPos\cG$.
  Now if \cM is any other posetal prop and $P:\cG\to\cM$ is a map of relational polygraphs, to extend $P$ to $\F\bSMPos\cG$ it suffices to check that it preserves the relation.
  This follows by an easy induction on the rules.
\end{proof}

Now, there are actually \emph{three} ways we could imagine introducing tensor products in a prop.
Fortunately, they are all equivalent.

\begin{lem}\label{thm:prop-tensor}
  For any two objects $A$ and $B$ in a prop \cP, the following are equivalent.
  \begin{enumerate}
  \item A morphism $(A,B) \to A\tensor B$, precomposition with which defines bijections\label{item:prop-tensor-left}
    \[ \cP(\Gamma,A\tensor B;\Delta) \to \cP(\Gamma,A,B;\Delta) \]
  \item A morphism $A\tensor B \to (A,B)$, postcomposition with which defines bijections\label{item:prop-tensor-right}
    \[ \cP(\Gamma;\Delta,A\tensor B) \to \cP(\Gamma;\Delta,A,B) \]
  \item Morphisms $(A,B) \to A\tensor B$ and $A\tensor B \to (A,B)$ whose composites in both directions
    \begin{gather*}
      A\tensor B \too (A,B)  \too A\tensor B\\
      (A,B)\too A\tensor B \too (A,B)
    \end{gather*}
    are identities.\label{item:prop-tensor-iso}
  \end{enumerate}
\end{lem}
\begin{proof}
  If we have~\ref{item:prop-tensor-iso}, then pre- or post-composing with the other morphism yields the bijections in~\ref{item:prop-tensor-left} and~\ref{item:prop-tensor-right}.
  On the other hand, given~\ref{item:prop-tensor-left}, the identity $(A,B)\to (A,B)$ must be the composite $(A,B)\to A\tensor B \to (A,B)$ for some unique morphism $A\tensor B \to (A,B)$, and the other composite must be the identity since its precomposite with $(A,B)\to A\tensor B$ is $(A,B)\to A\tensor B$; so~\ref{item:prop-tensor-iso} holds.
  (This is basically the Yoneda lemma.)
  The case of~\ref{item:prop-tensor-right} is dual.
\end{proof}

The case with units is similar.

\begin{lem}\label{thm:prop-unit}
  In a prop \cP, the following are equivalent.
  \begin{enumerate}
  \item A morphism $()\to \one$, precomposition with which defines bijections
    \[\cP(\Gamma,\one;\Delta) \to \cP(\Gamma;\Delta).\]
  \item A morphism $\one\to()$, postcomposition with which defines bijections
    \[\cP(\Gamma;\Delta,\one) \to \cP(\Gamma;\Delta).\]
  \item Morphisms $()\to \one$ and $\one\to()$ whose composites in both directions are identities.\qed
  \end{enumerate}
\end{lem}

\begin{thm}\label{thm:prop-smc}
  There is an equivalence between (1) symmetric monoidal categories and (2) props with tensors and units in the sense of \cref{thm:prop-tensor,thm:prop-unit}.
\end{thm}
\begin{proof}
  Given a symmetric monoidal category \cC, we define a prop \cP by
  \[ \cP(A_1,\dots,A_n ; B_1,\dots,B_m) = \cC(A_1\tensor \cdots\tensor A_n, B_1\tensor\cdots\tensor B_m)\]
  or, more succinctly, $\cP(\Gamma;\Delta) = \cC(\bigtensor\Gamma, \bigtensor\Delta)$.
  Of course, $\bigtensor() = \one$.
  The identity rules come from $\idfunc_\one$ and $f\tensor \idfunc_A$, while the composite of $f:\Gamma\to (\Xi,\Psi)$ and $g:(\Psi,\Phi)\to \Delta$ is $(\bigtensor\Xi \tensor g) \circ (f\tensor \bigtensor \Phi)$.
  Of course, this prop has tensors and units quite trivially.

  Conversely, given a prop with tensors and units, we have an underlying category with an object $\one$ and an operation $\tensor$.
  The functoriality of $\tensor$ is defined on $f:A\to A'$ and $g:B\to B'$ as the composite
  \[ A\tensor B \too (A,B) \xto{(f,\idfunc)} (A',B) \xto{(\idfunc,g)} (A',B') \too A'\tensor B'. \]
  The axioms of a prop ensure that this is the same as if we put $f$ and $g$ in the other order (or we could write simply $(f,g)$ using the yet-to-be-defined polycomposition).
  Functoriality is similarly easy.
  For associativity, we have
  \[ ((A\tensor B)\tensor C) \too (A\tensor B,C) \too (A,B,C) \too (A,B\tensor C) \to (A\tensor (B\tensor C)) \]
  and for unitality we have
  \[ (A\tensor \unit) \too (A,\unit) \too (A) \]
  and dually.
  It is straightforward to check that these are natural isomorphisms and satisfy the necessary axioms.
  For symmetry, we use the fact that our prop is a \emph{symmetric} polygraph and act by symmetry on the morphisms $(A,B)\to A\tensor B$ and $A\tensor B\to (A,B)$, obtaining ``inverse isomorphisms'' $(B,A) \to A\tensor B$ and $A\tensor B \to (B,A)$.
  From these we can show $A\tensor B \cong B\tensor A$ and check naturality and the axioms.

  It is straightforward to check that these two constructions are inverses.
\end{proof}

\cref{thm:prop-tensor,thm:prop-unit} give several ways to introduce $\tensor$ and $\unit$ into the type theory of posetal props.
For a sequent calculus, the symmetrical approach is probably the most natural:
\begin{mathpar}
  \inferrule{\Gamma,A,B\types \Delta}{\Gamma,A\tensor B\types \Delta}\;\tensorL\and
  \inferrule{\Gamma\types \Delta,A,B}{\Gamma\types \Delta,A\tensor B}\;\tensorR\and
  \inferrule{\Gamma\types\Delta}{\Gamma,\one\types\Delta}\;\one L\and
  \inferrule{\Gamma\types\Delta}{\Gamma\types\Delta,\one}\;\one R\and
\end{mathpar}
Of course, we also include the identity and generator rules (the former only for base types coming from \cG) and the type judgment:
\begin{mathpar}
  \inferrule{ }{()\types()}\and
  \inferrule{\Gamma\types\Delta\\ A\in\cG}{\Gamma,A\types \Delta,A}\and
  \inferrule{\Gamma\types\Xi,\Psi \\ f\in\cG(\Psi,\Phi; \Delta)}{\Gamma,\Phi \types \Delta,\Xi}\and
  \inferrule{A\in\cG}{\types A\type}\and
  \inferrule{ }{\types \one\type}\and
  \inferrule{\types A\type \\ \types B\type}{\types A\tensor B\type}
\end{mathpar}
We call this the \textbf{classical sequent calculus for symmetric monoidal posets under \cG}.
(``Classical'' because we have multiple formulas on both sides, ``posets'' because we are not yet distinguishing derivations or introducing terms.)

\begin{thm}\label{thm:seqcalc-smpos-invertible}
  All the rules for $\tensor$ and $\one$ in the classical sequent calculus for symmetric monoidal posets under \cG are invertible.
\end{thm}
\begin{proof}
  Suppose we have a derivation of $\Gamma,A\tensor B\types \Delta$; we want a derivation of $\Gamma,A,B\types \Delta$.
  We induct on the given derivation.
  \begin{enumerate}
  \item The easy case is when it ends with $\tensorL$ whose premise is what we want.
  \item If it ends with $\tensorL$ acting on another type, or any of $\tensorR$, $\one L$, or $\one R$, then we induct on the premise and then apply that rule to the result.
  \item If it ends with the identity rule for $C$, then since $C\in \cG$ it can't be $A\tensor B$, so we can again induct on the premise and apply the identity rule afterwards.
  \item Finally, if it ends with a generator rule for $f$, then since the domain of $f$ is a list of objects of $\cG$, none of them can be $A\tensor B$; so $A\tensor B$ must be in the sequent that the generator rule cuts with, and we can induct on that and then apply the generator rule.
  \end{enumerate}
  The other rules $\tensorR$, $\one L$, and $\one R$ are similar.
\end{proof}

\begin{thm}\label{thm:seqcalc-smpos-identity}
  The following general identity rule is admissible in the classical sequent calculus for symmetric monoidal posets under \cG:
  \begin{mathpar}
    \inferrule{\Gamma\types\Delta\\ \types A\type}{\Gamma,A\types \Delta,A}\and
  \end{mathpar}
  In particular, since $()\types()$, we have $A\types A$ whenever $\types A\type$.
\end{thm}
\begin{proof}
  We induct on the derivation of $\types A\type$.
  \begin{enumerate}
  \item If it is from \cG, we apply the postulated identity rule.
  \item If it is $\one$, we have the following derivation:
    \begin{mathpar}
      \inferrule*{\inferrule*{\inferrule*{ }{()\types()}}{() \types \one}}{\one\types\one}
    \end{mathpar}
  \item If it is $A_1\tensor A_2$, we have by induction a derivation $\sD_1$ of $\Gamma,A_1\types,\Delta,A_1$, and then (again by induction) a derivation $\sD_2$ of $\Gamma,A_1,A_2\types \Delta,A_1,A_2$.
    Now we augment this in a similar way:
    \begin{equation*}
      \inferrule*{
        \inferrule*{
          \inferrule*{\sD_2\\\\\vdots}{\Gamma,A_1,A_2\types \Delta,A_1,A_2}
        }{
          \Gamma,A_1\tensor A_2\types \Delta,A_1,A_2
        }}{
        \Gamma,A_1\tensor A_2\types \Delta,A_1\tensor A_2
      }\qedhere
    \end{equation*}
  \end{enumerate}
\end{proof}

\begin{thm}\label{thm:seqcalc-smpos-cutadm}
  The prop cut rule is admissible in the classical sequent calculus for symmetric monoidal posets under \cG.
\end{thm}
\begin{proof}
  Suppose given derivations of $\Gamma\types\Xi,\Psi$ and $\Psi,\Phi \types \Delta$; we want to derive $\Gamma,\Phi \types \Delta,\Xi$.
  We induct on the derivation of $\Psi,\Phi \types \Delta$.
  \begin{enumerate}
  \item If it ends with an identity or a generator, we do as in \cref{thm:prop-cutadm}.
  \item If it ends with $\tensorR$ or $\one R$, we can simply apply the inductive hypothesis to cut with the premise of that rule.
  \item If it ends with $\tensorL$, there are two cases.
    If the introduced $A\tensor B$ is in $\Phi$, then we can inductively cut the premise along $\Psi$ again.
  \item On the other hand, if the introduced $A\tensor B$ is in $\Psi$, then our other derivation has the form $\Gamma\types\Xi,\Psi',A\tensor B$.
    Since $\tensorR$ is invertible by \cref{thm:seqcalc-smpos-invertible}, we can also derive $\Gamma\types\Xi,\Psi',A,B$, and then cut with the premise $\Psi',A,B,\Phi \types \Delta$ of our $\tensorL$.
  \item The case of $\one L$ is similar.\qedhere
  \end{enumerate}
\end{proof}

\begin{thm}\label{thm:seqcalc-smpos-initial}
  For any relational polygraph \cG, the free symmetric monoidal poset generated by \cG is described by the classical sequent calculus for symmetric monoidal posets under \cG.
\end{thm}
\begin{proof}
  \cref{thm:seqcalc-smpos-identity,thm:seqcalc-smpos-cutadm} imply in particular that the sequent calculus gives us a posetal prop $\F\bSMPos\cG$.
  Moreover, the rules for $\tensor$ and $\one$ give $\F\bSMPos\cG$ the tensor and unit structure of \cref{thm:prop-tensor,thm:prop-unit}, so that it is a symmetric monoidal poset.
  Now if \cM is any other symmetric monoidal poset and $P:\cG\to\cM$ a map of symmetric relational polygraphs, there is a unique extension of $P$ to the objects of $\F\bSMPos\cG$.
  As usual, by induction on the rules of the sequent calculus and the fact that \cM is a posetal prop, this extension preserves the relations $\Gamma\le\Delta$.
  Finally, it also preserves the tensor and unit structure, so it is a strict functor of symmetric monoidal posets.
\end{proof}


\section{Props and symmetric monoidal categories}
\label{sec:prop-smc}

So much for symmetric monoidal posets.
What we really want, however, is a type theory for symmetric monoidal \emph{categories}, in which we can talk about equality of morphisms, and that can also deal with tensors in the codomain.

In our type theories for categories and multicategories in \cref{sec:category-cutadm,sec:multicat-moncat} (before introducing any operations such as products or tensors), we did not have to impose any equivalence relation on the derivations.
However, in the case of props this is no longer true, because of the interchange rule.
A simple case is when we have $f\in \cG(A;B)$ and $g\in\cG(C;D)$; then there are two distinct derivations of a sequent representing $f\circ_{()} g$:
\begin{mathpar}
  \inferrule*[Right=$g$]{
    \inferrule*{
      \inferrule*[Right=$f$]{
        \inferrule*{\inferrule*{ }{()\types()}}{A\types A}
      }{
        A\types B
      }}{
      A,C\types B,C
    }}{
    A,C\types B,D
  }\and
  \inferrule*[Right=$f$]{
    \inferrule*{
      \inferrule*[Right=$g$]{
        \inferrule*{\inferrule*{ }{()\types()}}{C\types C}
      }{
        C\types D
      }}{
      A,C\types A,D
    }}{
    A,C\types B,D
  }\and
\end{mathpar}
If we write down a term calculus whose terms correspond exactly to derivations, as we usually do, then the desired equality between these two derivations would look something like
\[ x:A, y:C \types (f,\idfunc)((\idfunc,g)(x,y)) \equiv (\idfunc,g)((f,\idfunc)(x,y)) : (B,D) \]
Note that unlike the $\beta$- and $\eta$-conversions, this equality is not directional: it makes no sense to regard one or the other side as ``simpler'' or ``more canonical'' than the other.
Thus, a canonical/atomic or focused theory will not help.

To deal with this in our type theory of props, we will break the bijection between terms and deductions, in a way that enables us to represent the above two derivations by \emph{literally the same} term (or tuple of terms), namely
\[ x:A, y:C \types (f(x),g(y)):(B,D) \]
This makes the metatheory and the initiality theorem rather more complicated, but at the end of the day it leads to a much more congenial type theory.
A nice example is the study of antipodes in noncommutative bimonoids that we discussed informally in \cref{sec:intro}.

The intuition in this notation is of course that $f(x):B$ and $g(y):D$.
We could write it as ``$f(x):B,g(y):D$'', but we choose to tuple the terms up as in $(f(x),g(y)):(B,D)$ for a couple of reasons.
The first reason is that when doing equational reasoning (such as for the antipode calculation), the equalities must relate entire tuples rather than single terms.
The second reason is that in general, we also need to include some ``terms without a type'' (e.g.\ coming from morphisms with empty codomain $()$, which is a judgmental representation of the unit object), and this looks a little nicer when all the terms are grouped together: we write for instance $(f(x),g(y)\mid h(z))$ to mean that $h(z):()$.

There are also, of course, function symbols with \emph{multiple} outputs.
To deal with this case we write $f_1(x)$, $f_2(x)$, and so on for the terms corresponding to all the types in the codomain.
For example, we write the composite of $f:(A,B) \to (C,D)$ with $g:(E,D)\to (F,G)$ as
\[ x:A, y:B, z:E \types (f_1(x,y),g_1(z,f_2(x,y)),g_2(z,f_2(x,y))):(C,F,G) \]
This does require one further technical device (that will be almost invisible in practice).
Suppose we have $f:()\to (B,C)$, written in type theory as $()\types (f_1,f_2):(B,C)$, and we compose/tensor it with itself to get a morphism $() \to (B,B,C,C)$.
We would na\"ively write this as $() \types (f_1,f_1,f_2,f_2)$, but this is ambiguous since we can't tell which $f_1$ matches which $f_2$.
We disambiguate the possibilities by writing $() \types (f_1,f'_1,f_2,f'_2)$ or $() \types (f_1,f'_1,f'_2,f_2)$.
Although this issue seems to only arise for morphisms with empty domain and greater than unary codomain, for consistency we formulate the syntax with a label (like $'$) on \emph{every} term former, and simply omit them informally (as in the vast majority of cases) when there is no risk of ambiguity.
We assume given an infinite alphabet of symbols \fA for this purpose (such as $','',''',\dots$, or $1,2,3,\dots$).

Now, if our terms are not simply representations of derivations, then we need to explain what terms \emph{are} before we explain what they mean.
For this purpose we define the class of \emph{pre-terms}, which have a type and a context but may not respect the linearity of the inputs.
The pre-terms, being syntax, are freely generated in an appropriate sense, so we can describe them using an auxiliary judgment $\Gamma \types^\fL M\pc A$.
The intent is that we will eventually judge $\Gamma \types (M_1,\dots,M_n):(A_1,\dots,A_n)$ where each $M_i$ is a pre-term such that $\Gamma \types M_i\pc A_i$.
We will also need to include pre-terms with no type at all, so we also include a judgment $\Gamma \types^\fL M\pc ()$ for this purpose.
In both cases the annotation \fL is a finite subset of \fA indicating which labels might have been used in the terms $M_i,N_i$, to avoid duplication.

The rules for these judgments are shown in \cref{fig:pre-terms}.
It may be tempting to require in the generator rule that all label sets $\fL_i$ in the premises should be disjoint, but this is \emph{not} correct.
For instance, if $f:A\to (B,C)$ and $g:(B,C) \to D$, the composite $g \circ_{(B,C)} f$ will be represented by the pre-term
\begin{mathpar}
  \inferrule*{
    x:A \types^{\fa} f^\fa_1(x)\pc B \\
    x:A \types^{\fa} f^\fa_2(x)\pc C \\
    g\in \cG(B,C;D)\\
    \fb\notin\{\fa\}
  }{
    x:A \types^{\{\fa,\fb\}} g^\fb(f^\fa_1(x),f^\fa_2(x)):D
  }
\end{mathpar}
in which the label \fa appears in multiple premises.
All that matters is that the \emph{new} label being introduced to mark $g$ has not been used before.

We allow an arbitrary finite set of labels in the ``variable'' rule to ensure that the pre-term judgment is closed under adding finitely many more unused labels.
(This can be proved by an easy induction.)
This makes some things more convenient to state, and doesn't matter because \fA is infinite so there will always be fresh labels available.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\fL\finsubset\fA \\ \fa\notin\fL}{\Gamma,x:A \types^{\fL\cup\{\fa\}} x^\fa\pc A}\and
    \inferrule{
      \Gamma \types^{\fL_1} M_1\pc A_1 \\ \Gamma\types^{\fL_2} M_2\pc A_2 \\ \cdots \\ \Gamma\types^{\fL_n} M_n\pc A_n \\\\
      f\in \cG(A_1,\dots,A_n; B_1,\dots,B_m)\\ 1\le j\le m \\ \forall i(\fa \notin \fL_i)
    }{
      \Gamma \types^{\fL_1\cup\cdots\cup\fL_n \cup \{\fa\}} f_j^\fa(M_1,\dots,M_n)\pc B_j
    }\and
    \inferrule{
      \Gamma \types^{\fL_1} M_1\pc A_1 \\ \Gamma\types^{\fL_2} M_2\pc A_2 \\ \cdots \\ \Gamma\types^{\fL_n} M_n\pc A_n \\\\
      f\in \cG(A_1,\dots,A_n; \cdot ) \\ \forall i(\fa \notin \fL_i)
    }{
      \Gamma \types^{\fL_1\cup\cdots\cup\fL_n \cup \{\fa\}} f^\fa(M_1,\dots,M_n)\pc ()
    }\and
  \end{mathpar}
  \caption{Rules for pre-terms}
  \label{fig:pre-terms}
\end{figure}

Next we give the rules for the term judgment.
To incorporate morphisms with codomain $()$, we write the judgment as
\[\Gamma \types^\fL (M_1,\dots,M_m\mid N_1,\dots,N_n):(A_1,\dots,A_m)\]
with the intended invariants that $\Gamma\types^{\fL} M_i\pc A_i$ for each $i$ and $\Gamma\types^{\fL} N_j\pc ()$ for each $j$.
Like the pre-term judgment, the term judgment is closed under adding finitely many more unused labels.
If $n=0$ we write simply $(M_1,\dots,M_m)$, omitting the $|$, and if $m=1$ as well we may omit the parentheses.
When pre-terms $M_i$ and $N_j$ appear in a valid term judgment, we refer to them as \textbf{terms}, and we refer to the $N_j$'s in particular as \textbf{null terms}.

We write $\vec M$ for a list of pre-terms, $\vec{x}$ for a list of variables, and $\vec M,N$ for concatenation thereof.
We use capital Greek letters $\Gamma,\Delta,\dots$ both for \emph{contexts} (lists of types with assigned variables such as $x:A,y:B$) and for simple lists of types; the latter can be made into a context by supposing variables $\vec{x}:\Gamma$, or appear as the consequent with assigned terms $\vec{M}:\Gamma$.
Finally, if $f\in\cG(A_1,\dots,A_n;B_1,\dots,B_m)$ we write $\vec{f}^\fa(\vec{M})$ for the list of pre-terms
\[(f_1^\fa(M_1,\dots,M_n),f_2^\fa(M_1,\dots,M_n),\dots,f_m^\fa(M_1,\dots,M_n))\]
where each $M_i \pc A_i$.
Note that the variables in a context are not literally treated ``linearly'', since they can occur multiple times in the multiple ``components'' of a map $f$.
For instance, a morphism $f:(A,B)\to (C,D)$ is represented by $x:A, y:B \types (f_1(x,y),f_2(x,y)):(C,D)$.
Instead the ``usages'' of a variable are controlled by the codomain arity of the morphisms applied to them.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{
      x_1:A_1,\dots,x_n:A_n\types^\fL (M_1,\dots,M_m\mid Z_1,\dots,Z_p):(B_1,\dots,B_m)\\
      \sigma\in \Sigma_n \\ \tau\in\Sigma_m \\ \rho\in\Sigma_p
    }{
      x_{\sigma 1}:A_{\sigma 1},\dots,x_{\sigma n}:A_{\sigma n}\types^\fL (M_{\tau 1},\dots,M_{\tau m}\mid Z_{\rho 1},\dots,Z_{\rho p}):(B_{\tau 1},\dots,B_{\tau m})
    }\and
    \inferrule{\fL\finsubset\fA}{() \types^{\fL} ():()}\and
    \inferrule{\Gamma\types^\fL (\vec{M}\mid \vec{Z}):\Delta\\
      \types A\type\\
      \fa\notin\fL
    }{
      \Gamma,x:A\types^{\fL\cup\{\fa\}} (\vec{M},x^\fa\mid \vec{Z}):(\Delta,A)
    }\and
    \inferrule{
      \Gamma\types^\fL (\vec M,\vec N\mid\vec{Z}):(\Xi,\Psi) \\
      f\in\cG(\Psi; \Delta)\\
      |\Delta|>0\\
      \fa\notin\fL
    }{
      \Gamma \types^{\fL\cup \{\fa\}} (\vec M,\vec f^\fa(\vec N)\mid\vec{Z}):(\Xi,\Delta)
    }\and
    \inferrule{
      \Gamma\types^\fL (\vec M,\vec N\mid\vec{Z}):(\Xi,\Psi) \\
      f\in\cG(\Psi; ())\\
      \fa\notin\fL
    }{
      \Gamma \types^{\fL\cup\{\fa\}} (\vec M\mid\vec{Z},f^\fa(\vec N)):\Xi
    }\and
  \end{mathpar}
  \caption{Rules for terms}
  \label{fig:cl-smc-terms}
\end{figure}

With these notations, the rules for the term judgment are shown in \cref{fig:cl-smc-terms}.
For clarity, we have made the exchange rule explicit at the beginning.
Note that these rules are exactly the rules of the type theory for posetal props \cref{sec:proppos-smpos}, with the additional term annotations.
Moreover, when we annotate the two problematic derivations discussed above, we obtain the same terms.
\begin{mathpar}
  \inferrule*[Right=$g$]{
    \inferrule*{
      \inferrule*[Right=$f$]{
        \inferrule*{\inferrule*{ }{()\types():()}}{x:A\types x:A}
      }{
        x:A\types f(x):B
      }}{
      x:A,y:C\types (f(x),y):(B,C)
    }}{
    x:A,y:C\types (f(x),g(y)):(B,D)
  }\and
  \inferrule*[Right=$f$]{
    \inferrule*{
      \inferrule*[Right=$g$]{
        \inferrule*{\inferrule*{ }{()\types():()}}{y:C\types y:C}
      }{
        y:C\types g(y):D
      }}{
      x:A,y:C\types (x,g(y)):(A,D)
    }}{
    x:A,y:C\types (f(x),g(y)):(B,D)
  }\and
\end{mathpar}

We now commence the metatheoretic analysis.

\begin{lem}
  If $\Gamma\types^\fL (M_1,\dots,M_m\mid N_1,\dots,N_n):(B_1,\dots,B_m)$ is derivable, then so are $\Gamma\types^{\fL} M_i\pc B_i$ and $\Gamma\types^{\fL} N_j\pc ()$ for each $i,j$.
\end{lem}
\begin{proof}
  A straightforward induction over derivations.
\end{proof}

\begin{lem}\label{thm:preterm-subadm}
  The following rule of substitution into pre-terms is admissible:
  \begin{mathpar}
    \inferrule{
      \Gamma \types^{\fM} N_1\pc A_1 \\ \cdots \\ \Gamma\types^{\fM} N_n\pc A_n\\\\
      \Phi,y_1:A_1,\dots,y_n,A_n \types^\fL M\pc B\\
      \fL\cap\fM = \emptyset
    }{
      \Gamma,\Phi \types^{\fL\cup\fM} M[N_1/y_1,\dots,N_n/y_n]\pc B
    }
  \end{mathpar}
\end{lem}
\begin{proof}
  We induct over the derivation of $\Phi,y:A \types^\fL M\pc B$.
  In almost all cases we simply apply the inductive hypothesis to the premise, thereby defining the meaning of substitution on pre-terms by recursing through the structure as usual.
  We require $\fL\cap\fM=\emptyset$ so that the rules that introduce new labels can still be re-applied to the substituted premise, since they require that their new label be fresh.

  As usual, the one rule that behaves differently is $\Gamma,x:A \types^{\fL} x^\fa\pc A$ when $x=y$.
  In this case the result of substitution is just $N$.
\end{proof}

We may write $M[\vec N/\vec y]$ as a shorthand for $M[N_1/y_1,\dots,N_n/y_n]$.
The requirement of label disjointness in \cref{thm:preterm-subadm} means that we cannot decompose these ``simultaneous substitutions'' into iterated single substitutions.
For instance, if $N_1$ and $N_2$ share labels, then \cref{thm:preterm-subadm} would not justify $M[N_1/y_1][N_2/y_2]$.

\begin{lem}\label{thm:prop-subadm}
  Substitution for terms (i.e.\ the cut rule for derivations) is admissible:
  \[\inferrule{
    \Gamma\types^\fL(\vec M,\vec N\mid\vec Z):(\Xi,\Psi) \\
    \vec y:\Psi,\Phi \types^\fM (\vec P\mid\vec W):\Delta\\
    \fM\cap\fL=\emptyset
  }{
    \Gamma,\Phi \types^{\fM\cup\fL} (\vec M, \vec P[\vec N/\vec y] \mid \vec Z, \vec W[\vec N/\vec y]):(\Xi,\Delta)
  }\]
\end{lem}
\begin{proof}
  As is usual for natural deductions, a very straightforward induction.
  Note that the requirement $\fM\cap\fL=\emptyset$ is a generalization of the requirement $\fa\notin\fL$ appearing in the primitive rules.
\end{proof}

We define the \textbf{height} of a derivation to be the number of rules appearing in it \emph{other} than the exchange rule.
The following ``invertibility'' lemma is key to the initiality theorem.

\begin{lem}\label{thm:prop-invertible}
  If we have a derivation of a given sequent \sQ, and an instance \cR of a rule whose conclusion is \sQ, then we can construct a derivation of \sQ that ends with \cR and has the same height as the given one.
\end{lem}
\begin{proof}
  This is trivially true for the exchange rule (since it supplies its own inverses and contributes no height) and for the trivial rule $()\types ():()$.
  For all the other rules \cR, suppose the given derivation ends with a different rule application $\cS$ (disregarding any exchanges in between).
  Now note that each rule introduces a new label and introduces exactly those terms in the judgment whose outer connective has that label.
  Therefore, the rule applications $\cR$ and $\cS$, being different, must introduce different labels, and therefore introduce \emph{disjoint} sets of terms.
  (This is where the argument would fail without labels.)

  It follows that the premise of \cS, say $\sQ'$, is also a conclusion of some instance $\cR'$ of the same rule as \cR.
  Therefore, we can apply the inductive hypothesis to obtain a derivation of this premise ending with $\cR'$, whose premise is $\sQ''$ say.
  Now by the same disjointness argument, we can apply an instance $\cS'$ of the same rule as \cS to $\sQ''$, and then $\cR$ to the conclusion of that rule, obtaining our desired derivation.
  The height is clearly preserved.
\end{proof}

That proof was written very abstractly, so consider the following example with $f\in\cG(A;D,E)$ and $g\in\cG(B;C,F)$.
We omit to write exchange rules, as well as the obvious variable rules at the top.
\begin{mathpar}
  \inferrule*[Right=$g$]{
    \inferrule*[Right=$f$]{
      x:A,y:B \types (y,x):(B,A)
    }{
      x:A,y:B \types (y,f_2(x),f_1(x)):(B,E,D)
    }
  }{
    x:A,y:B \types (g_1(y),f_2(x),g_2(y),f_1(x)):(C,E,F,D)
  }
\end{mathpar}
The conclusion of this rule is (modulo exchange) the conclusion of an instance of the $f$-generator rule.
Now the $g$-terms introduced by the actual rule that leads to this conclusion are disjoint from the $f$-terms that would be introduced by this $f$-generator rule.
Thus, by induction the premise $x:A,y:B \types (y,f_2(x),f_1(x)):(B,E,D)$ has a derivation of the same height ending with the $f$-generator rule; in this simple case that is in fact the derivation of it that we were given.
Now we can apply the $g$-generator rule to the premise of \emph{that} rule, namely $x:A,y:B C \types (y,x):(B C,A)$, and then follow it by the $f$-generator rule, obtaining the following derviation:
\begin{mathpar}
  \inferrule*[Right=$f$]{
    \inferrule*[Right=$g$]{
      x:A,y:B \types (y,x):(B C,A)
    }{
      x:A,y:B C \types (g_1(y),g_2(y),x):(C,F,A)
    }
  }{
    x:A,y:B \types (g_1(y),f_2(x),g_2(y),f_1(x)):(C,E,F,D)
  }
\end{mathpar}

\begin{thm}\label{thm:prop-initial}
  The free prop on a polygraph can be presented using this type theory: its morphisms $\Gamma\to\Delta$ are the \emph{pairs $(\vec M,\vec Z)$ of lists of terms} such that $\Gamma\types^\fL (M_1,\dots,M_n \mid Z_1,\dots,Z_p):\Delta$ is derivable for some $\fL\finsubset\fA$, modulo the equivalence relation generated by (a) permutation of labels and (b) permutation of the null terms $\vec Z$.
  In particular, if two derivations give the same lists of pre-terms, we stipulate that only one morphism results.
\end{thm}
\begin{proof}
  \cref{thm:prop-subadm} tells us how to compose morphisms.
  Associativity, unitality, and equivariance follow as usual, since substitution into pre-terms is basically ordinary substitution.
  In particular, these axioms hold as syntactic equalities of pre-terms.

  For the interchange rule, suppose given derivations of $\Gamma\types (\vec M\mid\vec Z):\Delta$ and $\Phi\types (\vec N\mid\vec W):\Xi$.
  We can then substitute them along $()$ in either order to get
  \begin{align*}
    \Gamma,\Phi &\types (\vec M,\vec N\mid \vec Z,\vec W):\Delta,\Xi\\
    \Phi,\Gamma &\types (\vec N,\vec M\mid \vec W,\vec Z):\Xi,\Delta\\
\intertext{Applying exchange to the second, we obtain}
    \Gamma,\Phi &\types (\vec M,\vec N\mid \vec W,\vec Z):\Delta,\Xi
  \end{align*}
  and we have $(\vec M,\vec N\mid \vec Z,\vec W) \equiv (\vec M,\vec N\mid \vec W,\vec Z)$ by the permutation condition (b).
  In particular, unlike the other axioms, interchange doesn't hold as an on-the-nose equality of syntax.

  In any case, we now have a prop $\F\bProp\cG$.
  We define the inclusion of polygraphs $\cG\to \F\bProp\cG$ by sending $f\in\cG(\Gamma,\Delta)$ to $\vec x:\Gamma \types (\vec f^\fa(\vec x)):\Delta$ (or to $\vec x:\Gamma \types (\,\mid\vec f^\fa(\vec x)):()$ if $\Delta=()$), for some $\fa\in\fA$.
  This is independent of the choice of \fa due to the permutation condition (a).

  Now suppose \cM is any prop and $P:\cG\to\cM$ is a morphism of polygraphs.
  We extend $P$ to the prop $\F\bProp\cG$ by induction on derivations.
  This is straightforward using the prop structure of \cM; the nontrivial thing is showing that the result depends only on the pre-term judgment rather than on the particular derivation.
  This is the purpose of \cref{thm:prop-invertible}, which we apply as follows.

  Suppose we have two derivations $\sD_1$ and $\sD_2$ of the same sequent $\sQ$, ending with rules $\cR_1$ and $\cR_2$ with premises $\sQ_1$ and $\sQ_2$ respectively.
  By disjointness of pre-terms as in the proof of \cref{thm:prop-invertible}, we can say that $\sQ_1$ is a conclusion of an instance of $\cR_2$, and similarly $\sQ_2$ is a conclusion of an instance of $\cR_1$.
  By \cref{thm:prop-invertible}, therefore, $\sQ_1$ has a derivation $\sD_1'$ of the same height ending with $\sR_2$, while $\sQ_2$ has a derivation $\sD_2'$ of the same height ending with $\sR_1$.
  Moreover, up to exchange (as always) the premise of these two ending rules must be the same sequent $\sQ_3$.
  By induction (which is applicable since the heights have been preserved), the given derivations of $\sQ_1$ and $\sQ_2$ yield the same morphism in \cM as these other derivations $\sD_1'$ and $\sD_2'$.

  Thus, it will suffice to show that once the sequent $\sQ_3$ is interpreted according to some particular derivation of it, we obtain the same interpretation of $\sQ$ by deriving it in the following two ways:
  \begin{mathpar}
    \inferrule*[Right=$\cR_1$]{\inferrule*[Right=$\cR_2$]{\vdots\\\\\sQ_3}{\sQ_1}}{\sQ}\and
    \inferrule*[Right=$\cR_2$]{\inferrule*[Right=$\cR_1$]{\vdots\\\\\sQ_3}{\sQ_2}}{\sQ}\and
  \end{mathpar}
  This follows from the associativity and interchange rules in \cM.

  Thus we have a map of polygraphs $\F\bProp\cG\to\cM$, which extends $P$ by construction.
  To see that it is the unique such extension that could be a prop functor, note that every primitive rule is essentially an instance of identity or cut.
  In the former case, its image in $\cM$ must also be an identity.
  In the latter case, we may inductively suppose the image in \cM of its first input is already uniquely determined, while its second input is either an identity (hence determined) or a generating morphism in \cG, and the images of the latter are determined by $P$.
  (Here we use again the imposed invariance (a) under permutation of labels, since otherwise $P$ would only determine the images of the generators with the particular labels we chose to define the inclusion $\cG\to\F\bProp\cG$.)
  Thus, their composite is also uniquely determined.

  Finally, we must show that $\F\bProp\cG\to\cM$ actually \emph{is} a prop functor, i.e.\ it preserves identities and composition.
  Identities are immediate, while composition follows as usual by inductively considering the definition of composition (i.e.\ substitution) in $\F\bProp\cG$.
\end{proof}

We can extend this type theory for props to a type theory for symmetric monoidal categories by introducing tensors and units, as we did in the posetal case.
For this purpose we reformulate the sequent calculus rules for these as a natural deduction, introducing the new rules for pre-terms and terms shown in \cref{fig:prop-smc-preterms,fig:prop-smc-terms}.

% \begin{figure}
%   \centering
%   \begin{mathpar}
%     \inferrule{A\in\cG}{\types A\type}\and
%     \inferrule{ }{\types \one\type}\and
%     \inferrule{\types A\type \\ \types B\type}{\types A\tensor B\type}
%     \inferrule{ }{()\types()}\and
%     \inferrule{\Gamma\types\Delta\\ \types A\type}{\Gamma,A\types \Delta,A}\and
%     \inferrule{\Gamma\types\Xi,\Psi \\ f\in\cG(\Psi; \Delta)}{\Gamma \types \Delta,\Xi}\and
%     \inferrule{\Gamma\types \Delta,A,B}{\Gamma\types \Delta,A\tensor B}\;\tensorI\and
%     % \inferrule{\Xi\types\Phi,\Psi,A\tensor B \\ \Gamma,\Psi,A,B\types \Delta}{\Gamma,\Xi\types \Phi,\Delta}\;\tensorE\and
%     \inferrule{\Gamma\types \Delta,A\tensor B}{\Gamma\types \Delta,A,B}\;\tensorE\and
%     \inferrule{\Gamma\types\Delta}{\Gamma\types\Delta,\one}\;\one I\and
%     % \inferrule{\Xi\types \Phi,\Psi,\one \\ \Gamma,\Psi\types\Delta}{\Gamma,\Xi\types\Phi,\Delta}\;\one E\and
%     \inferrule{\Gamma\types\Delta,\one}{\Gamma\types\Delta}\;\one E\and
%   \end{mathpar}
%   \caption{Classical natural deduction for symmetric monoidal categories}
%   \label{fig:clnatded-smc}
% \end{figure}

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{
      \Gamma\types^{\fL_1} M\pc A \\ \Gamma\types^{\fL_2} N\pc B \\
      \fa\notin \fL_1\cup\fL_2
    }{
      \Gamma\types^{\fL_1\cup\fL_2\cup\{\fa\}} \pair{M}{N}^\fa \pc A\tensor B
    }\and
    \inferrule{\fL \finsubset \fA \\ \fa\notin\fL}{\Gamma\types^{\fL\cup\{\fa\}} \ttt^\fa\pc\unit}\and
    \inferrule{\Gamma\types^\fL M\pc A\tensor B \\ \fa\notin\fL}{\Gamma\types^{\fL\cup\{\fa\}} \pi_1^\fa(M) \pc A}\and
    \inferrule{\Gamma\types^\fL M\pc A\tensor B \\ \fa\notin\fL}{\Gamma\types^{\fL\cup\{\fa\}} \pi_2^\fa(M) \pc B}\and
    \inferrule{\Gamma\types^\fL M\pc \unit \\ \fa\notin\fL}{\Gamma\types^{\fL\cup\{\fa\}} \cancel{M}^\fa:()}\and
  \end{mathpar}
  \caption{Pre-terms for symmetric monoidal categories}
  \label{fig:prop-smc-preterms}
\end{figure}

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{
      \Gamma\types^\fL (\vec{M},N,P\mid\vec{Z}):(\Delta,A,B)\\
      \fa\notin\fL
    }{
      \Gamma\types^{\fL\cup\{\fa\}} (\vec{M},\pair{N}{P}^\fa \mid\vec{Z}):(\Delta,A\tensor B)
    }\;\tensorI\and
    \inferrule{
      \Gamma\types^\fL (\vec{M},P\mid\vec{Z}):(\Delta,A\tensor B)\\
      \fa\notin\fL
    }{
      \Gamma\types^{\fL\cup\{\fa\}} (\vec{M},\pi_1^\fa(P),\pi_2^\fa(P)\mid\vec{Z}):(\Delta,A,B)
    }\;\tensorE\\
    \inferrule{
      \Gamma\types^\fL(\vec M\mid\vec Z):\Delta\\
      \fa\notin\fL
    }{
      \Gamma\types^{\fL\cup\{\fa\}} (\vec M,\ttt^\fa\mid\vec Z):(\Delta,\one)
    }\;\one I\and
    \inferrule{
      \Gamma\types^\fL(\vec M,N\mid\vec Z):(\Delta,\one)\\
      \fa\notin\fL
    }{
      \Gamma\types^{\fL\cup\{\fa\}}(\vec M\mid \cancel{N}^\fa,\vec Z):\Delta
    }\;\one E\and
  \end{mathpar}
  \caption{Terms for symmetric monoidal categories}
  \label{fig:prop-smc-terms}
\end{figure}

Now we need to deal with the $\beta$ and $\eta$ equalities, by introducing an equality judgment.
This takes the form
\[\Gamma \types^{\fL;\fM} (\vec M\mid\vec Z)\equiv (\vec N\mid\vec W) :(\vec B)\]
which should respect the invariants
\begin{alignat*}{2}
  \Gamma&\types^{\fL} M_i\pc B_i &\qquad
  \Gamma&\types^{\fL} Z_j\pc ()\\
  \Gamma&\types^{\fM} N_i\pc B_i &\qquad
  \Gamma&\types^{\fM} W_j\pc ()
\end{alignat*}
Note that these may not be well-typed term judgments.
% TODO: Should they be?

We assert that this relation $\equiv$ is a congruence on lists of pre-terms with respect to concatenation, substitution, and permutation, and that it is invariant under permutations of the label set \fA \emph{separately on each side}.
That is, there is no requirement that the actual labels used on the two sides of $\equiv$ match up; to the eyes of $\equiv$, the label annotations simply define a partition of the ``label occurrences'' in a list of pre-terms.
(We do, of course, have to keep track of individual labels at the level of pre-terms; it's only after they are grouped into lists that we can ignore the identity of the labels.)
% TODO: Do we need to worry more about what happens to the labels here?
We assert moreover that $\equiv$ allows us to permute the second parts of pre-term lists freely:
\[ (\vec M \mid \vec{Z}) \equiv (\vec M \mid \vec{\sigma Z})\]
Thus, it incorporates the permutation quotients (a) and (b) from \cref{thm:prop-initial}.

Relative to the above closure conditions, $\equiv$ is generated by the following relations on individual pre-terms (occurring anywhere in a list):
\begin{mathpar}
  \pi_1^\fa(\pair M N^\fb) \equiv M\and
  \pi_2^\fa(\pair M N^\fb) \equiv N\and
  \pair{\pi_1^\fa(P)}{\pi_2^\fa(P)}^\fb \equiv P\and
  \cancel{\ttt^\fa}^\fb \equiv ()\and
\end{mathpar}
and also the following relation on lists:
\begin{mathpar}
  (\vec M,\ttt^\fa \mid \vec Z,\cancel{W}^\fb) \equiv (\vec M,W\mid \vec Z)\and
\end{mathpar}

We leave it to the reader to extend the metatheory to deal with $\tensor$ and $\unit$ and prove the initiality theorem; there is very little difference with the prop case.
Similarly, we can construct ``presented'' props and symmetric monoidal categories by including arbitrary generators of $\equiv$.
The uniqueness of antipodes in a bimonoid presented in \cref{sec:intro} is an example of this: \cG has one object $M$ and four morphisms
\begin{alignat*}{2}
m&:(M,M)\to M &&\quad\text{(written infix as $m(x,y) = x\cdot y$)}\\
e&:()\to M\\
\triangle&:M\to (M,M) &&\quad\text{(written a little abusively as $\triangle_i(x) = x_i$)}\\
\ep &:M\to () &&\quad\text{(written $\ep(x) = \cancel{x}$)}
\end{alignat*}
while the generators of $\equiv$ are the bimonoid axioms.

Here is another example.
If $A$ is an object of a prop, a \textbf{dual} of $A$ is an object $A^*$ with morphisms $\eta:()\to (A,A^*)$ and $\ep:(A^*,A)\to ()$ such that $\ep \circ_{A^*} \eta = \idfunc_A$ and $\ep \circ_{A} \eta = \idfunc_A^*$.
(In a symmetric monoidal category this reduces to the usual notion of dual.)
If $A$ has a dual $A^*$, and $f:A\to A$, the \textbf{trace} of $f$ is the composite 
\[ \tr(f) = \ep \circ_{A,A^*} (f \circ_A \eta) \]
or equivalently $(\ep \circ_A f)\circ_{A,A^*} \eta$.
In the type theory for props, the axioms of a dual say
\begin{mathpar}
  x:A \types (\eta_1 \mid \ep(\eta_2,x)) \equiv x :A\and
  y:A^* \types (\eta_2 \mid \ep(y,\eta_1)) \equiv y :A^*
\end{mathpar}
while the trace is $(\,\mid \ep(\eta_2,f(\eta_1)))$.
Recall that $\equiv$ is a congruence for substitution; thus the dual axioms mean that \emph{any term} $M$ (appearing even as a sub-term of some other term) can be replaced by $\eta_1^\fa$ if we simultaneously add $\ep^\fb(\eta_2^\fa,M)$ to the list of null terms (here \fa and \fb are fresh labels).
And dually, if $\ep^\fb(\eta_2^\fa,M)$ appears in the null terms, for any term $M$, then it can be removed by replacing $\eta_1^\fa$ (wherever it appears) with $M$.

Now a classical fact about the trace is that it is \emph{cyclic}: if $A$ and $A'$ both have duals $A^*$ and $(A')^*$, and $f:A\to A'$ and $g:A'\to A$, then $\tr(gf) = \tr(fg)$.
To prove this using traditional commutative-diagram reasoning is quite involved.
It does have a pretty and intuitive proof using string diagrams.
However, its proof in the type theory for props is one line:
\[ \tr(gf) = (\,\mid \ep(\eta_2,g(f(\eta_1))))
= (\,\mid \ep(\eta_2,g(\eta'_1)),\ep'(\eta'_2,f(\eta_1)))
= (\,\mid \ep(\eta'_2,f(g(\eta'_1))))
= \tr(fg).
\]
Here $\eta,\ep$ exhibit the dual of $A$, while $\eta',\ep'$ exhibit the dual of $A'$.
The first and last equality are by definition;
the second applies the first duality equation for $A'$ with $x=f(\eta_1)$; and the third applies the first duality equation for $A$ with $x=g(\eta'_1)$.

One thing to note about these examples is that they use only the type theory of props, not its extension to symmetric monoidal categories.
In general, because the underlying prop of a symmetric monoidal category remembers its morphisms both into and out of tensor products, it seems rarely necessary to invoke the actual tensor product objects.

\begin{rmk}
  A \textbf{compact closed} category is a symmetric monoidal category in which every object has a dual.
  By adding appropriate objects, generators, and relations, we can obtain a type theory describing the free compact closed category on a polygraph, or on a graph, or on a category.
  This does not ``solve the coherence problem'' for compact closed categories, however, since with the additional $\equiv$ relations for duals, the terms no longer have an obvious canonical form.
  In fact, it turns out that an explicit description of the free compact closed category on a category can be given, and involves traces in an essential way; see~\cite{kl:cpt}.
\end{rmk}


\chapter{First-order logic}
\label{chap:fol}


\appendix

\chapter{Deductive systems}
\label{chap:dedsys}

The purpose of this appendix is to explain the formal apparatus underlying type theory from a mathematical perspective, giving precise meanings to words like ``judgment'', ``rule'', and ``derivation''.
This is rarely explained in detail, yet in my experience the unfamiliar terminology is a large part of what makes type theory difficult for mathematicians to understand.

Formally speaking, this appendix should come before \cref{chap:unary}.
However, its technicalities seem unlikely to be appreciated without some concrete exposure to the ideas that it is trying to explain, so I have placed it as an appendix instead.
I encourage the reader to skip back and forth between it and the main text as needed.

I should say that probably not all type theorists would agree with the meanings assigned herein to words like ``judgment''.
Constructive type theory also has a philosophical/foundational aspect that I will not attempt to explain or engage with.
The purpose of this appendix, like that of the entire book, is to explain type theory \emph{only} in its role as a language for reasoning about categorical structures, without meaning thereby to disparage its other roles or regard them as unimportant.


\section{Trees and free algebras}
\label{sec:trees}

As remarked in \cref{sec:generalities}, our perspective on type theory is that it presents \emph{free categorical structures} in a particularly convenient way.
Since categorical structures are particular kinds of \emph{algebraic} structures, it seems reasonable to think first about what free algebraic structures look like in general.
In this section we begin by considering ``algebras without axioms''.

A \textbf{signature} $\sig$ is a set $\sig_1$ of \textbf{operations} with a function $\ay:\sig_1 \to \dN$ assigning to each operation a natural number\footnote{Everything in this chapter works just as well if arities are arbitrary cardinal numbers (except that in \cref{sec:axioms} we would require the axiom of choice).
  However, for simplicity we restrict to the case of finite arities, since that is where our ultimate interest lies.
  On the other hand, there may certainly be infinitely many operations.} called its \textbf{arity}.
A $\sig$-\textbf{algebra} is a set $A$ together with, for each $m\in\sig_1$, a function $\act m :A^{\ay(m)} \to A$.
There is an obvious notion of $\sig$-algebra morphism, forming a category.

Algebras over a signature are a very ``primordial'' sort of algebraic structure, with arbitrary operations but no axioms allowed.
For instance, if $\sig_1=\{e,m\}$ with $\ay(e)=0$ and $\ay(m)=2$, then $\sig$-algebras are \emph{pointed magmas}: sets equipped with a basepoint and a binary operation.
We will see how to add axioms in \cref{sec:axioms}.

Free $\sig$-algebras are conveniently described in terms of {trees}.
A \textbf{tree} is a set whose elements are called \textbf{nodes}, together with a binary relation between them called \textbf{edge existence} (a ``relational graph'') that is connected and loop-free.
A tree is \textbf{rooted} if it is equipped with a chosen node called the \textbf{root}.
In a rooted tree every node $x$ has a unique (non-retracing) path to the root; if $x$ is not the root, this path goes through a unique edge connected to $x$ that we call \textbf{outgoing}, and the node at the other end of that edge is the \textbf{parent} of $x$.
The non-outgoing edges connected to $x$ are called \textbf{incoming}, and the nodes they connect it to are called its \textbf{children}.
A node is a \textbf{descendant} of $x$ if its path to the root passes through $x$, which is to say it is a child of a child of a\dots of $x$.
A node $x$ together with all its descendants forms another rooted tree with $x$ as the root.
% The \textbf{height} of a node is the length of its path to the root; this is always a natural number, and the height of a child of $x$ is one more than the height of $x$.
A \textbf{branch} is a non-retracing path starting at the root; a rooted tree is \textbf{well-founded} if there are no infinite branches.

If $\sig$ is a signature, then a $\sig$-\textbf{labeled tree} is a rooted tree equipped with a \textbf{labeling} function from nodes to $\sig_1$, along with for every node $x$ labeled by $m\in\cO$, a bijection from the incoming edges of $x$ to $\{1,\dots,\ay(m)\}$ (hence, in particular, that there are exactly $\ay(m)$ such edges).
There is an obvious notion of \emph{isomorphism} between labeled trees.
We write $W\sig$ for the set of all isomorphism classes of \emph{well-founded} $\sig$-labeled trees.
(Note that $W\sig$ is empty unless there is at least one nullary operation.)
Then $W\sig$ has a $\sig$-algebra structure defined as follows: given $m\in\sig_1$ and a list of trees $t_1,\dots,t_{\ay(m)}$, define a tree $\act m (t_1,\dots,t_{\ay(m)})$ with nodes $\{\star\} \sqcup \bigsqcup_i t_i$, where $\star$ is the root, with label $m$, and its children are the roots of the trees $t_i$.

The central fact is that $W\sig$ is the initial $\sig$-algebra.
We will give a classical set-theoretic proof of this for the comfort of a certain kind of reader, but readers of a different kind, or who already believe this fact, are welcome to skip the proof.
(From a constructive type-theoretic point of view, $W\sig$ and its initiality are sometimes a fundamental axiom not reducible to sets.)

\begin{thm}\label{thm:tree-ind}
  Suppose $P\subseteq W\sig$ has the property that for any $m$ and trees $t_1,\dots,t_{\ay(m)}$ such that each $t_i\in P$, then also $\act m(t_1,\dots,t_{\ay(m)})\in P$.
  Then $P= W\sig$.
\end{thm}
\begin{proof}
  Suppose not, so there is a well-founded $\sig$-labeled tree not in $P$.
  Let $m$ be the label of its root and $t_1,\dots,t_{\ay(m)}$ its children; then our given tree is (isomorphic to) $\act m(t_1,\dots,t_{\ay(m)})$.
  By the contrapositive of our assumption, therefore, there must be some $i$ such that $t_i\notin P$.
  Iterating, we obtain an infinite branch, contradicting well-foundedness.
\end{proof}

\begin{thm}\label{thm:tree-rec}
  For any $\sig$-algebra $A$, there is a unique $\sig$-algebra morphism $W\sig \to A$.
\end{thm}
\begin{proof}
  TODO: standard argument.
\end{proof}

Now that we have \emph{initial} $\sig$-algebras, note that \emph{free} $\sig$-algebras can be constructed by a simple modification.
Given $\sig$ and any set $X$, define a new signature $\sig[X]$ by $\sig[X]_1 = \sig_1 \sqcup X$, where $\ay(x)=0$ for all $x\in X$.
Then a $\sig[X]$-algebra is just a $\sig$-algebra together with a map from $X$ into its underlying set, so the initial such algebra is exactly the free $\sig$-algebra on $X$.
Thus, the forgetful functor from $\sig$-algebras to sets has a left adjoint.

A different way to express \cref{thm:tree-rec} is that \emph{given an arbitrary set $A$, to define a function $W\sig \to A$ it suffices to define a $\sig$-algebra structure on $A$}.
This may seem like a trivial reformulation, but it better reflects the way we use it to describe type theories.

In yet other words, we may define a function $f:W\sig \to A$ by specifying $f(\act m(t_1,\dots,t_{\ay(m)}))$ for each $m$, assuming recursively that $f(t_1),\dots,f(t_{\ay(m)})$ have already been defined.
Formally this is the same as specifying a $\sig$-algebra structure on $A$ --- the definition of ``$f(\act m(t_1,\dots,t_{\ay(m)}))$'' given the ``values of $f(t_1),\dots,f(t_{\ay(m)})$'' is precisely the action $\act m$ on $A$ --- but it often matches our thought processes best.

\subsection*{Exercises}

\begin{ex}\label{ex:wf-rigid}
  Prove that a well-founded $\sig$-labeled tree has no nonidentity automorphisms.
  Thus, the passage to isomorphism classes in the definition of $W\sig$ is ``categorically harmless''.
\end{ex}

\begin{ex}\label{ex:natw}
  Exhibit a signature $\sig$ such that $W\sig \cong \dN$ and \cref{thm:tree-ind} reduces to ordinary mathematical induction.
\end{ex}


\section{Indexed trees}
\label{sec:indexed-trees}

The signatures and algebras in \cref{sec:trees} have only one underlying set, or \emph{sort}, but sometimes algebraic structures have more than one sort.
As a simple example, we could consider a set together with a group acting on that set to be a single algebraic structure; then the group and the set are two sorts.

Categories could be regarded as having two sets, namely objects and arrows; but it is generally better to treat them differently.
Specifically, for a fixed set $\cO$, we regard categories with object set \cO as an algebraic structure whose set of sorts is $\cO\times \cO$.
Thus each hom-set is a separate sort, and each triple $A,B,C$ gives a different binary composition operation
\[ \circ_{A,B,C} : (\hom(B,C),\hom(A,B)) \to \hom(A,C) \]
This may seem a little odd, but as we will see it makes sense.

To deal with multi-sorted algebraic structures in general, we augment our signatures with a set $\sig_0$ of \textbf{sorts} together with, for each operation $m\in\sig_1$, an \textbf{output sort} $c_m\in\sig_0$ and also a list of \textbf{input sorts} $d_{m,1},\dots,d_{m,\ay(m)}$.
For brevity we write such an operation as $m:(d_{m,1},\dots,d_{m,\ay(m)}) \to c_m$.
From now on we call these \emph{multi-sorted signatures} simply \textbf{signatures}; the simpler signatures of \cref{sec:trees} we re-christen \textbf{one-sorted signatures}.
(In fact, a multi-sorted signature is essentially exactly the same as a ``multigraph'', \cref{defn:multigraph}.)

For a multi-sorted signature $\sig$, a $\sig$-\textbf{algebra} is a $\sig_0$-indexed family of sets $\{A_i\}_{i\in\sig_0}$ together with for each $m\in\sig_1$ a function $A_{d_{m,1}}\times \cdots \times A_{d_{m,\ay(m)}} \to A_{c_m}$.
For instance, if $\sig_0 = \{g,s\}$ and $\sig_1=\{m,t\}$ with
$m : (g,g) \to g$ and $t : (g,s) \to s$,
then an indexed algebra consists of two sets $A_g$ and $A_s$, a binary operation on $A_g$, and an action of $A_g$ on $A_s$.

Similarly, we define a $\sig$-\textbf{labeled tree} as before, with the additional requirement that if $x$ is the $k^{\mathrm{th}}$ child of $y$, and $x$ is labeled by $m\in\sig_1$ while $y$ is labeled by $p\in\sig_1$, then $c_m = d_{p,k}$.
For each $i\in\sig_0$, let $W\sig_i$ be the set of isomorphism classes of $\sig$-labeled trees for which the output sort of the root is $i$.
Then $\{W\sig_i\}_{i\in\sig_0}$ has a similar tautological $\sig$-algebra structure, and is the initial one.

\subsection*{Exercises}

\begin{ex}
  Prove that $\{W\sig_i\}_{i\in\sig_0}$ is the initial $\sig$-algebra.
\end{ex}

\section{Free algebras with axioms}
\label{sec:axioms}

Of course, most algebraic structures of interest contain axioms as well as operations; for instance, multiplication in a group or monoid must be associative and unital.
The free monoid on a set $X$ is naturally regarded as a quotient of the free pointed magma on $X$ that forces associativity and unitality to hold.
It turns out that we can construct free algebras of this sort quite generally by defining an equivalence relation \emph{as another indexed free algebra}.

Making this completely precise in general is a bit technical, so we will begin with a concrete example.
Suppose we want to generate the free semigroup on a set $X$.
Let $\F\bMag X$ denote the free magma on $X$, constructed as in \cref{sec:trees}.
(A magma is a set with a single binary operation; a semigroup is a magma whose operation is associative.)

Now define a signature $\sig^{\equivsym}$ with $\sig^{\equivsym}_0 = \F\bMag X\times \F\bMag X$ and the following operations.
\begin{itemize}
\item For each $x\in \F\bMag X$, a nullary operation $() \to (x,x)$.
\item For each $x,y\in \F\bMag X$, a unary operation $((x,y)) \to (y,x)$.
\item For each $x,y,z\in \F\bMag X$, a binary operation $((x,y),(y,z)) \to (x,z)$.
\item For each $x,y,z,w\in \F\bMag X$, a binary operation \[((x,y),(z,w)) \to (x\cdot z,y\cdot w),\] where $\cdot$ denotes the binary magma operation on $\F\bMag X$.
\item For each $x,y,z\in \F\bMag X$, a nullary operation \[() \to (x\cdot (y\cdot z),(x\cdot y)\cdot z).\]
\end{itemize}
An algebra for this signature is an $(\F\bMag X\times \F\bMag X)$-indexed family of sets $R(x,y)$ equipped with elements and operations
\begin{align*}
  e_x &\in R(x,x)\\
  R(x,y) &\to R(y,x)\\
  R(x,y)\times R(y,z) &\to R(x,z)\\
  R(x,y) \times R(z,w) &\to R(x\cdot z,y\cdot w)\\
  a_{x,y,z} &\in R(x\cdot (y\cdot z),(x\cdot y)\cdot z)
\end{align*}
Now for any such $R$, ``$R(x,y)$ is nonempty'' is a binary \emph{relation} on $\F\bMag X$, which we abusively denote also by $R(x,y)$.
The above elements and operations imply that this is an equivalence relation that is a congruence for the magma operation and moreover relates $x\cdot (y\cdot z)$ to $(x\cdot y)\cdot z$ for all $x,y,z$.
And conversely, if we have any such binary relation $\sim$, we can construct an indexed algebra $R$ by setting $R(x,y) = \unit$ if $x\sim y$ and $R(x,y)=\emptyset$ otherwise.

Let $\equivsym$ denote the binary relation obtained as above from nonemptiness of the \emph{initial} algebra for this indexed signature.

\begin{thm}\label{thm:free-monoid}
  The quotient of $\F\bMag X$ by $\equivsym$ is the free semigroup generated by $X$.
\end{thm}
\begin{proof}
  First we show that it is a semigroup.
  Given $u,v\in\F\bMag X/\equivsym$, choose representatives $x,y\in\F\bMag X$ for them, and let $u\cdot v$ be the equivalence class of $x\cdot y$.
  Since $\equiv$ is a congruence for the magma operation, the result is independent of the choice of representatives; thus $\F\bMag X/\equivsym$ is a magma.
  Now given $u,v,w \in \F\bMag X/\equivsym$, choose representatives $x,y,z$; then since $x\cdot (y\cdot z)\equiv (x\cdot y)\cdot z$, we have $u\cdot (v\cdot w) =  (u\cdot v)\cdot w$.
  Thus $\F\bMag X/\equivsym$ is a semigroup

  Now let $M$ be any other semigroup and $\psi:X\to M$ a map.
  Since $M$ is in particular a magma, we have a unique induced magma morphism $\phi : \F\bMag X\to M$.
  Define a binary relation $R$ on $\F\bMag X$ by saying that $R(x,y)$ means $\phi(x)=\phi(y)$.
  Since $\phi$ is a magma morphism and $M$ is a semigroup, $R$ can be regarded as an algebra for the above indexed signature.
  Thus it admits a map from the initial such algebra.
  Hence, if $x\equiv y$, then $R(x,y)$, i.e.\ $\phi(x)=\phi(y)$; so $\phi$ factors through $\F\bMag X/\equivsym$.
  It is straightforward to check that this factorization is a semigroup morphism and is the unique such extending $\psi$.
\end{proof}

In the general case, we proceed as follows.
Suppose $\sig$ is a (multi-sorted) signature and we have additionally a set $\axes$ of \textbf{axioms}, each of which is a pair $(a,b)$ of elements of the free algebra $W\sig[V]_i$ for some $i\in\sig_0$ and some finite set $V$.
Then for any $\sig$-algebra $A$, any axiom $a,b\in W\sig[V]_i$, and any function $g:V\to A$ (picking out some finite set of elements of $A$), we have an induced $\sig$-algebra map $\gbar:W\sig[V]\to A$.
We define a $(\sig,\axes)$-\textbf{algebra} to be a $\sig$-algebra $A$ such that $\gbar(a)=\gbar(b)$ for any $(a,b)\in\axes$ and $g:V\to A$.

For instance, associativity in a magma is represented by the axiom
\[ \left(
  \vcenter{\xymatrix@-1.5pc{ & m \ar@{-}[dl] \ar@{-}[dr] \\ x && m \ar@{-}[dl] \ar@{-}[dr] \\ & y && z }},
  \vcenter{\xymatrix@-1.5pc{ && m \ar@{-}[dl] \ar@{-}[dr] \\ & m \ar@{-}[dl] \ar@{-}[dr] && z \\ x && y }}
\right)
\in W\sig[\{x,y,z\}]
\]
The $(\sig,\axes)$-algebras in this case are exactly semigroups.

Now, given a set $X$, we define a signature $\sig^{\equivsym}$ with
\[\sig^{\equivsym}_0 = \setof{(i,x,y) | i\in \sig_0; x,y\in W\sig[X]_i}\]
and the following operations:
\begin{itemize}
\item For each $x\in W\sig[X]_i$, a nullary operation $() \to (i,x,x)$.
\item For each $x,y\in W\sig[X]_i$, a unary operation $((i,x,y)) \to (i,y,x)$.
\item For each $x,y,z\in W\sig[X]_i$, a binary operation $((i,x,y),(i,y,z)) \to (i,x,z)$.
\item For each operation $m:(d_{m,1},\dots,d_{m,\ay(m)}) \to c_m$ in $\sig$, and each collection of pairs of elements $x_k,y_k \in W\sig[X]_{d_{m,k}}$ for $1\le k\le \ay(m)$, an operation
  \begin{multline*}
    ((d_{m,1},x_1,y_1),\dots,(d_{m,\ay(m)},x_{\ay(m)},y_{\ay(m)})) \\\too
    (c_m, \act m(x_1,\dots,x_{\ay(m)}), \act m(y_1,\dots,y_{\ay(m)})).
  \end{multline*}
\item For each axiom $a,b\in W\sig[V]_i$ in $\axes$ and each function $g:V\to W\sig[X]$ with unique extension $\gbar:W\sig[V]\to W\sig[X]$, a nullary operation
  \[ () \to (i,\gbar(a),\gbar(b)). \]
\end{itemize}
Let $\equivsym_i$ be the binary relation on $W\sig[X]_i$ defined by $a\equiv_i b$ if the sort $(i,a,b)$ is nonempty in the initial $\sig^{\equivsym}$-algebra.

\begin{thm}\label{thm:tree-quotient}
  Each $\equivsym_i$ is an equivalence relation and a congruence for the $\sig$-algebra structure, and the quotients $W\sig[X]_i/\equivsym_i$ form the free $(\sig,\axes)$-algebra.\qed
\end{thm}

As in \cref{sec:trees}, we will usually think of this theorem slightly differently: to define a family of maps $f_i:W\sig[X]_i/\equivsym_i\to A_i$, it suffices to define each $f_{c_m}(\act m(t_1,\dots,t_{\ay(m)}))$ assuming recursively that $f_{d_{m,1}}(t_1),\dots, f_{d_{m,\ay(m)}}(t_{\ay(m)})$ have been defined, and also to check that for any axiom $(a,b)\in W\sig[V]_i$ and $g:V\to W\sig[X]_i$ we have $f_i(\gbar(a))=f_i(\gbar(b))$.

\subsection*{Exercises}

\begin{ex}\label{ex:tree-quotient}
  Prove \cref{thm:tree-quotient}.
\end{ex}

\begin{ex}
  Why is the axiom of choice required to generalize \cref{thm:tree-quotient} to the case of infinitary operations?
\end{ex}


\section{Rules and deductive systems}
\label{sec:rules}

The basic machinery of type theory is an iteration and reformulation of the preceding sections in different language, simultaneously introducing convenient notations.

We consider a sequence of signatures $\sig^{(1)},\sig^{(2)},\dots,\sig^{(n)}$ for which the sorts of $\sig^{(k)}$ are defined in terms of the initial algebras $W\sig^{(j)}$ for the previous signatures $j<k$.
For instance, we might have $\sig^{(2)}_0 = W\sig^{(1)} \times W\sig^{(1)}$.
A particularly important special case is when $\sig^{(k)}$ is $(\sig^{(j)})^{\equivsym}$ for some $j<k$ and some set of axioms, as in \cref{sec:axioms}.

Each sort in one of the signatures $\sig^{(k)}$ is called a \textbf{judgment}.
We write $\cJ$ for a generic judgment, but we use more specific and congenial notation in particular cases, such as:
\begin{itemize}
\item When categories with object set \cO are regarded as an $(\cO\times\cO)$-sorted theory as mentioned in \cref{sec:indexed-trees}, the sort $(A,B)$ is usually written $A\types B$.
  This signature (with an $\equivsym$ on top of it) corresponds to the cut-ful type theory for categories from \cref{sec:category-cutful}.
  The cut-free type theory for categories has different operations but the same sorts, and uses the same notation.
\item If $\sig^{(1)}$ is a one-sorted signature regarded as describing the \emph{objects} of some categorical structure, then we denote its sort by ``$\mathsf{type}$''.
  We generally then have $\sig^{(2)}_0 = W\sig^{(1)} \times W\sig^{(1)}$ (for a unary type theory), with sorts again written as $A\types B$, where now $A$ and $B$ are elements of the initial $\sig^{(1)}$-algebra rather than elements of a fixed set \cO.
\item The multicategorical and polycategorical theories of \cref{chap:simple,chap:polycats} use a similar notation $\Gamma\types\Delta$ for sorts $(\Gamma,\Delta)$ where one or both of $\Gamma$ and $\Delta$ is a list rather than a single item.
\item If $\sig^{(k)}=(\sig^{(j)})^{\equivsym}$, then its sort $(\cJ,x,y)$ is usually written $x\equiv y : \cJ$.
\end{itemize}

In general, each operation $m:(\cJ_1,\dots,\cJ_n) \to \cJ'$ in one of the signatures $\sig^{(k)}$ is called a \textbf{rule}, and written
\[ \inferrule*[right=$m$]{\cJ_1 \\ \cdots \\ \cJ_n}{\cJ'}. \]
The input judgments $\cJ_1,\dots,\cJ_n$ of a rule are called its \textbf{premises}, and the output judgment $\cJ'$ is called its \textbf{conclusion}.

Finally, each element of $W\sig^{(k)}$ is called a \textbf{derivation} (sometimes a derivation \emph{of} its root judgment) and written by placing rules on top of each other to form its tree structure.
For instance, if \cJ denotes the single sort of the signature for semigroups, then the associativity axiom of a monoid is
\[
\inferrule*[Right=$m$]{\inferrule*[Right=$x$]{\qquad}{\cJ} \\
  \inferrule*[Right=$m$]{\inferrule*[Right=$y$]{\qquad}{\cJ}\\\inferrule*[Right=$z$]{\qquad}{\cJ}}{\cJ}}{\cJ}
\qquad \equiv \qquad
\inferrule*[Right=$m$]{\inferrule*[Right=$m$]{\inferrule*[Right=$x$]{\qquad}{\cJ}\\\inferrule*[Right=$y$]{\qquad}{\cJ}}{\cJ} \\
  \inferrule*[Right=$z$]{\qquad}{\cJ}}{\cJ}
\]
Note the rules with empty premises, corresponding to nullary operations.
Similarly, for the cut-ful type theory for categories, associativity is the collection of axioms (one for each $A,B,C\in\cO$)
\begin{multline*}
\inferrule*[right=$\circ_{A,B,D}$]{\inferrule*[Right=$x$]{\qquad}{A\types B} \\
  \inferrule*[Right=$\circ_{B,C,D}$]{\inferrule*[Right=$y$]{\qquad}{B\types C}\\\inferrule*[Right=$z$]{\qquad}{C\types D}}{B\types D}}{A\types D}
\\ \equiv \qquad
\inferrule*[right=$\circ_{A,C,D}$]{\inferrule*[right=$\circ_{A,B,C}$]{\inferrule*[Right=$x$]{\qquad}{A\types B}\\\inferrule*[Right=$y$]{\qquad}{B\types C}}{A\types C} \\
  \inferrule*[Right=$z$]{\qquad}{C\types D}}{A\types D}
\end{multline*}

The whole sequence of signatures $\sig^{(1)},\sig^{(2)},\dots,\sig^{(n)}$ is called a \textbf{deductive system}.
Thus, for instance, the signature $\sig[X]$ for semigroups under a fixed set $X$, together with the axiom-signature for monoids under $X$ on top of it, form a single deductive system.
Some deductive systems (probably not all) deserve to be called \emph{type theories}; but we will not attempt to give any definition of this class except by the examples we consider (throughout the entire book).

There is one further feature that seems trivial, but is eventually responsible for much of the value of categorical logic.
Since the judgments in $\sig^{(k)}$ are defined in terms of the \emph{elements} of $W\sig^{(j)}$ for $j<k$, and the latter are rooted trees, the notation would rapidly get unwieldy if each $\cJ$ in a rule contained within it some number of derivation trees.
Thus, we also represent derivations by \textbf{terms}, which are a ``linear'' syntax containing enough information to reconstruct the derivation.
For instance, the expressions $x\cdot (y\cdot z)$ and $(x\cdot y)\cdot z$ for the two sides of associativity are terms, in which we have represented the rule $m$ by an infix operation ``$\cdot$''.

If $M$ is a term representing a derivation of the judgment \cJ, we generally write $M:\cJ$.
(A notable exception is that if \cJ is the sort of a one-sorted $\sig^{(1)}$ presenting the objects of a category, as mentioned above, we usually write ``$A\type$'' or ``$\types A\type$'' rather than ``$A:\mathsf{type}$''.)
We describe a syntax for terms by annotating the rules of a deductive system with terms, so that for instance the multiplication of a semigroup would be
\[ \inferrule*[Right=$m$]{M:\cJ \\ N:\cJ}{M\cdot N :\cJ} \]
Here $M$ and $N$ are ``metavariables'' standing for terms, indicating that whatever terms we have representing two derivations of \cJ, we represent their combination by $m$ by juxtaposing them with an infix dot.
(We always assume that parentheses are added as necessary to ensure correct grouping.)

For purposes of this discussion, ``terms with variables from the context'' such as $x:A\types M:B$ can be regarded as merely a variant notation of something like $(x.M) : (A\types B)$, so that we still have a single thing called a ``term'' that represents the entire derivation.
Similarly, an equality judgment like $x:A \types M\equiv N:B$ is shorthand for $(x.M)\equiv (x.N) : (A\types B)$.
We always consider terms of this form modulo ``$\alpha$-equivalence'', i.e.\ the consistent renaming of variables, as discussed on page~\pageref{sec:alpha} in \cref{sec:alpha}.

There is no unique way to assign terms to a deductive system; all that is necessary is to describe some kind of syntax from which a derivation tree can be algorithmically extracted.
Since this is not a book about syntax, we will not attempt to make precise exactly what this means, but it is worth saying a bit more about about it.
In fact, when a human mathematician reads an expression such as $x\cdot (y\cdot z)$, they generally mentally organize it as a tree without really thinking about it: here the first $\cdot$, being the ``outer'' operation, is the root, with children $x$ and $y\cdot z$, and the latter decomposes further into another $\cdot$ node with children $y$ and $z$.
This ``internal syntax tree'' is exactly the intended derivation tree.
An alternative reading where the second $\cdot$ is the root with children ``$x\cdot (y$'' and ``$z)$'' is ruled out by our intuitive understanding of the meaning of parentheses.
When a computer reads such an expression it likewise constructs an internal tree representation, but the programmer has to explicitly instruct it how to do so; this is called \textbf{parsing}.

If we are given a putative term claiming to represent a derivation of some judgment, then after parsing there is a further step of verifying that the ``parse tree'' indeed corresponds to a valid derivation tree.
This is called \textbf{type-checking}.
Technically it could be done at the same time as parsing, but both human and electronic mathematicians generally separate them; thus the parse tree is a sort of ``raw abstract syntax'' that knows how operations are grouped but not whether the operations actually mean anything yet.

There are two main points I want to make here about type-checking.
One is that the input to type-checking is usually a parsed term together with a putative type for that term, and so the term notations only need to contain enough information to reconstruct the derivation tree when supplemented with the latter.
For instance, we have noted that the cut-ful type theory for categories technically involves a different composition operation $\circ_{A,B,C}$ for each triple of objects, so that terms would technically have to be written as $h\circ_{A,C,D} (g\circ_{A,B,C} f)$.
However, if we are given a term whose outer operation is a composition and that claims to represent a derivation of a judgment $A\types D$, then the composition must be $\circ_{A,?,D}$.
Thus in general it suffices to indicate the object being composed over, as in $h\circ_C (g\circ_B f)$.

In many cases we can omit further information because it can be inferred from context; for instance, if we know that $h:C\to D$ then a term of the form ``$h\circ (-)$'' can only mean ``$h\circ_C (-)$''.
Human mathematicians omit information informally and unsystematically, and we have done the same throughout the book.
The implementors of electronic mathematicians have elaborate and precise algorithms for ``inferring from context'' enabling the omission of information, but most of these are far beyond our scope.\footnote{One that is worth mentioning, however, is that a canonical/atomic calculus like that of \cref{sec:atomcan} can be type-checked in a ``bidirectional'' way. canonical terms $M\can A$ are type-checked as usual with both $M$ and $A$ being given, while atomic terms $M\atom A$ instead ``type-synthesize'': only $M$ is given and its type $A$ is deduced.
  Paradoxically, this sometimes enables the omission of more information.
  For instance, if when type-checking a function application $f(a)\can B$ the term $f$ can synthesize its type $A\to B$ (note that it is atomic since application is an elimination rule for a negative type), then we can extract the type $A$ at which we have to type-check the argument $a\can A$; thus the notation for function application doesn't need to notate the type $A$.}

The other point I want to make is that with type-checking (and also ``proof search'') in mind, type theorists tend to read the rules of a deductive system ``bottom-up''.
That is, instead of thinking of a rule
\begin{mathpar}
  \inferrule*{\cJ_1 \\ \cJ_2}{\cJ}
\end{mathpar}
as meaning ``if we have $\cJ_1$ and $\cJ_2$ we can deduce $\cJ$'', they instead think ``if we want to deduce $\cJ$, it suffices to have $\cJ_1$ and $\cJ_2$''.
This is the direction that a type-checking algorithm applies the rule: given a parsed term $M$ and a putative type $A$, the rule tells us how to break down the job of checking that $M:A$ into simpler type-checking tasks that can be done recursively.
It is also the direction that the rule is often applied when \emph{searching} for a derivation of \cJ, by the same sort of recursive procedure.
At least when talking to type theorists, it can be helpful to be familiar with this way of thinking.

Finally, I must say that there are actually plenty of exceptions to this general picture, and that things can get much more complicated.
The most notable example is dependent type theories, which among other things have terms that do not exactly represent derivations because of the ``coercion'' rule:
\begin{mathpar}
  \inferrule*{M:A \\ A\equiv B}{M:B}
\end{mathpar}
Dependent type theories also break the clean ``stratification'' of a deductive system $\sig^{(1)},\sig^{(2)},\dots,\sig^{(n)}$, since in the judgment $\types A\type$ the type $A$ can now contain terms from the ``higher level'' judgment $\Gamma\types M:B$; thus the whole system must be defined by one big mutual induction (in type-theoretic lingo it is an ``inductive-inductive definition'').
However, it is still helpful to understand the ``standard'' picture in order to better appreciate these variations.


\bibliographystyle{alpha}
\bibliography{all}

\end{document}
