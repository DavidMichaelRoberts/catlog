\documentclass{article}
\usepackage{mathpartir,cancel}
\newif\ifcref\creftrue
\input{decls}
\title{Categorical logic from a categorical point of view}
\author{}
\date{\today}
\autodefs{\bSet\bPoset\bRelGr\bCat\bGr\bmSLat\ftype}
\let\types\vdash
\let\sect\section
\def\idfunc{\mathsf{id}}
\def\type{\;\ftype}
% meet
\let\meet\wedge
\def\meetL{\mathord{\meet}L}
\def\meetR{\mathord{\meet}R}
\def\meetE{\mathord{\meet}E}
\def\meetI{\mathord{\meet}I}
% join
\let\join\vee
\begin{document}
\maketitle
%\setcounter{tocdepth}{1}
\tableofcontents

\section{Appetizer: inverses in group objects}
\label{sec:intro}

In this section we consider an extended example.
We do not expect the reader to understand it very deeply, but we hope it will give some motivation for what follows, as well as a taste of the power and flexibility of categorical logic as a tool for category theory.

Our example will consist of several varations on the following theorem:

\begin{thm}
  If a monoid has inverses (hence is a group), then those inverses are unique.
\end{thm}

When ``monoid'' and ``group'' have their usual meaning, namely sets equipped with structure, the proof is easy.
For any $x$, if $i(x)$ and $j(x)$ are both two-sided inverse of $x$, then we have
\[ i(x) = i(x) \cdot e = i(x) \cdot (x \cdot j(x)) = (i(x)\cdot x)\cdot j(x) = e\cdot j(x) = j(x) \]
However, the theorem is true much more generally than this.
We consider first the case of monoid/group objects in a category with products.
A \emph{monoid object} is an object $M$ together with maps $m:M\times M \to M$ and $e:1\to M$ satisfying associativity and unitality axioms:
\begin{equation}
  \vcenter{\xymatrix{
      M\times M\times M\ar[r]^-{1\times m}\ar[d]_{m\times 1} &
      M\times M\ar[d]^m\\
      M\times M\ar[r]_m &
      M
    }}
  \qquad
  \vcenter{\xymatrix{ M \ar[r]^-{(1,e)} \ar[dr]_{1} &
    M\times M \ar[d]_m & M \ar[l]_-{(e,1)} \ar[dl]^{1} \\
    & M }}
\end{equation}
An \emph{inverse operator} for a monoid object is a map $i:M\to M$ such that
\begin{equation}
  \vcenter{\xymatrix@C=1pc{& M\times M \ar[rr]^{i\times 1} && M\times M \ar[dr]^m \\
      M \ar[ur]^{\Delta} \ar[dr]_{\Delta} \ar[rr]^{!} && 1 \ar[rr]^{e} && M \\
      & M\times M \ar[rr]_{1\times i} && M\times M \ar[ur]_{m}}}
\end{equation}
The internalized claim, then, is that any two inverse operators are equal.
The \emph{internal logic of a category with products} allows us to prove this by using essentially the same argument that we did in the case of ordinary monoids in \bSet.
The morphisms $m$ and $e$ are represented in this logic by the notations
\begin{mathpar}
  x:M,y:M \types x\cdot y :M \and
  \types e:M.
\end{mathpar}
Don't worry if this notation doesn't make a whole lot of sense yet.
The symbol $\types$ (called a ``turnstile'') is the logic version of a morphism arrow $\to$, and the entire notation is called a \emph{sequent} or a \emph{judgment}.
The fact that $m$ is a morphism $M\times M \to M$ is indicated by the fact that $M$ appears twice to the left of $\types$ and once to the right; the comma ``$,$'' in between $x:M$ and $y:M$ represents the product $\times$, and the variables $x,y$ are there so that we have a good notation ``$x\cdot y$'' for the morphism $m$.
In particular, the notation $x:M,y:M \types x\cdot y :M$ should be bracketed as
\[ ((x:M),(y:M)) \types ((x\cdot y) :M). \]
Similarly, the associativity, unit, and inverse axioms are indicated by the notations
\begin{mathpar}
  x:M,y:M,z:M \types (x\cdot y)\cdot z = x\cdot (y\cdot z) : M \\
  x:M \types x\cdot e = x : M \and
  x:M \types e\cdot x = x : M \\F
  x:M \types x\cdot i(x) = e : M \and
  x:M \types i(x) \cdot x = e : M
\end{mathpar}
Now the \bSet-based proof can be essentially copied in this notation:
\[ x:M \types i(x) = i(x) \cdot e = i(x) \cdot (x \cdot j(x)) = (i(x)\cdot x)\cdot j(x) = e\cdot j(x) = j(x) : M.\]
The essential point is that the notation \emph{looks set-theoretic}, with ``variables'' representing ``elements'', and yet (as we will see) its formal structure is such that it can be interpreted into \emph{any} category with products.
Therefore, writing the proof in this way yields automatically a proof of the general theorem that any two inverse \emph{operators} for a monoid \emph{object} in a category with products are equal.

To be sure, such a proof could be written in standard categorical style using commutative diagrams as well (\cref{ex:fp-inv-uniq}), and experienced category theorists become quite good at translating proofs in this way.
The point of categorical logic is that this sort of work is \emph{so} straightforward that it's actually completely unnecessary to do at all: we can prove a ``meta-theorem'' that does all the work for us.

Before leaving this appetizer section, we mention some further generalizations of this result.
While type theory allows us to use set-like notation to prove facts about any category with finite products, the allowable notation is fairly limited, essentially restricting us to algebraic calculations with variables.
However, if our category has more structure, then we can ``internalize'' more set-theoretic arguments.

As an example, note that for ordinary monoids in sets the uniqueness of inverses can be expressed ``pointwise'' rather than in terms of inverse-assigning operators.
In other words, for each element $x\in M$, if $x$ has two two-sided inverses $y$ and $z$, then $y=z$.
If we think hard enough, we can express this diagrammatically in terms of the category \bSet is to consider the following two sets:
\begin{align*}
  A &= \setof{(x,y,z)\in M^3 | xy=e, yx=e, xz=e, zx=e}\\
  B &= \setof{(y,z)\in M^2 | y=z}
\end{align*}
In other words, $A$ is the set of elements $x$ equipped with two inverses, and $B$ is the set of pairs of equal elements.
Then the uniqueness of pointwise inverses can be expressed by saying there is a commutative diagram
\[ \xymatrix{ A \ar[d] \ar[r] & B \ar[d] \\ M^3 \ar[r]_{\pi_{23}} & M^2 } \]
where the vertical arrows are inclusions and the lower horizontal arrow projects to the second and third components.

This is a statement that makes sense for a monoid object $M$ in any category with finite \emph{limits}.
The object $B$ can be constructed categorically as the equalizer of the two projections $M\times M \toto M$ (which is in fact isomorphic to $M$ itself), while the object $A$ is a ``joint equalizer'' of four parallel pairs, one of which is
\[ \vcenter{\xymatrix{ & M \times M \ar[dr]^m \\
    M\times M\times M \ar[ur]^{\pi_{12}} \ar[dr]_{!} && M \\
    & 1 \ar[ur]_e }} \]
and the others are similar.
We can then try to \emph{prove}, in this generality, that there is a commutative square as above.
We can do this by manipulating arrows, or by appealing to the Yoneda lemma, but we can also use the \emph{internal logic of a category with finite limits}.
This is a syntax like the internal logic for categories with finite products, but which also allows us to \emph{hypothesize equalities}.
The judgment in question is
\begin{equation}\label{eq:pointwise-unique-inverses}
  x:M, y:M, z:M, x\cdot y = e, y\cdot x=e, x\cdot z = e, z\cdot x = e \types y=z.
\end{equation}
As before, the comma binds the most loosely, so this should be read as
\[ ((x:M), (y:M), (z:M), (x\cdot y = e), (y\cdot x=e), (x\cdot z = e), (z\cdot x = e)) \types (y=z). \]
We can prove this by set-like equational reasoning, essentially just as before.
The ``interpretation machine'' then produces from this a morphism $A\to B$, for the objects $A$ and $B$ constructed above.

Next, note that in the category \bSet, the uniqueness of inverses ensures that if every element $x\in M$ has an inverse, then there is a \emph{function} $i:M\to M$ assigning inverses --- even without using the axiom of choice.
(If we define functions as sets of ordered pairs, as is usual in set-theoretic foundations, we could take $i = \setof{(x,y) | xy=e}$; the pointwise uniqueness ensures that this is indeed a function.)
This fact can be expressed in the internal logic of an \emph{elementary topos}.
We postpone the definition of a topos until later; for now we just remark that its structure allows both sides of the turnstile $\types$ to contain \emph{logical formulas} such as $\exists x, \forall y, \phi(x,y)$ rather than just elements and equalities.
In this language we can state and prove the following:
\[ \forall x:M, \exists y:M, x\cdot y = e \land y\cdot x = e \types
\exists i:M^M, \forall x:M, (x\cdot i(x) = e \land i(x)\cdot x = e)
\]
As before, the proof is essentially exactly like the usual set-theoretic one.
Moreover, the interpretation machine allows us to actually extract an ``inverse operator'' morphism in the topos from this proof.
As before, such a result can also be stated and proved using arrows and commutative diagrams, but as the theorems get more complicated, the translation gets more tedious to do by hand, and the advantage of type-theoretic notation becomes greater.

So much for adding extra structure.
In fact, we can also take structure away!
A monoid object can be defined internal to any \emph{monoidal} category, not just a cartesian monoidal one; now the structure maps are $m:M\otimes M\to M$ and $e:I\to M$, and the commutative diagrams are essentially the same.

To define an inverse operator in this case, however, we need some sort of ``diagonal'' $\Delta:M\to M\otimes M$ and also a ``projection'' or ``augmentation'' $\varepsilon:M\to I$.
The most natural hypothesis is that these maps make $M$ into a \emph{comonoid} object, i.e.\ a monoid in the opposite monoidal category, and that the monoid and comonoid structures preserve each other; this is the notion of a \emph{bimonoid} (or ``bialgebra'').
(\cref{ex:cartmon-bimon-uniq}: in a cartesian monoidal category, every object is a bimonoid in a unique way.)

Now given a bimonoid $M$, we can define an ``inverse operator'' --- which in this context is usually called an \emph{antipode} --- to be a map $i:M\to M$ such that
\begin{equation}
  \vcenter{\xymatrix@C=1pc{& M\otimes M \ar[rr]^{i\otimes 1} && M\otimes M \ar[dr]^m \\
      M \ar[ur]^{\Delta} \ar[dr]_{\Delta} \ar[rr]^{\varepsilon} && I \ar[rr]^{e} && M \\
      & M\otimes M \ar[rr]_{1\otimes i} && M\otimes M \ar[ur]_{m}}}
\end{equation}
commutes, where now $\Delta$ and $\varepsilon$ are the comonoid structure of $M$ rather than the diagonal and projection of a cartesian product.
A bimonoid equipped with an antipode is called a \emph{Hopf monoid} (or ``Hopf algebra'').
The obvious question then is, if a bimonoid has two antipodes, are they equal?

In some cases it is possible to apply the previous results directly.
For instance, the category of \emph{(co)commutative} comonoids in a symmetric monoidal category inherits a monoidal structure that turns out to be \emph{cartesian} (\cref{ex:ccmon-cart}), so a cocommutative bimonoid is actually a monoid in a cartesian monoidal category, and we can apply the first version of our result.
Similarly, the category of commutative monoids is cocartesian, so a commutative bimonoid is a comonoid in a cocartesian monoidal category, so we can apply the dual of the first version of our result.
But what if neither the multiplication nor the comultiplication is commutative?

Internal logic is up to this task.
In a monoidal category we can consider judgments with multiple outputs as well as multiple inputs.
This allows us to describe monoids and comonoids in a roughly ``dual'' way:
\begin{alignat*}{2}
  x:M, y:M &\types x\cdot y:M &\qquad x:M &\types (x_1,x_2):(M,M)\\
  &\types e:M &\qquad x:M &\types \cancel{x}:()\\
  x:M,y:M,z:M &\types (x\cdot y)\cdot z = x\cdot (y\cdot z) :M &\qquad x:M &\types (x_{11},x_{12},x_2)=(x_1,x_{21},x_{22}):(M,M,M)\\
  x:M &\types x\cdot e=x:M &\qquad x:M &\types (x_1,\cancel{x_2}) = x:M\\
  x:M &\types e\cdot x=x:M &\qquad x:M &\types (\cancel{x_1},x_2) = x:M
\end{alignat*}
In this language, the bimonoid axioms are
\begin{align*}
  x:M,y:M &\types (x_1\cdot y_1,x_2\cdot y_2) = ((x\cdot y)_1,(x\cdot y)_2) :(M,M)\\
          &\types (e_1,e_2)=(e,e):(M,M)\\
  x:M,y:M &\types \cancel{x\cdot y} = (\cancel{x},\cancel{y}) : ()\\
  &\types \cancel{e}=():()
\end{align*}
And an antipode is a map $x:M \types i(x):M$ such that
\begin{align*}
  x:M &\types x_1\cdot i(x_2) = (e,\cancel{x}) :M\\
  x:M &\types i(x_1)\cdot x_2 = (e,\cancel{x}) :M
\end{align*}
Now if we have another antipode $j$, we can compute
\begin{align*}
  x:M \types i(x)
  &= i(x)\cdot e\\
  &= (i(x_1)\cdot e,\cancel{x_2})\\
  &= i(x_1)\cdot (x_{21} \cdot j(x_{22}))\\
  &= (i(x_1)\cdot x_{21}) \cdot j(x_{22})\\
  &= (\cancel{x_1},e \cdot j(x_{2}))\\
  &= e\cdot j(x)\\
  &= j(x) \qquad :M
\end{align*}
yielding the same result $i=j$.
So even in a non-cartesian situation, we can use a very similar set-like argument, as long as we keep track of where elements get ``duplicated and discarded''.

This concludes our ``appetizer''; I hope it has given you a taste of what categorical logic looks like, and what it can do for category theory.
In the next section we will rewind back to the beginning and start with very simple cases.

\subsection*{Exercises}

\begin{ex}\label{ex:fp-inv-uniq}
  Prove, using arrows and commutative diagrams, that any two inverse operators for a monoid object in a category with finite products are equal.
\end{ex}

\begin{ex}\label{ex:cartmon-bimon-uniq}
  Prove that in a cartesian monoidal category, every object is a bimonoid in a unique way.
\end{ex}

\begin{ex}\label{ex:ccmon-cart}
  Show that the category of cocommutative comonoids in a symmetric monoidal category inherits a monoidal structure, and that this monoidal structure is cartesian.
\end{ex}

\begin{ex}\label{ex:antipode}
  Prove, using arrows and commutative diagrams, that any two antipodes for a bimonoid (not necessarily commutative or cocommutative) are equal.
\end{ex}


\section{Posets}
\label{sec:poset}

% TODO: Mention structural induction.

To begin our formal investigation of categorical logic, we start with the simplest sort of categories: those in which each hom-set has at most one element.
These are well-known to be equivalent to \emph{preordered sets}, where the existence of an arrow $A\to B$ is regarded as the assertion that $A\le B$.
I will abusively call them \emph{posets}, although traditionally posets (partially ordered sets) also satisfy the antisymmetry axiom (if $A\le B$ and $B\le A$ then $A=B$); from a category-theoretic perspective, antisymmetry means asking a category to be skeletal, which is both unnatural and pointless.
Conveniently, posets also correspond to the simplest version of logic, namely \emph{propositional} logic.

From a category-theoretic perspective, the question we are concerned with is the following.
Suppose we have some objects in a poset, and some ordering relations between them.
For instance, we might have
\begin{mathpar}
  A\le B \and A\le C \and D\le A \and B \le E \and D\le C
\end{mathpar}
Now we ask, given two of these objects --- say, $D$ and $E$ --- is it necessarily the case that $D\le E$?
In other words, is it the case in \emph{any} poset containing objects $A,B,C,D,E$ satisfying the given relations that $D\le E$?
In this example, the answer is yes, because we have $D\le A$ and  $A\le B$ and $B\le E$, so by transitivity $D\le E$.
More generally, we would like a method to answer all possible questions of this sort.

There is an elegant categorical way to do this based on the notion of \emph{free structure}.
Namely, consider the category \bPoset of posets, and also the category \bRelGr of \emph{relational graphs}, by which I mean sets equipped with an arbitrary binary relation.
There is a forgetful functor $U:\bPoset \to \bRelGr$, which has a left adjoint $F$.

Now, the abstract information about ``five objects $A,B,C,D,E$ satisfying five given relations'' can be regarded as an object $\cG$ of \bRelGr, and to give five such objects satisfying those relations in a poset \cP is to give a map $\cG \to U\cP$ in \bRelGr.
By the adjunction, therefore, this is equivalent to giving a map $F\cG \to \cP$ in \bPoset.
Therefore, a given inequality such as $A\le E$ will hold in \emph{all} posets if and only if it holds in the \emph{particular, universal} poset $F\cG$ freely generated by the assumed data.

Thus, to answer all such questions at once, it suffices to give a concrete presentation of the free poset $F\cG$ generated by a relational graph \cG.
In this simple case, it is easy to give an explicit description of $F$: it is the reflexive-transitive closure.
But since soon we will be trying to generalize vastly, we want instead a general method to describe free objects.
From our current perspective, this is the role of type theory.

As noted in \cref{sec:intro}, when we move into type theory we use the symbol $\types$ instead of $\to$ or $\le$.
Type theory is concerned with \emph{(hypothetical) judgments}, which (roughly speaking) are syntactic gizmos of the form ``$\Gamma\types\Delta$'', where $\Gamma$ and $\Delta$ are syntactic gadgets whose specific nature is determined by the specific type theory under consideration (and, thus, by the particular kind of categories we care about).
We call $\Gamma$ the \emph{antecedent} or \emph{context}, and $\Delta$ the \emph{consequent} or \emph{co-context}.
In our simple case of posets, the judgments are simply
\[ A \types B \]
where $A$ and $B$ are objects of our (putative) poset; such a judgment represents the relation $A\le B$.
In general, the categorical view is that a hypothetical judgment represents a sort of \emph{morphism} (or, as we will see later, a sort of \emph{object}) in some sort of categorical structure.

In addition to a class of judgments, a type theory consists of a collection of \emph{rules} by which we can operate on such judgments.
Each rule can be thought of as a partial $n$-ary operation on the set of possible judgments for some $n$ (usually a finite natural number), taking in $n$ judgments (its \emph{premises}) that satisfy some compatibility conditions and producing an output judgment (its \emph{conclusion}).
We generally write a rule in the form
\begin{mathpar}
  \inferrule{\cJ_1 \\ \cJ_2 \\ \cdots \\ \cJ_n}{\cJ}
\end{mathpar}
with the premises above the line and the conclusion below.
A rule with $n=0$ is sometimes called an \emph{axiom}.
The categorical view is that we have a given ``starting'' set of judgments representing some objects and putative morphisms in the ``underlying data'' of a categorical structure, and the closure of this set under application of the rules yields the objects and morphisms in the \emph{free} structure it generates.

This is all very general and abstract, so let's bring it back down to earth in our very simple example.
Since the properties distinguishing a poset are reflexivity and transitivity, we have two rules:
\begin{mathpar}
  \inferrule{ }{A\types A} \and
  \inferrule{A\types B \\ B\types C}{A\types C}
\end{mathpar}
in which $A,B,C$ represent arbitrary objects.
In other words, the first rule is that for any object $A$ we have a $0$-ary rule whose conclusion is $A\types A$, while the second is that for any objects $A,B,C$ we have a $2$-ary rule whose premises are $A\types B$ and $B\types C$ (that is, any two judgments of which the consequent of the first is the antecedent of the second) and whose conclusion is $A\types C$.
We will refer to the pair of these two rules as the \textbf{free type theory of posets}.

Hopefully it makes sense that we can construct the reflexive-transitive closure of a relational graph by expressing its relations in this funny syntax and then closing up under these two rules, since they are exactly reflexivity and transitivity.
Categorically, of course, that means identities and composition.
In type theory the composition/transitivity rule is often called \emph{cut}, and plays a unique role; we will touch on this later on, though it is not as important from a purely categorical standpoint.

In the example we started from,
\begin{mathpar}
  A\le B \and A\le C \and D\le A \and B \le E \and D\le C
\end{mathpar}
we have the two instances of the transitivity rule
\begin{mathpar}
  \inferrule{D\types A \\ A\types B}{D\types B}\and
  \inferrule{D\types B \\ B\types E}{D\types E}
\end{mathpar}
allowing us to conclude $D\types E$.
When applying multiple rules in sequence to reach a conclusion, it is customary to write them in a ``tree'' structure like so:
\begin{mathpar}
  \inferrule*{\inferrule*{D\types A \\ A\types B}{D\types B} \\ B\types E}{D\types E}
\end{mathpar}
Such a tree is called a \emph{derivation}.
The way to typeset rules and derivations in \LaTeX\ is with the \texttt{mathpartir} package; the above diagram was produced with
\begin{verbatim}
  \inferrule*{
    \inferrule*{D\types A \\ A\types B}{D\types B} \\
    B\types E
  }{
    D\types E
  }
\end{verbatim}
Note that \texttt{mathpartir} has only recently made it into standard distributions of \LaTeX, so if you have an older system you may need to download it manually.

Formally speaking, what we have observed is the following \emph{initiality theorem}.

\begin{thm}\label{thm:poset-initial-1}
  For any relational graph \cG, the free poset $F_{\bPoset}\cG$ that it generates is has the same objects and its morphisms are the judgments that are derivable from \cG in free type theory of posets.
\end{thm}

This enables us to reach conclusions about arbitrary posets by deriving judgments in type theory.

Another way to express this result is to incorporate \cG into the rules.
Given a relational graph \cG, we define the \textbf{type theory of posets under \cG} to be the free type theory of posets together with a 0-ary rule
\begin{mathpar}
  \inferrule{ }{A\types B}
\end{mathpar}
for any relation $A\le B$ in \cG.
Now a derivation can be written without any ``leaves'' at the top, such as
\begin{mathpar}
  \inferrule*{\inferrule*{\inferrule*{ }{D\types A} \\ \inferrule*{ }{A\types B}}{D\types B} \\ \inferrule*{ }{B\types E}}{D\types E}
\end{mathpar}
Clearly this produces the same judgments; thus the initiality theorem can also be expressed as follows.

\begin{thm}\label{thm:poset-initial-2}
  For any relational graph \cG, the free poset $F_{\bPoset}\cG$ that it generates is has the same objects and its morphisms are the derivable judgments in the type theory of posets under \cG.
\end{thm}

We can extract from this our first general statement about categorical logic: it is \emph{a syntax for generating free categorical structures using derivations from rules}.
The reader may be forgiven at this time for wondering what the point is; but bear with us and things will get less trivial.


\section{Categories}
\label{sec:categories}

Let's now generalize from posets to categories.
The relevant adjunction is now between categories \bCat and \emph{directed graphs} \bGr; the latter are sets $\cG$ of ``vertices'' equipped with a set $\cG(A,B)$ of ``edges'' for each $x,y\in \cG$.
Thus, we hope to generate the free category $F_{\bCat}\cG$ on a directed graph \cG type-theoretically.

Our judgments $A\types B$ will still represent morphisms from $A$ to $B$, but now of course there can be more than one such morphism.
Thus, to specify a particular morphism, we need more information than the simple \emph{derivability} of a judgment $A\types B$.
Na\"ively, the first thing we might try is to identify this extra information with the \emph{derivation} of such a judgment, i.e.\ with the tree of rules that were applied to reach it.
This makes the most sense if we take the approach of \cref{thm:poset-initial-2} rather than \cref{thm:poset-initial-1}, so that distinct edges $f,g\in \cG(A,B)$ can be regarded as distinct \emph{rules}
\begin{mathpar}
  \inferrule*[right=$f$]{ }{A\types B} \and
  \inferrule*[right=$g$]{ }{A\types B} \and
\end{mathpar}
Thus, for instance, if we have also $h\in \cG(B,C)$, the distinct composites $h\circ g$ and $h\circ f$ will be represented by the distinct derivations
\begin{mathpar}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{B\types C} \\
    \inferrule*[right=$g$]{ }{A\types B}
  }{
    A\types C
  }\and
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{B\types C} \\
    \inferrule*[right=$f$]{ }{A\types B}
  }{
    A\types C
  }\and
\end{mathpar}
Note that when we have distinct rules with the same premises and conclusion, we have to label them so that we can tell which is being applied.
For consistency, we begin labeling the identity and composition rules too, with $\circ$ and $\idfunc$.

Of course, this na\"ive approach founders on the fact that composition in a category is supposed to be associative and unital, since the two composites $h\circ (g\circ f)$ and $(h\circ g)\circ f$, which ought to be equal, nevertheless correspond to distinct derivations:
\begin{equation}\label{eq:assoc}
  \begin{array}{c}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{C\types D} \\
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$g$]{ }{B\types C} \\
      \inferrule*[right=$f$]{ }{A\types B}
    }{
      A\types C
    }}{
    A\types D
  }\\\\
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$h$]{ }{C\types D} \\
      \inferrule*[right=$g$]{ }{B\types C}
    }{
      B\types D
    }\\
    \inferrule*[right=$f$]{ }{A\types B}
  }{
    A\types D
  }
  \end{array}
\end{equation}
Thus, with this type theory we don't get the free category on \cG, but rather some free category-like structure that lacks associativity and unitality.

There are two ways to deal with this problem.
The first is to simply quotient by an equivalence relation.
Our equivalence relation will have to identify the two derivations in~\eqref{eq:assoc}, and also the similar pairs for identities:
\begin{mathpar}
  \inferrule{\inferrule*[right=$\idfunc$]{ }{A\types A}\\ {A\types B}}{A\types B}\circ
  \qquad\equiv\qquad A\types B
  \\
  \inferrule{A\types B \\ \inferrule*[right=$\idfunc$]{ }{B\types B}}{A\types B}\circ
  \qquad\equiv\qquad A\types B
\end{mathpar}
Our equivalence relation must also be a ``congruence for the tree-construction of derivations'', meaning that these identifications can be made anywhere in the middle of a long derivation, such as:
\begin{mathpar}
  \inferrule{\inferrule*{}{\sD_1\\\\\vdots} \\
    \inferrule*[right=$\circ$]{\inferrule*[right=$\idfunc$]{ }{A\types A}\\ \inferrule*{\sD_2\\\\\vdots}{A\types B}}{A\types B}
  }{\vdots\\\\\sD_3}
  \qquad\equiv\qquad
  \inferrule{\inferrule*{}{\sD_1\\\\\vdots} \\
    \inferrule*{\sD_2\\\\\vdots}{A\types B}
  }{\vdots\\\\\sD_3}
\end{mathpar}
We will also have to close it up under reflexivity, symmetry, and transitivity to make an equivalence relation.

Of course, it quickly becomes tedious to draw such derivations, so it is convenient to adopt a more succinct syntax for them.
We begin by labeling each judgment with a one-dimensional syntactic representation of its derivation tree, such as:
\begin{mathpar}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$g$]{ }{g:(B\types C)} \\
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$\idfunc$]{ }{\idfunc_B:(B\types B)} \\
      \inferrule*[right=$f$]{ }{f:(A\types B)}
    }{
      (\idfunc_B\circ f):(A\types B)
    }}{
    (g\circ (\idfunc_B\circ f)) : (A\types C)
  }
\end{mathpar}
These labels are called \emph{terms}.
Of course, in this case they are none other than the usual notation for composition and identities.
Now the generators of our equivalence relation look even more familiar:
\begin{align*}
  \phi \circ (\psi \circ \chi) &\equiv (\phi\circ\psi)\circ\chi\\
  \phi \circ \idfunc_A &\equiv \phi\\
  \idfunc_B \circ \phi &\equiv \phi
\end{align*}
Here $\phi,\psi,\chi$ denote \emph{arbitrary terms}, corresponding to the fact that arbitrary derivations can appear at the top of our identified trees; and similarly these identifications can also happen anywhere inside another term, so that for instance
\[ k\circ (h\circ (g\circ f)) \equiv k\circ ((h\circ g)\circ f).  \]
Of course, we only impose these relations when they make sense.
We can describe the conditions under which this happens using rules for a secondary judgment $\phi\equiv \psi : (A\types B)$, like so:
\begin{mathpar}
  \inferrule{\phi:(A\types B) \\ \psi:(B\types C) \\ \chi:(C\types D)}{(\phi \circ (\psi \circ \chi) \equiv (\phi\circ\psi)\circ\chi) : (A\types D)}\\
  \inferrule{\phi:(A\types B)}{(\phi \circ \idfunc_A \equiv \phi):(A\types B)}\and
  \inferrule{\phi:(A\types B)}{(\idfunc_B \circ \phi \equiv \phi):(A\types B)}
\end{mathpar}
When the rules for $\circ$ and $\idfunc$ are augmented by these rules generating a congruence $\equiv$, and we add axioms for the edges of a given directed graph \cG, we call the result the \textbf{cut-ful type theory of categories under \cG}.
It should hopefully be clear that this does work, so that we have the initiality theorem.

\begin{thm}\label{thm:category-initial-1}
  The free category on a directed graph $\cG$ has the same objects as \cG, and its morphisms are the terms such that $\phi :(A\types B)$ is derivable in the cut-ful type theory of categories under \cG, modulo the equivalence relation $\phi\equiv \psi:(A\types B)$.\qed
\end{thm}

Of course, once again very little seems to be happening; we are just using a complicated funny syntax to build a free algebraic structure.
Therefore, it is the second way to deal with the problem of associativity that is more interesting.
In this case what we do is \emph{remove the composition rule $\circ$ entirely}; instead we ``build (post)composition into the axioms''.
That is, the only rule independent of \cG is identities:
\[ \inferrule{ }{A\types A}\,\idfunc \]
while for every edge $f\in \cG(A,B)$ we take the following rule:
\[ \inferrule{X\types A}{X\types B} \,f \]
for any $X$.
Informally, one might say that we represent $f$ by its ``image under the Yoneda embedding''.

Note that we have made a choice to build in \emph{postcomposition}; we could also have chosen to \emph{precomposition}.
In the current context, either choice would work just as well; but later on we will see that there were reasons to choose postcomposition here.
We will call this the \textbf{cut-free type theory of categories under \cG}.

In this theory, if we have $f\in\cG(A,B)$, $g\in\cG(B,C)$, and $h\in \cG(C,D)$ there is \emph{only one way} to derive $A\types D$:
\begin{mathpar}
  \inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{A\types A}
      }{
        A\types B
      }
    }{
      A\types C
    }
  }{
    A\types D
  }
\end{mathpar}
Thus, we no longer have to worry about distinguishing between $h\circ (g\circ f)$ and $(h\circ g)\circ f$.
Of course, we have a new problem: if we are trying to build a category, then we \emph{do} need to be able to compose arrows!
So we need the following theorem:

\begin{thm}\label{thm:cutadm-categories}
  If we have derivations of $A\types B$ and $B\types C$ in the cut-free type theory of categories under \cG, then we can construct a derivation of $A\types C$.
\end{thm}
\begin{proof}
  We induct on the number of rules appearing in the given derivation of $B\types C$.

  If there is only one rule, then that rule must be $\idfunc$, since all the other rules have a premise and would need a second rule to feed into them.
  In this case, it be that $B=C$; so our given derivation of $A\types B$ is also a derivation of $A\types C$.

  If there are $n+1$ rules, then we must have $f\in\cG(D,C)$ and our derivation of $B\types C$ ends like this:
  \begin{mathpar}
    \inferrule*[right=$f$]{\inferrule*{\sD\\\\\vdots}{B\types D}}{B\types C}
  \end{mathpar}
  In particular, it contains a derivation \sD of $B\types D$ with only $n$ rules.
  Thus, by the inductive hypothesis we have a derivation, say $\sD'$, of $A\types D$.
  Now we can simply follow this with the rule for $f$:
  \begin{mathpar}
    \inferrule*[right=$f$]{\inferrule*{\sD'\\\\\vdots}{A\types D}}{A\types C}
  \end{mathpar}
\end{proof}

In type-theoretic lingo, \cref{thm:cutadm-categories} says that \textbf{the cut rule is admissible} in the cut-free type theory of categories under \cG.
In other words, although the cut/composition rule
\begin{mathpar}
  \inferrule*[right=$\circ$]{A\types B \\ B\types C}{A\types C}
\end{mathpar}
is not \emph{part of the type theory} as defined, it is nevertheless true that whenever we have derivations of the premises of this rule, we can construct a derivation of its conclusion.

Another way to say what is going on is that the morphisms in the free category on a directed graph \cG have an explicit description as \emph{finite strings of composable edges} in \cG.
We have just given an inductive definition of ``finite string of composable edges'': there is a finite string (of length 0) from $A$ to $A$; and if we have such a string from $X$ to $A$ and an edge $f\in\cG(A,B)$, we can construct a string from $X$ to $B$.
Thus we have the analogous initiality theorem:

\begin{thm}\label{thm:category-initial-2}
  The free category on a directed graph $\cG$ has the same objects as \cG, and its morphisms are the derivations of $A\types B$ in the cut-free type theory for categories under \cG.\qed
\end{thm}

This gives us a second, more interesting, general statement about categorical logic: it is
{a syntax for generating free categorical structures using derivations from rules}
\emph{that yield elements in canonical form}, eliminating (or reducing) the need for quotients.

To end this section, we note that we can introduce \emph{terms} for the cut-free type theory as well in a pleasing way.
Rather than describing the entire judgment $A\types B$ with a term, we assign a formal variable such as $x$ to the domain $A$, and then a term containing $x$ to the codomain $B$, such as $h(g(f(x)))$.
We write this as
\[ x:A \types h(g(f(x))) : B\]
The identity rules and axiom rules can now be written as
\begin{mathpar}
  \inferrule{ }{x:A\types x:A}\,\idfunc \and
  \inferrule{x:X\types \alpha:A}{x:X\types f(\alpha):B} \,f
\end{mathpar}
Here $\alpha$ denotes an arbitrary term.
Thus, for instance, the composite of $h$, $g$, and $f$ would be written like so:
\begin{mathpar}
  \inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{x:A\types x:A}
      }{
        x:A\types f(x):B
      }
    }{
      x:A\types g(f(x)): C
    }
  }{
    x:A\types h(g(f(x))):D
  }
\end{mathpar}
If we trace through the proof of \cref{thm:cutadm-categories} in this notation, it says that if we have $x:A\types \beta:B$ and $y:B\types \gamma:C$, we can \emph{substitute} $\beta$ for $y$ in $\gamma$ to obtain a new judgment $x:A \types \gamma[\beta/y]:C$.
For example, given $x:A \types g(f(x)):B$ and $y:B\types h(y):C$, the derivation constructed is $x:A\types h(g(f(x))):C$, in which the $y$ in $h(y)$ has been replaced by $g(f(x))$.
Thus, \cref{thm:cutadm-categories} can be rephrased to say that \textbf{substitution is admissible} in the cut-free type theory of categories under \cG.

Note, by the way, that this perspective gives one reason why we chose to build in postcomposition rather than precomposition.
If we used precomposition instead, then we could still develop analogous notions of term and substitution, but they would be backwards: we would have to represent $f:A\to B$ as $f(u):A \types u:B$ rather than $x:A \types f(x):B$.
Of course, we are much more familiar with applying functions to variables than co-applying functions to co-variables.

\subsection*{Exercises}

\begin{ex}
  Prove \cref{thm:category-initial-2}.
  You will have to start by proving that the composition defined in \cref{thm:cutadm-categories} is in fact associative and unital, so that it forms a category.
\end{ex}

\begin{ex}\label{ex:categories-over}
  Let \sM be a fixed category; then we have an induced adjunction between $\bCat/\sM$ and $\bGr/\sM$.
  Describe a cut-free type theory for presenting the free category-over-\sM on a directed-graph-over-\sM, and prove the initiality theorem (the analogue of \cref{thm:category-initial-2}).
  Note that you will have to prove that cut is admissible first.
  \textit{(Hint: index the judgments by arrows in \sM, so that for instance $A\types_\alpha B$ represents an arrow lying over a given arrow $\alpha$ in $\sM$.)}
\end{ex}


\section{Meet-semilattices}
\label{sec:mslat}

Moving gradually up the ladder of nontriviality, we now consider categories with finite products, or more precisely binary products and a terminal object.
In fact, let us revert back to the posetal world first and consider posets with binary meets and a top element, i.e.\ meet-semilattices.
We will make all this structure algebraic, so that our meet-semilattices are posets (which, recall, is not necessarily skeletal) \emph{equipped with} a chosen top element and an operation assigning to each pair of objects a meet.
We then have an adjunction relating the category \bmSLat of suchmeet-semilattices (and morphisms preserving all the structure strictly) with the category \bRelGr of relational graphs, and we want to describe the free meet-semilattice on a relational graph \cG.

One new feature this introduces is that the objects of $F_\bmSLat \cG$ will no longer be the same as those of \cG: we need to add a top element and freely apply the meet operation.
In order to describe this type-theoretically, we introduce a new judgment ``$\types X\type$'', meaning that $X$ will be one of the objects of the poset we are generating.
The rules for this judgment are
\begin{mathpar}
  \inferrule{ }{\types \top\type} \and
  \inferrule{\types X\type \\ \types Y\type}{\types X\meet Y\type}
\end{mathpar}
When talking about type theory under \cG, we additionally include ``axiom'' rules saying that each object of \cG is a type:
\begin{mathpar}
  \inferrule{A\in\cG}{\types A\type}
\end{mathpar}
Thus, for instance, if $A,B\in\cG$ we have a deduction
\begin{mathpar}
  \inferrule*{
    \inferrule*{A\in\cG}{\types A\type}\\
    \inferrule*{\inferrule*{ }{\types \top\type} \\ \inferrule*{B\in\cG}{\types B\type}}{\types \top\meet B\type}
  }{
    \types (A\meet (\top\meet B))\type
  }
\end{mathpar}
so that $A\meet (\top\meet B)$ will be one of the objects of $F_\bmSLat\cG$.

Now we need to describe its morphisms, i.e.\ the relation $\le$ in $F_\bmSLat\cG$.
The obvious thing to do is to assert the universal property of the meet and the top element:
\begin{mathpar}
  \inferrule{ }{A\types \top}\and
  \inferrule{ }{A\meet B \types A}\and
  \inferrule{ }{A\meet B \types B}\and
  \inferrule{A\types B \\ A\types C}{A\types B\meet C}
\end{mathpar}
This works, but it forces us to go back to asserting transitivity/cut.
For instance, if $A,B,C\in \cG$ we have the following derivation:
\begin{mathpar}
  \inferrule*{
    \inferrule*{ }{(A\meet B)\meet C \types A\meet B}\\
    \inferrule*{ }{A\meet B \types A}
  }{
    (A\meet B)\meet C \types A
  }
\end{mathpar}
but there is no way to deduce this without using the cut rule.
Thus, this \textbf{cut-ful type theory for meet-semilattices under \cG} works, but to have a better class of ``canonical forms'' for its relations we would also like a cut-free version.

What we need to do is to treat the ``projections'' $A\meet B \to A$ and $A\meet B\to B$ similarly to how we treated the edges of \cG in \cref{sec:categories}.
However, at this point we have to make a choice of whether to build in postcomposition or precomposition:
\[
\inferrule{A\types C}{A\meet B \types C} \qquad\text{or}\qquad
\inferrule{C\types A\meet B}{C\types A} \quad ?
\]
Both choices work (that is, they make cut admissible), and lead to different kinds of type theories with different properties.
The first leads to a kind of type theory called \textbf{sequent calculus}, and the second to a kind of type theory called \textbf{natural deduction}.
We consider each in turn.

% TODO: Add something to the names of these theories to indicate that they have one-variable contexts?

\subsection{Sequent calculus for meet-semilattices}
\label{sec:seqcalc-mslat}

To be precise, for a relational graph \cG, the \textbf{unary sequent calculus for meet-semilattices under \cG} has the following rules (in addition to the rules for the judgment $\types A\type$ mentioned above).
We label each rule on the right to make them easier to refer to later on.
\begin{mathpar}
  \inferrule{A\in \cG}{A\types A}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ X\types A}{X\types B}\;f\and
  \inferrule{\types A\type}{A\types \top}\;\top R\and
  \inferrule{A\types C \\ \types B\type}{A\meet B \types C}\;\meetL1\and
  \inferrule{B\types C \\ \types A\type}{A\meet B \types C}\;\meetL2\and
  \inferrule{A\types B \\ A\types C}{A\types B\meet C}\;\meetR
\end{mathpar}
There are several things to note about this.
The first is that we have included in the premises some judgments of the form $\types A\type$.
This ensures that whenever we can derive a sequent $A\types B$, that $A$ and $B$ are well-formed as types.
However, we don't need to assume explicitly as premises that \emph{all} types appearing in any sequent are well-formed, only those that are introduced without belonging to any previous sequents; this is sufficient for the following inductive proof.

\begin{thm}\label{sec:seqcalc-mslat-wftype}
  In the unary sequent calculus for meet-semilattices under \cG, if $A\types B$ is derivable, then so are $\types A\type$ and $\types B\type$.
\end{thm}
\begin{proof}
  By induction on the derivation of $A\types B$.
  \begin{itemize}
  \item If it is the $\idfunc$ rule, then $A\in\cG$ and so $\types A\type$.
  \item If it ends with the rule $f$ for some $f\in\cG(A,B)$, then $B\in \cG$ and so $\types A\type$, while $X\types A$ and so $\types X\type$ by the inductive hypothesis.
  \item If it ends with the rule $\top R$, then $\types A\type$ by assumption.  
  \item If it ends with the rule $\meetL1$, then $\types B\type$ by assumption, while $\types A\type$ and $\types C\type$ by the inductive hypothesis; thus also $\types A\meet B\type$.
  \item The cases for $\meetL2$ and $\meetR$ are similar.\qedhere
  \end{itemize}
\end{proof}

The second thing to note is that we only assert the identity rule $A\types A$ when $A$ is a \emph{generating object} (also called a \emph{base type}), i.e.\ an object of \cG.
This is sufficient because in the sequent calculus, we can derive the identity rule for any type:

\begin{thm}\label{thm:seqcalc-mslat-idadm}
  In the unary sequent calculus for meet-semilattices under \cG, if $A$ is a type (that is, if $\types A\type$ is derivable), then $A\types A$ is derivable.
\end{thm}
\begin{proof}
  We induct on the derivation of $\types A\type$.
  There are three cases:
  \begin{enumerate}
  \item $A$ is in \cG.  In this case $A\types A$ is an axiom.
  \item $A=\top$.  In this case $\top\types\top$ is a special case of the rule that anything $\types\top$.
  \item $A=B\meet C$ and we have derivations $\sD_B$ and $\sD_C$ of $\types B \type$ and $\types C\type$ respectively.
    Therefore we have, inductively, derivations $\sD_1$ and $\sD_2$ of $B\types B$ and $C\types C$, and we can put them together like this:
    \begin{equation*}
      \inferrule*{
        \inferrule*{
          \inferrule*{\sD_1\\\\\vdots}{B\types B} \\
          \inferrule*{\sD_C\\\\\vdots}{\types C\type}
        }{
          B\meet C \types B
        }\\
        \inferrule*{
          \inferrule*{\sD_2\\\\\vdots}{C\types C}\\
          \inferrule*{\sD_B\\\\\vdots}{\types B\type}
        }{
          B\meet C\types C
        }
      }{
        B\meet C\types B\meet C
      }\qedhere
    \end{equation*}
  \end{enumerate}
\end{proof}

In other words, the general identity rule
\[ \inferrule{\types A\type}{A\types A} \]
is also \emph{admissible}.
This is a general characteristic of sequent calculi.

Next we prove that the cut rule is admissible for this sequent calculus too.

\begin{thm}\label{thm:seqcalc-mslat-cutadm}
  In the unary sequent calculus for meet-semilattices under \cG, if $A\types B$ and $B\types C$ are derivable, then so is $A\types C$.
\end{thm}
\begin{proof}
  By induction on the derivation of $B\types C$.
  \begin{enumerate}
  \item If it is $\idfunc$, then $B=C$.
    Now $A\types C$ is just $A\types B$ and we are done.
  \item If it is $f\in\cG(C',C)$, then we have a derivation of $B\types C'$.
    So by the inductive hypothesis we can derive $A\types C'$, whence also $A\types C$ by the rule for $f$.
  \item If it ends with $\top R$, then $C=\top$.
    Since $A\types B$ is derivable, by \cref{sec:seqcalc-mslat-wftype} $\types A\type$ is also derivable; thus by $\top R$ we have $A\types \top$.
  \item If it ends with $\meetR$, then $C=C_1\meet C_2$ and we have derivations of $B\types C_1$ and $B\types C_2$.
    By the inductive hypothesis we can derive both $A\types C_1$ and $A\types C_2$, to which we can apply $\meetR$ to get $A\types C_1\meet C_2$.
  \item If it ends with $\meetL1$, then $B=B_1\meet B_2$ and we can derive $B_1\types C$.
    We now do a secondary induction on the derivation of $A\types B$.
    \begin{enumerate}
    \item It cannot end with $\idfunc$ or $f$ or $\top R$, since $B=B_1\meet B_2$ is not in $\cG$ and not equal to $\top$.
    \item If it ends with $\meetL1$, then $A=A_1\meet A_2$ and we can derive $A_1\types B$.
      By the inductive hypothesis, we can derive $A_1 \types C$, and hence by $\meetL1$ also $A \types C$.
      The case of $\meetL2$ is similar.
    \item Finally, if it ends with $\meetR$, then we can derive $A\types B_1$ and $A\types B_2$.
      Recall that we are also assuming a derivation of $B_1\types C$.
      Thus, by the inductive hypothesis on $A\types B_1$ and $B_1\types C$, we can derive $A\types C$.
      \label{item:mslat-principal-cut}\qedhere
    \end{enumerate}
  \end{enumerate}
\end{proof}

This simple proof already displays many of the characteristic features of a cut-admissibility argument.
The final case~\ref{item:mslat-principal-cut} is called the \textbf{principal case} for the operation $\meet$, when the type $B$ we are composing over (also called the \textbf{cut formula}) is obtained from $\meet$ and both sequents are also obtained from the $\meet$ rules.

Finally, we have the initiality theorem:

\begin{thm}\label{thm:seqcalc-mslat-initial}
  For any relational graph $\cG$, the free meet-semilattice $F_\bmSLat \cG$ it generates is described by the unary sequent calculus for meet-semilattices under \cG: its objects are the $A$ such that $\types A\type$ is derivable, with $A\le B$ just when $A\types B$ is derivable.
\end{thm}
\begin{proof}
  \cref{thm:seqcalc-mslat-idadm,thm:seqcalc-mslat-cutadm} show that this defines a poset; let us denote it $F\cG$.
  The rule $\top R$ implies that $\top$ is a top element, while the rules $\meetL1$, $\meetL2$, and $\meetR$ imply that $A\meet B$ is a binary meet.
  Therefore, we have a meet-semilattice.
  Moreover, the rules $\idfunc$ and $f$ yield a map of posets $\cG\to F\cG$.

  Now suppose $\cM$ is any other meet-semilattice with a map $P:\cG\to\cM$.
  Recall that a meet-semilattices is equipped with a chosen top element and meet function.
  We extend $P$ to a map from the objects of $F\cG$ by recursion on the construction of the latter, sending $\top$ to the chosen top element of \cM, and $A\meet B$ to the chosen meet in \cM of the (recursively defined) images of $A$ and $B$.
  This is clearly the only possible meet-semilattice map extending $P$, and it clearly preserves the chosen meets and top element, so it suffices to check that it is a poset map.
  This follows by a straightforward induction over the rules for deriving the judgment $A\types B$.
\end{proof}

To finish, we observe that this sequent calculus has another important property.
Inspecting the rules, we see that the operations $\meet$ and $\top$ only ever appear in the \emph{conclusions} of rules.
Each operation $\meet$ and $\top$ has zero or more rules allowing us to introduce it on the right of the conclusion, and likewise zero or more rules allowing us to introduce it on the left.
(Specifically, $\meet$ has two left rules and one right rule, while $\top$ has zero left rules and one right rule.)
This is convenient if we are given a sequent $A\types B$ and want to figure out whether it is derivable: we can choose rules to apply ``in reverse'' by breaking down $A$ and $B$ according to their construction out of $\meet$ and $\top$.

The phrase \emph{sequent calculus}, like \emph{type theory}, is difficult to define precisely, but sequent calculi generally exhibit the properties we have observed in this subsection: admissibility of the identity rule (based on an axiom applying only to ground types), admissibility of cut, and type operations appearing only in the conclusions of rules.

\subsection{Natural deduction for meet-semilattices}
\label{sec:natded-mslat}

Now suppose we make the other choice about how to treat projections.
We call this the \textbf{unary natural deduction for meet-semilattices under \cG}; its rules (in addition to those for $\types A\type$) are
\begin{mathpar}
  \inferrule{\types X\type}{X\types X}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ X\types A}{X\types B}\;f\and
  \inferrule{\types X\type}{X\types \top}\;\top I\and
  \inferrule{X\types B\meet C}{X \types B}\;\meetE1\and
  \inferrule{X\types B\meet C}{X \types C}\;\meetE2\and
  \inferrule{X\types B \\ X\types C}{X\types B\meet C}\;\meetI
\end{mathpar}

We observe first that this theory has the same well-formedness property as the sequent calculus:

\begin{thm}\label{sec:natded-mslat-wftype}
  In the unary natural deduction for meet-semilattices under \cG, if $A\types B$ is derivable, then so are $\types A\type$ and $\types B\type$.\qed
\end{thm}

Unlike the sequent calculus, however, the general identity rule is not admissible: there is no way to derive $A\meet B \types A\meet B$ from $A\types A$ and $B\types B$.
Thus, we assert that the $\idfunc$ rule applies to all types, not just those coming from \cG.

Cut, however, is still admissible:

\begin{thm}\label{thm:natded-mslat-cutadm}
  In the unary natural deduction for meet-semilattices under \cG, if $A\types B$ and $B\types C$ are derivable, then so is $A\types C$.
\end{thm}
\begin{proof}
  We induct on the derivation of $B\types C$.
  \begin{enumerate}
  \item The cases when it ends with $\idfunc$, $f$, $\top I$, and $\meetI$ are just like those in \cref{thm:seqcalc-mslat-cutadm} for $\idfunc$, $f$, $\top R$, and $\meetR$.
  \item If it ends with $\meetE1$, then we have $B\types C\meet D$ for some $D$.
    Thus, $A\types C\meet D$ by the inductive hypothesis, so $A\types C$ by $\meetE1$.
    The case of $\meetE2$ is similar.\qedhere
  \end{enumerate}
\end{proof}

The proof is noticeably simpler than that of \cref{thm:seqcalc-mslat-cutadm}; we don't need the secondary inner induction.
This is essentially due to the fact that all the rules of this theory involve an \emph{arbitrary} type $X$ on the left.
This is characteristic of \emph{natural deduction} theories.
Accordingly, instead of the rules of sequent calculus that introduce operations like $\meet$ and $\top$ on the left and right, we have rules like $\top I$ and $\meetI$ that introduce them on the right, and also rules that \emph{eliminate} them on the right like $\meetE1$ and $\meetE2$.
(Later on we will be able to give a more convincing explanation of the origin of the phrase ``natural deduction''.)

Of course, we should also prove the initiality theorem:

\begin{thm}\label{thm:natded-mslat-initial}
  For any relational graph $\cG$, the free meet-semilattice $F_\bmSLat \cG$ it generates is described by the unary natural deduction for meet-semilattices under \cG: its objects are the $A$ such that $\types A\type$ is derivable, with $A\le B$ just when $A\types B$ is derivable.
\end{thm}
\begin{proof}
  Almost exactly like \cref{thm:seqcalc-mslat-initial}.
\end{proof}

\subsection*{Exercises}

\begin{ex}\label{ex:mslat-invertible}
  Prove that the rules $\bot R$, $\meetL1$, $\meetL2$, and $\meetR$ in the unary sequent calculus for meet-semilattices are \emph{invertible}, in the sense that whenever we have a derivation of their conclusions, we also have a derivation of all their premises.
\end{ex}

\begin{ex}\label{ex:jslat}
  Describe a sequent calculus for \emph{join-semilattices} (posets with a bottom element and binary joins), and prove the initiality theorems for it (including the identity and cut admissibility theorems).
  The rules for $\bot$ and $\join$ should be exactly dual to the rules for $\top$ and $\meet$.
\end{ex}

\begin{ex}\label{ex:lattices}
  By putting together the rules for meet- and join-semilattices, describe a sequent calculus for \emph{lattices} (posets with a top and bottom element and binary meets and joins), and prove the initiality theorem.
\end{ex}

\begin{ex}\label{ex:lattices-invertible}
  Prove that in your sequent calculus for lattices from \cref{ex:lattices}, the rules for $\bot,\top,\meet,\join$ are all invertible in the sense of \cref{ex:mslat-invertible}.
\end{ex}

\begin{ex}\label{ex:seqcalc-poset-fib}
  A map of posets $P:\sA\to\sM$ is called a \emph{(cloven) fibration} if whenever $b\in\sA$ and $x\le P(b)$, there is a chosen $a\in \sA$ such that $P(a)=x$ and $a\le b$ and moreover for any $c\in\sA$, $c\le b$ and $P(c)\le x$ together imply $c\le a$.
  The object $a$ can be written as $x^*(b)$.
  Given a fixed poset \sM, describe a sequent calculus for fibrations over \sM by adding rules governing the operations $x^*$ to the cut-free theory of \cref{ex:categories-over}, and prove the initiality theorem.
\end{ex}

\begin{ex}\label{ex:natded-poset-fib}
  Now describe instead a natural deduction for fibrations over \sM, and prove the initiality theorem.
\end{ex}

\begin{ex}\label{ex:poset-bifib}
  A map of posets $P:\sA\to\sM$ is called an \emph{opfibration} if $P\op:\sA\op\to\sM\op$ is a fibration.
  The analogous operation takes $a\in \sA$ and $P(a)\le y$ to a $b\in \sA$ with $P(b)=y$ and $a\le b$ and a universal property; we write this $b$ as $y_!(a)$.
  We say $P$ is a \emph{bifibration} if it is both a fibration and an opfibration.
  Describe a sequent calculus for bifibrations over a fixed \sM, and prove the initiality theorem.
\end{ex}

\begin{ex}\label{ex:poset-bifib-adj}
  Use your sequent calculus from \cref{ex:poset-bifib} to prove that in a bifibration, if $x\le y$ in $\sM$, we have an adjunction $y_! \dashv x^*$.
\end{ex}


\section{Categories with products}
\label{sec:cat-prod}


% This is an instance of the \emph{modularity} of type theory: generally speaking, the rules for different categorical operations such as $\meet$ and $\join$ should be \emph{unrelated} to each other, so that each can be analyzed separately and included or excluded.


\bibliographystyle{alpha}
\bibliography{all}

\end{document}
