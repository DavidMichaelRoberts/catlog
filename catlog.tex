\documentclass{book}
\usepackage{mathpartir,cancel,cmll,textcomp,environ}
%\usepackage{appendix}  % for chapter appendices, use \begin{subappendices}
\newif\ifcref\creftrue
\input{decls}
\title{Categorical logic from a categorical point of view}
\author{Michael Shulman}
\date{\today}
\makeatletter
\autodefs{\bSet\bMag\bVect\bPoset\bRelGr\bCat\bGr\bmSLat\ftype\bPrCat\bCoprCat\bMCat\bRepMCat\bSymMulti\bMGr\bMPos\bRelMGr\bMonPos\bMonCat\bSMPos\bSMC\bProp\bTV\bHeyting\fapp\bTh}
\autodefs{\nAut\nShuf}
\def\tv{\mathbf{2}}
\def\tr{\mathrm{tr}}
\let\sect\section
\def\idfunc{\mathsf{id}}
\def\finsubset{\subset_{\mathrm{fin}}}
% coalgebras
\let\comult\triangle
\let\counit\varepsilon
% well-founded trees and algebras
\def\ay{\mathrm{ar}}
\def\sig{\Sigma}
\def\axes{\Lambda}
\def\act#1{[#1]}
\def\equivsym{\mathord{\equiv}}
% derivations
\def\derivof#1{\inferrule*{\vdots}{#1}}
\def\labderivof#1#2{\inferrule*{#1\\\\\vdots}{#2}}
% contexts
\let\merge\boxplus
% judgments
\let\types\vdash
\def\type{\;\ftype}
\newcommand{\pc}{\mathrel{\mathord{:}?}}
\newcommand{\atom}{\mathrel{\downarrow}}
\newcommand{\can}{\mathrel{\uparrow}}
\newcommand{\atomcan}{\ensuremath{\mathord{\downarrow\uparrow}}}
\newcommand{\focus}[1]{[#1]}
\newcommand{\postcsym}[1]{#1\mathord{\circ}}
\newcommand{\postc}[2]{\postcsym{#1}(#2)}
\newcommand{\precsym}[1]{\mathord{\circ}#1}
\let\preceeds\prec
\renewcommand{\prec}[2]{(#1)\precsym{#2}}
\newcommand{\actv}[1]{\underline{#1}}
% substitution
\newcommand{\alsub}[1]{\langle #1\rangle}
\newcommand{\hsub}[1]{\llbracket #1\rrbracket}
\newcommand{\hid}[1]{\mathbbb{id}_{#1}}
% free functors
\newcommand{\F}[1]{\mathfrak{F}_{#1}}
% composition
\def\comp#1{\circ_{#1}}
% meet
\let\meet\wedge
\def\meetL{\mathord{\meet}L}
\def\meetR{\mathord{\meet}R}
\def\meetE{\mathord{\meet}E}
\def\meetI{\mathord{\meet}I}
% join
\let\join\vee
\let\bigjoin\bigvee
\def\joinL{\mathord{\join}L}
\def\joinR{\mathord{\join}R}
\def\joinE{\mathord{\join}E}
\def\joinI{\mathord{\join}I}
% unit
\def\unit{\mathbbb{1}}%{\top}%{\mathbf{1}}
\def\ttt{\mathord{\ast}}%{\mathsf{tt}}
% product
\def\timesE{\ensuremath{\mathord{\times}E}}
\def\timesI{\ensuremath{\mathord{\times}I}}
\def\timesR{\ensuremath{\mathord{\times}R}}
\def\timesL{\ensuremath{\mathord{\times}L}}
\def\pair#1#2{\langle #1,#2\rangle}
\def\pr#1#2#3{\pi_{#1}^{#2,#3}}
% coproduct
\def\plusE{\mathord{+}E}
\def\plusI{\mathord{+}I}
\def\plusL{\mathord{+}L}
\def\plusR{\mathord{+}R}
\def\inl{\mathsf{inl}}
\def\inr{\mathsf{inr}}
\def\acase#1#2{\mathsf{match}_{#1+#2}}
\def\case{\mathsf{match}_+}
% general positives
\def\match{\mathsf{match}}
% empty
\def\zero{\mathbf{0}}
\def\zeroE{\mathbf{0}E}
%\def\abort{\mathsf{abort}}
\def\abort{\match_{\zero}}
% one
\def\one{\mathbf{1}}
\def\ott{\mathord{\star}}%{\mathsf{tt}}
%\def\discard#1in{\mathsf{discard}\; #1 \; \mathsf{in} \;}
% tensor
\let\tensor\otimes
\def\tensorL{\mathord{\tensor}L}
\def\tensorR{\mathord{\tensor}R}
\def\tensorI{\mathord{\tensor}I}
\def\tensorE{\mathord{\tensor}E}
% \def\flet#1:{\mathsf{let}\;#1 \@ifnextchar:\@fletdoublecolon\@fletsinglecolon}
% \def\fletp#1:{\mathsf{let}'\;#1 \@ifnextchar:\@fletdoublecolon\@fletsinglecolon}
% \def\@fletdoublecolon:=#1in{\Coloneqq #1\;\mathsf{in}\;}
% \def\@fletsinglecolon=#1in{\coloneqq #1\;\mathsf{in}\;}
\def\tpair#1#2{\text{\textlquill} #1,#2 \text{\textrquill}}%{\llparenthesis #1,#2\rrparenthesis}
\let\bigtensor\bigotimes
% hom
\let\map\hom
\let\hom\multimap
\def\homL{\mathord{\hom}L}
\def\homR{\mathord{\hom}R}
\def\homI{\mathord{\hom}I}
\def\homE{\mathord{\hom}E}
\let\To\Rightarrow
\def\ToL{\mathord{\To}L}
\def\ToR{\mathord{\To}R}
\def\ToI{\mathord{\To}I}
\def\ToE{\mathord{\To}E}
\def\toL{\mathord{\to}L}
\def\toR{\mathord{\to}R}
\def\toI{\mathord{\to}I}
\def\toE{\mathord{\to}E}
% Theories
\def\timesthy#1{\bTh_{\times,#1}}
\def\m#1{\llbracket #1\rrbracket}
% 
% Collect exercises at the end of each chapter.  The upshot of this
% hackery is that we can write \begin{ex}\label{ex:name}...\end{ex} as
% usual in the text, and then at the end of each chapter
% write \ChapterExercises to get a re-stated list of all exercises
% from that chapter (or, more precisely, since the last
% time \ChapterExercises was run) with the same numbers.
%
% NB: Every \begin{ex} MUST be followed immediately by a \label!
\newtheorem*{rep@theorem}{\rep@title}
\def\repex#1\endrepexname{%
  \def\rep@title{\bf Exercise \ref{#1}}
  \begin{rep@theorem} }
\def\endrepex{\end{rep@theorem}}
\def\exaccum{}
\def\exname{}
\def\get@lab#1\label{\get@@lab}
\def\get@@lab#1{\gdef\exname{#1}\eat@body}
\long\def\eat@body#1\end@eat{}
\NewEnviron{ex}{%
  \begin{@ex}\BODY\end{@ex}%
  \ea\get@lab\BODY\end@eat
  \g@addto@macro\exaccum{\repex}
  \ea\g@addto@macro\ea\exaccum\ea{\exname}
  \g@addto@macro\exaccum{\endrepexname}
  \ea\g@addto@macro\ea\exaccum\ea{\BODY}
  \g@addto@macro\exaccum{\endrepex}
}{}
\def\ChapterExercises{\section*{Collected Exercises}
  For convenient reference, we collect the exercises from all sections in this chapter.
  \bgroup\def\label##1{}\exaccum\egroup%
  \def\exaccum{}}
%
\makeatother
\begin{document}
\maketitle
\setcounter{tocdepth}{2}
\tableofcontents

\setcounter{chapter}{-1}
\chapter{Introduction}
\label{chap:intro}

% [TODO: Introduce the word "connective" somewhere; maybe only for posets?]

% [TODO: Mention how the syntactic/free object gives "completeness" of a theory.]

\section{Appetizer: inverses in group objects}
\label{sec:intro}

In this section we consider an extended example.
We do not expect the reader to understand it very deeply, but we hope it will give some motivation for what follows, as well as a taste of the power and flexibility of categorical logic as a tool for category theory.

Our example will consist of several varations on the following theorem:

\begin{thm}
  If a monoid has inverses (hence is a group), then those inverses are unique.
\end{thm}

When ``monoid'' and ``group'' have their usual meaning, namely sets equipped with structure, the proof is easy.
For any $x$, if $i(x)$ and $j(x)$ are both two-sided inverse of $x$, then we have
\[ i(x) = i(x) \cdot e = i(x) \cdot (x \cdot j(x)) = (i(x)\cdot x)\cdot j(x) = e\cdot j(x) = j(x) \]
However, the theorem is true much more generally than this.
We consider first the case of monoid/group objects in a category with products.
A \emph{monoid object} is an object $M$ together with maps $m:M\times M \to M$ and $e:1\to M$ satisfying associativity and unitality axioms:
\begin{equation}
  \vcenter{\xymatrix{
      M\times M\times M\ar[r]^-{1\times m}\ar[d]_{m\times 1} &
      M\times M\ar[d]^m\\
      M\times M\ar[r]_m &
      M
    }}
  \qquad
  \vcenter{\xymatrix{ M \ar[r]^-{(1,e)} \ar[dr]_{1} &
    M\times M \ar[d]_m & M \ar[l]_-{(e,1)} \ar[dl]^{1} \\
    & M }}
\end{equation}
An \emph{inverse operator} for a monoid object is a map $i:M\to M$ such that
\begin{equation}
  \vcenter{\xymatrix@C=1pc{& M\times M \ar[rr]^{i\times 1} && M\times M \ar[dr]^m \\
      M \ar[ur]^{\Delta} \ar[dr]_{\Delta} \ar[rr]^{!} && 1 \ar[rr]^{e} && M \\
      & M\times M \ar[rr]_{1\times i} && M\times M \ar[ur]_{m}}}
\end{equation}
The internalized claim, then, is that any two inverse operators are equal.
The \emph{internal logic of a category with products} allows us to prove this by using essentially the same argument that we did in the case of ordinary monoids in \bSet.
The morphisms $m$ and $e$ are represented in this logic by the notations
\begin{mathpar}
  x:M,y:M \types x\cdot y :M \and
  \types e:M.
\end{mathpar}
Don't worry if this notation doesn't make a whole lot of sense yet.
The symbol $\types$ (called a ``turnstile'') is the logic version of a morphism arrow $\to$, and the entire notation is called a \emph{sequent} or a \emph{judgment}.
The fact that $m$ is a morphism $M\times M \to M$ is indicated by the fact that $M$ appears twice to the left of $\types$ and once to the right; the comma ``$,$'' in between $x:M$ and $y:M$ represents the product $\times$, and the variables $x,y$ are there so that we have a good notation ``$x\cdot y$'' for the morphism $m$.
In particular, the notation $x:M,y:M \types x\cdot y :M$ should be bracketed as
\[ ((x:M),(y:M)) \types ((x\cdot y) :M). \]
Similarly, the associativity, unit, and inverse axioms are indicated by the notations
\begin{mathpar}
  x:M,y:M,z:M \types (x\cdot y)\cdot z = x\cdot (y\cdot z) : M \\
  x:M \types x\cdot e = x : M \and
  x:M \types e\cdot x = x : M \\F
  x:M \types x\cdot i(x) = e : M \and
  x:M \types i(x) \cdot x = e : M
\end{mathpar}
Now the \bSet-based proof can be essentially copied in this notation:
\[ x:M \types i(x) = i(x) \cdot e = i(x) \cdot (x \cdot j(x)) = (i(x)\cdot x)\cdot j(x) = e\cdot j(x) = j(x) : M.\]
The essential point is that the notation \emph{looks set-theoretic}, with ``variables'' representing ``elements'', and yet (as we will see) its formal structure is such that it can be interpreted into \emph{any} category with products.
Therefore, writing the proof in this way yields automatically a proof of the general theorem that any two inverse \emph{operators} for a monoid \emph{object} in a category with products are equal.

To be sure, such a proof could be written in standard categorical style using commutative diagrams as well (\cref{ex:fp-inv-uniq}), and experienced category theorists become quite good at translating proofs in this way.
The point of categorical logic is that this sort of work is \emph{so} straightforward that it's actually completely unnecessary to do at all: we can prove a ``meta-theorem'' that does all the work for us.

Before leaving this appetizer section, we mention some further generalizations of this result.
While type theory allows us to use set-like notation to prove facts about any category with finite products, the allowable notation is fairly limited, essentially restricting us to algebraic calculations with variables.
However, if our category has more structure, then we can ``internalize'' more set-theoretic arguments.

As an example, note that for ordinary monoids in sets the uniqueness of inverses can be expressed ``pointwise'' rather than in terms of inverse-assigning operators.
In other words, for each element $x\in M$, if $x$ has two two-sided inverses $y$ and $z$, then $y=z$.
If we think hard enough, we can express this diagrammatically in terms of the category \bSet is to consider the following two sets:
\begin{align*}
  A &= \setof{(x,y,z)\in M^3 | xy=e, yx=e, xz=e, zx=e}\\
  B &= \setof{(y,z)\in M^2 | y=z}
\end{align*}
In other words, $A$ is the set of elements $x$ equipped with two inverses, and $B$ is the set of pairs of equal elements.
Then the uniqueness of pointwise inverses can be expressed by saying there is a commutative diagram
\[ \xymatrix{ A \ar[d] \ar[r] & B \ar[d] \\ M^3 \ar[r]_{\pi_{23}} & M^2 } \]
where the vertical arrows are inclusions and the lower horizontal arrow projects to the second and third components.

This is a statement that makes sense for a monoid object $M$ in any category with finite \emph{limits}.
The object $B$ can be constructed categorically as the equalizer of the two projections $M\times M \toto M$ (which is in fact isomorphic to $M$ itself), while the object $A$ is a ``joint equalizer'' of four parallel pairs, one of which is
\[ \vcenter{\xymatrix{ & M \times M \ar[dr]^m \\
    M\times M\times M \ar[ur]^{\pi_{12}} \ar[dr]_{!} && M \\
    & 1 \ar[ur]_e }} \]
and the others are similar.
We can then try to \emph{prove}, in this generality, that there is a commutative square as above.
We can do this by manipulating arrows, or by appealing to the Yoneda lemma, but we can also use the \emph{internal logic of a category with finite limits}.
This is a syntax like the internal logic for categories with finite products, but which also allows us to \emph{hypothesize equalities}.
The judgment in question is
\begin{equation}\label{eq:pointwise-unique-inverses}
  x:M, y:M, z:M, x\cdot y = e, y\cdot x=e, x\cdot z = e, z\cdot x = e \types y=z.
\end{equation}
As before, the comma binds the most loosely, so this should be read as
\[ ((x:M), (y:M), (z:M), (x\cdot y = e), (y\cdot x=e), (x\cdot z = e), (z\cdot x = e)) \types (y=z). \]
We can prove this by set-like equational reasoning, essentially just as before.
The ``interpretation machine'' then produces from this a morphism $A\to B$, for the objects $A$ and $B$ constructed above.

Next, note that in the category \bSet, the uniqueness of inverses ensures that if every element $x\in M$ has an inverse, then there is a \emph{function} $i:M\to M$ assigning inverses --- even without using the axiom of choice.
(If we define functions as sets of ordered pairs, as is usual in set-theoretic foundations, we could take $i = \setof{(x,y) | xy=e}$; the pointwise uniqueness ensures that this is indeed a function.)
This fact can be expressed in the internal logic of an \emph{elementary topos}.
We postpone the definition of a topos until later; for now we just remark that its structure allows both sides of the turnstile $\types$ to contain \emph{logical formulas} such as $\exists x, \forall y, \phi(x,y)$ rather than just elements and equalities.
In this language we can state and prove the following:
\[ \forall x:M, \exists y:M, x\cdot y = e \land y\cdot x = e \types
\exists i:M^M, \forall x:M, (x\cdot i(x) = e \land i(x)\cdot x = e)
\]
As before, the proof is essentially exactly like the usual set-theoretic one.
Moreover, the interpretation machine allows us to actually extract an ``inverse operator'' morphism in the topos from this proof.
As before, such a result can also be stated and proved using arrows and commutative diagrams, but as the theorems get more complicated, the translation gets more tedious to do by hand, and the advantage of type-theoretic notation becomes greater.

So much for adding extra structure.
In fact, we can also take structure away!
A monoid object can be defined internal to any \emph{monoidal} category, not just a cartesian monoidal one; now the structure maps are $m:M\otimes M\to M$ and $e:I\to M$, and the commutative diagrams are essentially the same.

To define an inverse operator in this case, however, we need some sort of ``diagonal'' $\Delta:M\to M\otimes M$ and also a ``projection'' or ``augmentation'' $\varepsilon:M\to I$.
The most natural hypothesis is that these maps make $M$ into a \emph{comonoid} object, i.e.\ a monoid in the opposite monoidal category, and that the monoid and comonoid structures preserve each other; this is the notion of a \emph{bimonoid} (or ``bialgebra'').
(\cref{ex:cartmon-bimon-uniq}: in a cartesian monoidal category, every object is a bimonoid in a unique way.)

Now given a bimonoid $M$, we can define an ``inverse operator'' --- which in this context is usually called an \emph{antipode} --- to be a map $i:M\to M$ such that
\begin{equation}
  \vcenter{\xymatrix@C=1pc{& M\otimes M \ar[rr]^{i\otimes 1} && M\otimes M \ar[dr]^m \\
      M \ar[ur]^{\Delta} \ar[dr]_{\Delta} \ar[rr]^{\varepsilon} && I \ar[rr]^{e} && M \\
      & M\otimes M \ar[rr]_{1\otimes i} && M\otimes M \ar[ur]_{m}}}
\end{equation}
commutes, where now $\Delta$ and $\varepsilon$ are the comonoid structure of $M$ rather than the diagonal and projection of a cartesian product.
A bimonoid equipped with an antipode is called a \emph{Hopf monoid} (or ``Hopf algebra'').
The obvious question then is, if a bimonoid has two antipodes, are they equal?

In some cases it is possible to apply the previous results directly.
For instance, the category of \emph{(co)commutative} comonoids in a symmetric monoidal category inherits a monoidal structure that turns out to be \emph{cartesian} (\cref{ex:ccmon-cart}), so a cocommutative bimonoid is actually a monoid in a cartesian monoidal category, and we can apply the first version of our result.
Similarly, the category of commutative monoids is cocartesian, so a commutative bimonoid is a comonoid in a cocartesian monoidal category, so we can apply the dual of the first version of our result.
But what if neither the multiplication nor the comultiplication is commutative?

Internal logic is up to this task.
In a monoidal category we can consider judgments with multiple outputs as well as multiple inputs.\footnote{For the benefit of readers who are already experts, I should mention that this is \emph{not} ordinary ``classical linear logic'': the comma represents the same monoidal structure $\tensor$ on both sides of the turnstile, rather than $\tensor$ on the left and $\parr$ on the right.}
This allows us to describe monoids and comonoids in a roughly ``dual'' way.
Don't worry about the precise syntax being used on the right; it will be explained in \cref{sec:prop-smc}.
\begin{alignat*}{2}
  x:M, y:M &\types x\cdot y:M &\qquad x:M &\types (x_1,x_2):(M,M)\\
  &\types e:M &\qquad x:M &\types (\mid\cancel{x}):()\\
  x:M,y:M,z:M &\types (x\cdot y)\cdot z = x\cdot (y\cdot z) :M &\qquad x:M &\types (x_{11},x_{12},x_2)=(x_1,x_{21},x_{22}):(M,M,M)\\
  x:M &\types x\cdot e=x:M &\qquad x:M &\types (x_1\mid\cancel{x_2}) = x:M\\
  x:M &\types e\cdot x=x:M &\qquad x:M &\types (x_2\mid\cancel{x_1}) = x:M
\end{alignat*}
In this language, the bimonoid axioms are
\begin{align*}
  x:M,y:M &\types (x_1\cdot y_1,x_2\cdot y_2) = ((x\cdot y)_1,(x\cdot y)_2) :(M,M)\\
          &\types (e_1,e_2)=(e,e):(M,M)\\
  x:M,y:M &\types (\mid\cancel{x\cdot y}) = (\mid\cancel{x},\cancel{y}) : ()\\
  &\types (\mid\cancel{e})=():()
\end{align*}
And an antipode is a map $x:M \types i(x):M$ such that
\begin{align*}
  x:M &\types x_1\cdot i(x_2) = (e\mid\cancel{x}) :M\\
  x:M &\types i(x_1)\cdot x_2 = (e\mid\cancel{x}) :M
\end{align*}
Now if we have another antipode $j$, we can compute
\begin{align*}
  x:M \types i(x)
  &= i(x)\cdot e\\
  &= (i(x_1)\cdot e\mid\cancel{x_2})\\
  &= i(x_1)\cdot (x_{21} \cdot j(x_{22}))\\
  &= (i(x_1)\cdot x_{21}) \cdot j(x_{22})\\
  &= (e \cdot j(x_{2})\mid\cancel{x_1})\\
  &= e\cdot j(x)\\
  &= j(x) \qquad :M
\end{align*}
yielding the same result $i=j$.
So even in a non-cartesian situation, we can use a very similar set-like argument, as long as we keep track of where elements get ``duplicated and discarded''.

This concludes our ``appetizer''; I hope it has given you a taste of what categorical logic looks like, and what it can do for category theory.
In the next section we will rewind back to the beginning and start with very simple cases.

\subsection*{Exercises}

\begin{ex}\label{ex:fp-inv-uniq}
  Prove, using arrows and commutative diagrams, that any two inverse operators for a monoid object in a category with finite products are equal.
\end{ex}

\begin{ex}\label{ex:cartmon-bimon-uniq}
  Prove that in a cartesian monoidal category, every object is a bimonoid in a unique way.
\end{ex}

\begin{ex}\label{ex:ccmon-cart}
  Show that the category of cocommutative comonoids in a symmetric monoidal category inherits a monoidal structure, and that this monoidal structure is cartesian.
\end{ex}

\begin{ex}\label{ex:antipode}
  Prove, using arrows and commutative diagrams, that any two antipodes for a bimonoid (not necessarily commutative or cocommutative) are equal.
\end{ex}


\section{On type theory and category theory}
\label{sec:generalities}

Since there are many other introductions to categorical logic (a non-exhaustive list could include~\cite{mr:focl,ls:hocl,jacobs:cltt,goldblatt:topoi,ptj:elephant}), it seems appropriate to say a few words about what distinguishes this one.
Our description may not make very much sense to the beginner who doesn't yet know what we are talking about, but it may help to orient the expert, and as the beginner becomes more expert he or she can return to it later on.

Our perspective is very much that of the category theorist: our primary goal is to use type theory as a convenient syntax to prove things about categories.
The way that it does this is by giving concrete presentations of \emph{free} categorical structures, so that by working in those presentations we can deduce conclusions about \emph{any} such structure.
There are other such syntaxes for category theory, notably string diagram calculi, that function in a similar way (giving a concrete presentation of free structures) to the extent that they are made precise.
Indeed, the \emph{usual} way of reasoning in category theory, in which we speak explicitly about objects, arrows, commutative diagrams, and so on, can also be interpreted, from this point of view, to be simply making use of the \emph{obvious} presentation of a free structure rather than some fancier one.
(It can be tempting for the category theorist to want to generalize away from free structures to arbitrary ones, but this temptation should be resisted; see \cref{rmk:free}.)

In particular, this means that we are not interested in aspects of type theory such as computability, canonicity, proof search, cut-elimination, focusing, and so on \emph{for their own sake}.
However, at the same time we recognize their importance for type theory as a subject in its own right, which suggests that they should not be ignored by the category theorist.
If nothing else, the category theorist will encounter these words when speaking to type theorists, and so it is advantageous to have at least a passing familiarity with them.

In fact, our perspective is that it is precisely the esoteric-sounding notion of \emph{cut elimination} (or \emph{admissibility of substitution}) that essentially \emph{defines} what we mean by a \emph{type theory}.
Of course this is not literally true; a more careful statement would be that type theories with cut elimination are those that exhibit the most behavior most characteristic of type theories.
(Jean-Yves Girard remarked that ``a logic without cut-elimination is like a car without an engine.'')
A ``type theory without cut elimination'' may still yield explicit presentations of free structures, but reasoning with such a presentation will not yield the characteristic benefits of categorical logic.

So what is this mysterious cut-elimination, from a categorical perspective?
Informally, it says that the morphisms in a free categorical structure can be presented \emph{without explicit reference to composition}.
This is a bit of a cheat, because as we will see, in fact what we do is to build just enough ``implicit'' reference to composition into our rules to ensure that we no longer need to talk about composition explicitly.
However, this does not make the process trivial, and it can still yield valuable results.

As a simple example of nontriviality, if an arrow is constructed by applying a universal property, then that property automatically determines some of the composites of that arrow.
For instance, a pairing $\pair{f}{g}:X\to A\times B$ must compose with the projections $\pi_1:A\times B\to A$ and $\pi_2:A\times B\to B$ to give $f$ and $g$ respectively.
Thus, these composites do not need to be ``built in'' by hand.

Another interesting fact about cut-elimination is that the composition it produces is automatically associative (and unital), despite the fact that we do not apparently put associativity in anywhere (even implicitly).
Do\v{s}en~\cite{dosen:cutelim-cats}\footnote{[TODO: Read Do\v{s}en and Petri\'{c}, \textit{Proof-Theoretical Coherence}, \url{http://www.mi.sanu.ac.rs/~kosta/publications.htm}]} uses this to ``explain'' or ``justify'' the definition of category (and other basic category-theoretic notions) in terms of cut-elimination.
Of course, for our intended audience of category theorists it is cut-elimination, rather than associativity, that requires explanation and justification; but nevertheless the relationship is intriguing.

Both of these facts are instances of an underlying general principle: by presenting a free categorical structure without explicit reference to composition, we are free to then \emph{define} its composition as an operation on its already-existing morphisms, and we can choose this definition so as to ensure that various desirable properties hold automatically.
This eliminates or reduces the need for quotienting by equivalence relations in the presentation of a free structure.
Put differently, a type theory isolates a class of \emph{canonical forms} for morphisms.
In simple cases every morphism has exactly one canonical form, so that no equivalence relation on the canonical forms is needed; in more complicated situations we still need an equivalence relation, but we can make do with a much simplified one.

Another characteristic advantage of categorical logic is that it enables us to use ``set-like'' reasoning to prove things about arbitrary categories, by means of ``term calculi'' associated to its presentations of free structures.
From this perspective, the admissibility of substitution (which is another name for cut-elimination) says that \emph{the meaning of a notation can be evaluated simply on the basis of the notation as written, without having to guess at the thought processes of the person who wrote it down}.
This is obviously a desirable feature, and arguably even a necessary one if our ``notation'' is to be worthy of the name.

Despite the usefulness of terms, we will maintain and emphasize throughout the principle that terms should be just a convenient notation for derivation trees.
This perspective has many advantages.
For instance, it means that a (constructive) proof of cut-elimination \emph{is already} a definition of substitution into terms; it is not necessary to separately define a notion of ``substitution into terms'' and then prove that this \emph{separately defined} notion of substitution is admissible.
It also deals quite nicely with the problems of $\alpha$-equivalence and bound variable renaming: as an operation on derivations, substitution doesn't need to care about ``whether a free variable is going to get captured''; the point is just that when we choose a term to represent the substituted derivation we have to accord with the general rules for how terms are assigned to derivations.

Most importantly, however, adhering to the ``terms are derivations'' principle greatly simplifies the proofs of the central ``initiality theorems'' (that the type theory really does present the initial category with appropriate structure), since we can define a map out of the type theory \emph{by induction on derivations} and deduce immediately that it is also defined on terms.
If the ``terms are derivations'' principle is broken, then one generally ends up wanting to induct on derivations anyway, and then having to prove laboriously that the resulting ``operation'' on terms is independent of their derivations.

For similar reasons, we present our type theories so as to ensure that as many structural rules as possible are admissible rather than primitive: not only cut/substitution, but also exchange, contraction, and weakening.
Many introductions to type theory are somewhat vague about exactly how these rules are to be imposed, especially for substructural theories such as linear logic with exchange only.
However, when we try to use type theory to present a free symmetric monoidal category (as opposed to a free symmetric monoidal poset), we have to worry about the functoriality of the exchange rule, which technically requires being explicit about exactly how exchange works.
If we make exchange admissible, then it is automatically functorial, just as making substitution admissible gives associativity for free; this considerably simplifies the theory.
Having structural rules as primitive would also make the notation quite tedious if we continued to adhere to the principle that terms are just a notation for derivations.

In fact, much of the literature on categorical logic and type theory contains gaps or even errors relating to these points.
It is very tempting to prove the initiality theorem by induction on derivations without realizing that by breaking the ``terms are derivations'' principle one thereby incurs an obligation to prove that the interpretation of a term is independent of its derivation.
It is also very tempting to include too many primitive rules, perhaps based on the thought that if a rule is true anyway, it's simpler to assume it than to have to prove it; of course, thinking of rules as \emph{operations} in an algebraic theory makes clear that if there are too many of them, then the initial algebra will be too big.

Another unusual feature of our treatment is the emphasis on multicategories (of various generalized sorts, including also the still more general ``polycategories'' and their generalizations).
Although multicategories have been present in categorical logic from close to the beginnings of both (Lambek's original definition of multicategory~\cite{lambek:dedsys-ii} was motivated by logical considerations), they are rarely mentioned in introductions to the subject.
One concrete advantage of using multicategories is a more direct correspondence between the type theory and the category theory: type theory distinguishes between a sequent $A,B\types C$ and a sequent $A\times B\types C$ (even though they are bijectively related), so it seems natural to work with a categorical structure that also distinguishes between morphisms $(A,B)\to C$ and $A\times B\to C$.

However, the correspondence and motivation goes deeper than that.
We may ask \emph{why} type theory distinguishes these two kinds of sequents?
We will discuss this in more detail in \cref{sec:why-multicats}, but the short answer is that ``it makes cut-elimination work''.
More specifically, it enables us to formulate type theory in such a way that \emph{each rule refers to at most one type former}; this enables us to ``commute these rules past each other'' in the proof of cut-elimination.
Moreover, including sequents such as $A,B\types C$ allows us to describe certain operations in a type-theoretic style that would not otherwise be possible, such as a monoidal tensor product.
A type theorist speaks of this in terms of \emph{deciding on the judgmental structure first} (including ``structural rules'') and then defining the connectives to ``internalize'' various aspects of that structure.

From a categorical point of view, the move to (generalized) multicategories has the feature that \emph{it gives things universal properties}.
For instance, the tensor product in a monoidal category has no universal property, but the tensor product in a multicategory does.
In general, from a well-behaved 2-monad $T$ we can define a notion of ``$T$-multicategory''~\cite{burroni:t-cats,leinster:higher-opds,hermida:coh-univ,cs:multicats} in which $T$-algebra structure acquires a universal property (specifically, $T$ is replaced by a lax- or colax-idempotent 2-monad with the same algebras).
In type theoretic language, the move to $T$-multicategories corresponds to including the desired operations in the judgmental structure.
The fact that the $T$-operations then have universal properties is what enables us to write down the usual sort of type-theoretic left/right or introduction/elimination rules for them.

Making this correspondence explicit is helpful for many reasons.
Pedagogically, it can help the category theorist, who believes in universal properties, to understand why type theories are formulated the way they are.
It also makes the ``initiality theorems'' more modular: first we model the judgmental structure with a multicategory, and then we add more type formers corresponding to objects with various universal properties.
It can even be helpful from a purely type-theoretic perspective, suggesting more systematic ways to formulate cut admissibility theorems (see e.g.\ \cref{thm:monpos-cutadm,thm:natded-logic-multicutadm} [TODO: Also the polycategorical case]).
Finally, it provides a guide for new applications of categorical logic: when seeking a categorical structure to model a given type theory, we should look for a kind of multicategory corresponding to its judgments; while when seeking an internal logic for a categorical structure, we should represent it using universal properties in some kind of multicategory, from which we can extract an appropriate judgmental structure.

These facts about cut-elimination and multicategories have surely been known in some form to experts for a long time, but I am not aware of a clear presentation of them for the beginner coming from a category-theoretic background.
They are not strictly necessary if one wants simply to use type theory for internal reasoning about categories, and there are plenty of excellent introductions that take a geodesic route to that application.
However, we believe that they yield a deeper understanding of the type/category correspondence; and they are especially valuable when it comes to designing type theories that correspond to new categorical structures (or vice versa).

We will not assume that the reader has any acquaintance with type theory, or any interest in it apart from its uses for category theory.
However, because one of our goals is to help the reader become familiar with the lingo and concerns of type theorists, we sometimes include a little more detail than is strictly necessary for categorical applications.
The reader should feel free to skip over these brief digressions.



\chapter{Unary type theories}
\label{chap:unary}

We begin our study of type theories and their categorical counterparts with a class of very simple cases that we will call \emph{unary type theories}.
(This terminology is not standard in the literature.)
On the type-theoretic side the word ``unary'' indicates that there is only one type on each side of a sequent $A\types B$.
On the categorical side it means, roughly, that we deal with categories rather than any kind of multicategory.

In some ways the unary case is fairly trivial, but for that very reason it serves as a good place to become familiar with basic notions of type theory and how they correspond to category theory.
In later chapters we will generalize away from unarity in various ways.


\section{Posets}
\label{sec:poset}

We start with the simplest sort of categories: those in which each hom-set has at most one element.
These are well-known to be equivalent to \emph{preordered sets}, where the existence of an arrow $A\to B$ is regarded as the assertion that $A\le B$.
I will abusively call them \emph{posets}, although traditionally posets (partially ordered sets) also satisfy the antisymmetry axiom (if $A\le B$ and $B\le A$ then $A=B$).
From a category-theoretic perspective, antisymmetry means asking a category to be skeletal, which is both unnatural and pointless.
Conveniently, posets also correspond to the simplest version of logic, namely \emph{propositional} logic.

From a category-theoretic perspective, the question we are concerned with is the following.
Suppose we have some objects in a poset, and some ordering relations between them.
For instance, we might have
\begin{mathpar}
  A\le B \and A\le C \and D\le A \and B \le E \and D\le C
\end{mathpar}
Now we ask, given two of these objects --- say, $D$ and $E$ --- is it necessarily the case that $D\le E$?
In other words, is it the case in \emph{any} poset containing objects $A,B,C,D,E$ satisfying the given relations that $D\le E$?
In this example, the answer is yes, because we have $D\le A$ and  $A\le B$ and $B\le E$, so by transitivity $D\le E$.
More generally, we would like a method to answer all possible questions of this sort.

There is an elegant categorical way to do this based on the notion of \emph{free structure}.
Namely, consider the category \bPoset of posets, and also the category \bRelGr of \emph{relational graphs}, by which I mean sets equipped with an arbitrary binary relation.
There is a forgetful functor $U:\bPoset \to \bRelGr$, which has a left adjoint $F$.

Now, the abstract information about ``five objects $A,B,C,D,E$ satisfying five given relations'' can be regarded as an object $\cG$ of \bRelGr, and to give five such objects satisfying those relations in a poset \cP is to give a map $\cG \to U\cP$ in \bRelGr.
By the adjunction, therefore, this is equivalent to giving a map $F\cG \to \cP$ in \bPoset.
Therefore, a given inequality such as $A\le E$ will hold in \emph{all} posets if and only if it holds in the \emph{particular, universal} poset $F\cG$ freely generated by the assumed data.

Thus, to answer all such questions at once, it suffices to give a concrete presentation of the free poset $F\cG$ generated by a relational graph \cG.
In this simple case, it is easy to give an explicit description of $F$: it is the reflexive-transitive closure.
But since soon we will be trying to generalize vastly, we want instead a general method to describe free objects.
From our current perspective, this is the role of type theory.

As noted in \cref{sec:intro}, when we move into type theory we use the symbol $\types$ instead of $\to$ or $\le$.
Type theory is concerned with \emph{(hypothetical) judgments}, which (roughly speaking) are syntactic gizmos of the form ``$\Gamma\types\Delta$'', where $\Gamma$ and $\Delta$ are syntactic gadgets whose specific nature is determined by the specific type theory under consideration (and, thus, by the particular kind of categories we care about).
We call $\Gamma$ the \emph{antecedent} or \emph{context}, and $\Delta$ the \emph{consequent} or \emph{co-context}.
In our simple case of posets, the judgments are simply
\[ A \types B \]
where $A$ and $B$ are objects of our (putative) poset; such a judgment represents the relation $A\le B$.
In general, the categorical view is that a hypothetical judgment represents a sort of \emph{morphism} (or, as we will see later, a sort of \emph{object}) in some sort of categorical structure.

In addition to a class of judgments, a type theory consists of a collection of \emph{rules} by which we can operate on such judgments.
Each rule can be thought of as a partial $n$-ary operation on the set of possible judgments for some $n$ (usually a finite natural number), taking in $n$ judgments (its \emph{premises}) that satisfy some compatibility conditions and producing an output judgment (its \emph{conclusion}).
We generally write a rule in the form
\begin{mathpar}
  \inferrule{\cJ_1 \\ \cJ_2 \\ \cdots \\ \cJ_n}{\cJ}
\end{mathpar}
with the premises above the line and the conclusion below.
A rule with $n=0$ is sometimes called an \emph{axiom}.
The categorical view is that we have a given ``starting'' set of judgments representing some objects and putative morphisms in the ``underlying data'' of a categorical structure, and the closure of this set under application of the rules yields the objects and morphisms in the \emph{free} structure it generates.

We will attempt to make all of this precise in \cref{chap:dedsys}, which the reader is free to consult now.
However, it is probably more illuminating at the moment to bring it back down to earth in our very simple example.
Since the properties distinguishing a poset are reflexivity and transitivity, we have two rules:
\begin{mathpar}
  \inferrule{ }{A\types A} \and
  \inferrule{A\types B \\ B\types C}{A\types C}
\end{mathpar}
in which $A,B,C$ represent arbitrary objects.
In other words, the first says that for any object $A$ we have a $0$-ary rule whose conclusion is $A\types A$, while the second says that for any objects $A,B,C$ we have a $2$-ary rule whose premises are $A\types B$ and $B\types C$ (that is, any two judgments of which the consequent of the first is the antecedent of the second) and whose conclusion is $A\types C$.
We will refer to the pair of these two rules as the \textbf{free type theory of posets}.

Hopefully it makes sense that we can construct the reflexive-transitive closure of a relational graph by expressing its relations in this funny syntax and then closing up under these two rules, since they are exactly reflexivity and transitivity.
Categorically, of course, that means identities and composition.
In type theory the composition/transitivity rule is often called \textbf{cut}, and plays a unique role, as we will see later.

In the example we started from,
\begin{mathpar}
  A\le B \and A\le C \and D\le A \and B \le E \and D\le C
\end{mathpar}
we have the two instances of the transitivity rule
\begin{mathpar}
  \inferrule{D\types A \\ A\types B}{D\types B}\and
  \inferrule{D\types B \\ B\types E}{D\types E}
\end{mathpar}
allowing us to conclude $D\types E$.
When applying multiple rules in sequence to reach a conclusion, it is customary to write them in a ``tree'' structure like so:
\begin{mathpar}
  \inferrule*{\inferrule*{D\types A \\ A\types B}{D\types B} \\ B\types E}{D\types E}
\end{mathpar}
Such a tree is called a \emph{derivation}.
The way to typeset rules and derivations in \LaTeX\ is with the \texttt{mathpartir} package; the above diagram was produced with
\begin{verbatim}
  \inferrule*{
    \inferrule*{D\types A \\ A\types B}{D\types B} \\
    B\types E
  }{
    D\types E
  }
\end{verbatim}
Note that \texttt{mathpartir} has only recently made it into standard distributions of \LaTeX, so if you have an older system you may need to download it manually.

Formally speaking, what we have observed is the following \emph{initiality theorem}.

\begin{thm}\label{thm:poset-initial-1}
  For any relational graph \cG, the free poset $\F{\bPoset}\cG$ that it generates is has the same objects and its morphisms are the judgments that are derivable from \cG in free type theory of posets.
\end{thm}
\begin{proof}
  In the preceding discussion we assumed it as known that the free poset on a relational graph is its reflexive-transitive closure, which makes this theorem more or less obvious.
  However, it is worth also presenting an explicit proof that does not assume this, since same pattern of proof will reappear many times for more complicated type theories where we don't know the answer in advance.

  Thus, let us define $\F{\bPoset}\cG$ as stated in the theorem.
  The reflexivity and transitivity rules imply that $\F{\bPoset}\cG$ is in fact a poset.
  Now suppose $\cA$ is any other poset and $P:\cG\to\cA$ is a map of relational graphs.
  The objects of $\F{\bPoset}\cG$ are the same as those of \cG, so $P$ extends uniquely to a map on underlying sets $\F{\bPoset}\cG\to\cA$.
  Thus it suffices to show that this map is order-preserving, i.e.\ that if $A\types B$ is derivable from \cG in the free type theory of posets, then $P(A)\le P(B)$.

  For this purpose we \emph{induct on the derivation of $A\types B$}.
  There are multiple ways to phrase such an induction.
  One is to define the \emph{height} of a derivation to be the number of rules appearing in it, and then induct on the height of the derivation of $A\types B$.
  \begin{enumerate}
  \item If there are no rules at all, then $A\types B$ must come from a relation $A\le B$ in \cG; hence $P(A)\le P(B)$ since $P$ is a map of relational graphs.
  \item If there are $n>0$ rules, then consider the last rule.
    \begin{enumerate}
    \item If it is the identity rule $A\types A$, then $P(A)\le P(A)$ in \cA since \cA is a poset and hence reflexive.
    \item Finally, if it is the transitivity rule, then each of its premises $A\types B$ and $B\types C$ must have a derivation with strictly smaller height, so by the (strong) inductive hypothesis we have $P(A)\le P(B)$ and $P(B)\le P(C)$.
      Since \cA is a poset and hence transitive, we have $P(A)\le P(C)$.\qedhere
    \end{enumerate}
  \end{enumerate}
\end{proof}

A different way to phrase such an induction, which is more flexible and more type-theoretic in character, uses what is called \emph{structural induction}.
This means that rather than introduce the auxiliary notion of ``height'' of a derivation, we apply a general principle that \emph{to prove that a property $P$ holds of all derivations, it suffices to show for each rule that if $P$ holds of the premises then it holds of the conclusion}.
We can also define operations on derivations by \emph{structural recursion}, meaning that it suffices to define what happens to the conclusion of each rule assuming that we have already defined what happens to the premises.
Structural induction and recursion can be justified formally by set-theoretic arguments --- see \cref{chap:dedsys} for some general statements.
However, intuitively they implicit in what is meant by saying that ``derivations are what we obtain by applying rules one by one,'' just as ordinary mathematical induction is implicit in saying that ``the natural numbers are what we obtain by starting with zero and constructing successors one by one'', and constructive type-theoretic foundations for mathematics often take them as axiomatic.
From now on we will use structural induction and recursion on derivations in all type theories without further comment.

However, it is proved, \cref{thm:poset-initial-1} enables us to reach conclusions about arbitrary posets by deriving judgments in type theory.
In our present trivial case this is not very useful, but as we will see it becomes more useful for more complicated structures.

Another way to express the initiality theorem is to incorporate \cG into the rules.
Given a relational graph \cG, we define the \textbf{type theory of posets under \cG} to be the free type theory of posets together with a 0-ary rule
\begin{mathpar}
  \inferrule{ }{A\types B}
\end{mathpar}
for any relation $A\le B$ in \cG.
Now a derivation can be written without any ``leaves'' at the top, such as
\begin{mathpar}
  \inferrule*{\inferrule*{\inferrule*{ }{D\types A} \\ \inferrule*{ }{A\types B}}{D\types B} \\ \inferrule*{ }{B\types E}}{D\types E}
\end{mathpar}
Clearly this produces the same judgments; thus the initiality theorem can also be expressed as follows.

\begin{thm}\label{thm:poset-initial-2}
  For any relational graph \cG, the free poset $\F{\bPoset}\cG$ that it generates is has the same objects and its morphisms are the derivable judgments in the type theory of posets under \cG.\qed
\end{thm}

We can extract from this our first general statement about categorical logic: it is \emph{a syntax for generating free categorical structures using derivations from rules}.
The reader may be forgiven at this time for wondering what the point is; but bear with us and things will get less trivial.


\section{Categories}
\label{sec:categories}

Let's now generalize from posets to categories.
The relevant adjunction is now between categories \bCat and \emph{directed graphs} \bGr; the latter are sets $\cG$ of ``vertices'' equipped with a set $\cG(A,B)$ of ``edges'' for each $x,y\in \cG$.
Thus, we hope to generate the free category $\F{\bCat}\cG$ on a directed graph \cG type-theoretically.

Our judgments $A\types B$ will still represent morphisms from $A$ to $B$, but now of course there can be more than one such morphism.
Thus, to specify a particular morphism, we need more information than the simple \emph{derivability} of a judgment $A\types B$.
Na\"ively, the first thing we might try is to identify this extra information with the \emph{derivation} of such a judgment, i.e.\ with the tree of rules that were applied to reach it.
This makes the most sense if we take the approach of \cref{thm:poset-initial-2} rather than \cref{thm:poset-initial-1}, so that distinct edges $f,g\in \cG(A,B)$ can be regarded as distinct \emph{rules}
\begin{mathpar}
  \inferrule*[right=$f$]{ }{A\types B} \and
  \inferrule*[right=$g$]{ }{A\types B} \and
\end{mathpar}
Thus, for instance, if we have also $h\in \cG(B,C)$, the distinct composites $h\circ g$ and $h\circ f$ will be represented by the distinct derivations
\begin{mathpar}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{B\types C} \\
    \inferrule*[right=$g$]{ }{A\types B}
  }{
    A\types C
  }\and
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{B\types C} \\
    \inferrule*[right=$f$]{ }{A\types B}
  }{
    A\types C
  }\and
\end{mathpar}
Note that when we have distinct rules with the same premises and conclusion, we have to label them so that we can tell which is being applied.
For consistency, we begin labeling the identity and composition rules too, with $\circ$ and $\idfunc$.

Of course, this na\"ive approach founders on the fact that composition in a category is supposed to be associative and unital, since the two composites $h\circ (g\circ f)$ and $(h\circ g)\circ f$, which ought to be equal, nevertheless correspond to distinct derivations:
\begin{equation}\label{eq:assoc}
  \begin{array}{c}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{C\types D} \\
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$g$]{ }{B\types C} \\
      \inferrule*[right=$f$]{ }{A\types B}
    }{
      A\types C
    }}{
    A\types D
  }\\\\
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$h$]{ }{C\types D} \\
      \inferrule*[right=$g$]{ }{B\types C}
    }{
      B\types D
    }\\
    \inferrule*[right=$f$]{ }{A\types B}
  }{
    A\types D
  }
  \end{array}
\end{equation}
Thus, with this type theory we don't get the free category on \cG, but rather some free category-like structure that lacks associativity and unitality.
There are two ways to deal with this problem; we consider them in turn.

\subsection{Explicit cuts}
\label{sec:category-cutful}

The first solution is to simply quotient by an equivalence relation.
Our equivalence relation will have to identify the two derivations in~\eqref{eq:assoc}, and also the similar pairs for identities:
\begin{mathpar}
  \inferrule{\inferrule*[right=$\idfunc$]{ }{A\types A}\\ {A\types B}}{A\types B}\;\circ
  \qquad\equiv\qquad A\types B
  \\
  \inferrule{A\types B \\ \inferrule*[right=$\idfunc$]{ }{B\types B}}{A\types B}\;\circ
  \qquad\equiv\qquad A\types B
\end{mathpar}
Our equivalence relation must also be a ``congruence for the tree-construction of derivations'', meaning that these identifications can be made anywhere in the middle of a long derivation, such as:
\begin{mathpar}
  \inferrule{\inferrule*{}{\sD_1\\\\\vdots} \\
    \inferrule*[right=$\circ$]{\inferrule*[right=$\idfunc$]{ }{A\types A}\\ \inferrule*{\sD_2\\\\\vdots}{A\types B}}{A\types B}
  }{\vdots\\\\\sD_3}
  \qquad\equiv\qquad
  \inferrule{\inferrule*{}{\sD_1\\\\\vdots} \\
    \inferrule*{\sD_2\\\\\vdots}{A\types B}
  }{\vdots\\\\\sD_3}
\end{mathpar}
We will also have to close it up under reflexivity, symmetry, and transitivity to make an equivalence relation.

Of course, it quickly becomes tedious to draw such derivations, so it is convenient to adopt a more succinct syntax for them.
We begin by labeling each judgment with a one-dimensional syntactic representation of its derivation tree, such as:
\begin{mathpar}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$g$]{ }{g:(B\types C)} \\
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$\idfunc$]{ }{\idfunc_B:(B\types B)} \\
      \inferrule*[right=$f$]{ }{f:(A\types B)}
    }{
      (\idfunc_B\comp{B} f):(A\types B)
    }}{
    (g\comp{B} (\idfunc_B\comp{B} f)) : (A\types C)
  }
\end{mathpar}
These labels are called \emph{terms}.
Of course, in this case they are none other than the usual notation for composition and identities.
Formally, this means the rules are now:
\begin{mathpar}
  \inferrule{f\in\cG(A,B)}{f:(A\types B)}\and
  \inferrule{A\in\cG}{\idfunc_A : (A\types B)}\and
  \inferrule{\phi:(A\types B) \\ \psi:(B\types C)}{\psi\comp{B} \phi:(A\types C)}
\end{mathpar}
% TODO: Mention somewhere the assumptions of "external" facts as premises
Here $\phi,\psi$ denote arbitrary terms, and if they contain $\circ$'s themselves then we put parentheses around them, as in the example above.
Now the generators of our equivalence relation look even more familiar:
\begin{align*}
  \chi \comp{C} (\psi \comp{B} \phi) &\equiv (\chi\comp{C}\psi)\comp{B}\phi\\
  \phi \comp{A} \idfunc_A &\equiv \phi\\
  \idfunc_B \comp{B} \phi &\equiv \phi
\end{align*}
Again $\phi,\psi,\chi$ denote arbitrary terms, corresponding to the fact that arbitrary derivations can appear at the top of our identified trees; and similarly these identifications can also happen anywhere inside another term, so that for instance
\[ k\comp{C} (h\comp{B} (g\comp{A} f)) \equiv k\comp{C} ((h\comp{B} g)\comp{A} f).  \]

Of course, we only impose these relations when they make sense.
We can describe the conditions under which this happens using rules for a secondary judgment $\phi\equiv \psi : (A\types B)$.
The rules for our generating equalities are
\begin{mathpar}
  \inferrule{\phi:(A\types B) \\ \psi:(B\types C) \\ \chi:(C\types D)}{(\chi \comp{C} (\psi \comp{B} \phi) \equiv (\chi\comp{C}\psi)\comp{B}\phi) : (A\types D)}\\
  \inferrule{\phi:(A\types B)}{(\phi \circ \idfunc_A \equiv \phi):(A\types B)}\and
  \inferrule{\phi:(A\types B)}{(\idfunc_B \circ \phi \equiv \phi):(A\types B)}
\end{mathpar}
and we must also have rules ensuring that we have an equivalence relation and a congruence:
\begin{mathpar}
  \inferrule{\phi:(A\types B)}{(\phi\equiv\phi):(A\types B)}\and
  \inferrule{(\phi\equiv\psi):(A\types B)}{(\psi\equiv\phi):(A\types B)}\and
  \inferrule{(\phi\equiv\psi):(A\types B)\\(\psi\equiv\chi):(A\types B)}{(\phi\equiv\chi):(A\types B)}\and
  \inferrule{(\phi_1\equiv\psi_1):(A\types B)\\(\phi_2\equiv\psi_2):(B\types C)}{(\phi_2\comp{B} \phi_1 \equiv \psi_2\comp{B}\psi_1):(A\types C)}
\end{mathpar}
The last of these is sufficient, in our simple case, to ensure we have a congruence; in general we would have to have one such equality rule for each basic rule of the theory (except for those with no premises, like $\idfunc$).

Many of our type theories will involve such an equality judgment, for which we always use the notation $\equivsym$, and the need for the equivalence relation and congruence rules is always the same.
Thus, we generally decline to mention them, stating only the ``interesting'' generating equalities for the theory.
A general framework for such equality judgments is described in \cref{sec:axioms}.

In our case, when the rules for $\circ$ and $\idfunc$ are augmented by these rules for $\equiv$, and we also add axioms for the edges of a given directed graph \cG, we call the result the \textbf{cut-ful type theory for categories under \cG}.
It may seem obvious that this produces the free category on \cG, but again we write it out carefully to help ourselves get used to the patterns.
In particular, we want to emphasize the role played by the following lemma:

\begin{lem}\label{thm:category-tad}
  If $\phi :(A\types B)$ is derivable in the cut-ful type theory for categories under \cG, then it has a unique derivation.
\end{lem}
\begin{proof}
  The point is that the terms produced by all the rules have disjoint forms.
  If $\phi$ is of the form ``$f$'' for some $f\in\cG(A,B)$, then it can only be derived by the first rule applied to $f$.
  If it is of the form ``$\idfunc_A$'', then it can only be derived by the identity rule applied to $A$.
  Finally, if it is of the form ``$\psi\comp{C}\phi$'' it can only be derived by the composition rule applied to $\phi:(A\types C)$ and $\psi:(C\types B)$, and by induction the latter judgments also have unique derivations.
\end{proof}

In other words, the terms (before we impose the relation $\equiv$ on them) really are simply one-dimensional representations of derivations, as we intended.
Not everything that ``looks like a term'' represents a derivation, but if it does, it represents a unique one.
(We have not precisely defined exactly what ``looks like a term'', but it should make intuitive sense; a formal definition is given in \cref{chap:dedsys}.)
It is easy to see that conversely every derivation is represented by a unique term, since the above rules for annotating derivations by terms are deterministic.

The above simple inductive proof of \cref{thm:category-tad} depends in particular on the presence of the subscript on the symbol $\circ$.
Similar annotations will reappear in many subsequent theories.
In the present case we could omit these annotations and still reconstruct a unique derivation, because we know the domain and codomain of all the generating morphisms in \cG.
However, this would require a more ``global'' analysis of the term; whereas a clean inductive proof such as the above has the advantage that it can be regarded as a recursively defined \emph{algorithm for type-checking}.
This can be programmed into a computer, and arguably represents reasonably faithfully what human mathematicians do when reading syntax.
With that said, when writing for a human reader (and even an electronic reader whose programmer has been clever enough) it is often possible to leave off annotations of this sort without fear of ambiguity, and we will frequently do so.

Not all type theories have the property that terms uniquely determine their derivations by a direct inductive algorithm; but those that don't tend to be much more complicated to analyze and prove the initiality theorem for.
We will call this property \textbf{terms are derivations} or \textbf{type-checking is possible}, and we will always attempt to construct our type theories so that it holds.

\begin{rmk}
  Technically, there is either more or less happening here than may appear (depending on your point of view).
  A term as we write it on the page is really just a string of symbols, whereas in the proof of \cref{thm:category-tad} we have assumed that a term such as ``$f\comp{B} (g\comp{A} h)$'' can uniquely be read as $\comp{B}$ applied to ``$f$'' and ``$g\comp{A} h$''.
  This simple string of symbols could technically be regarded as $\comp{A}$ applied to ``$f\comp{B} (g$'' and ``$h)$'', but of course that would make no sense because those are not meaningful terms in their own right (in particular, they contain unbalanced parentheses).

  Thus, something \emph{more} must be happening, and that something else is called \emph{parsing} a term.
  Human mathematicians do it instinctively without thinking; electronic mathematicians have to be programmed to do it.
  In either case, the result of parsing a string of symbols is an ``internal'' representation (a mental idea for humans, a data structure for computers) that generally has the form of a tree, indicating the ``outermost'' operation as the root with its operands as branches, and so on, for instance:
  \[ f\comp{B} (g\comp{A} h) \qquad\leadsto\qquad \vcenter{\xymatrix@-1pc{ & \comp{B} \ar@{-}[dl] \ar@{-}[dr]\\
      f && \comp{A} \ar@{-}[dl]\ar@{-}[dr]\\
      & g && h }} \]

  Of course, this ``internal'' tree representation of a term is nothing but the corresponding derivation flipped upside-down.
  So in that sense \cref{thm:category-tad} is actually saying \emph{less} than one might think: the derivation tree is actually being constructed by the silent step of parsing, while the type-checking algorithm consists only of \emph{labeling} the nodes of this tree by rules in a consistent manner.
  We will not say much more about parsing, however;
  % (though we discuss it a bit further in \cref{chap:dedsys});
  we trust the human reader to do it on their own, and we trust programmers to have good algorithms for it.
\end{rmk}

Now we can prove the initiality theorem.

\begin{thm}\label{thm:category-initial-1}
  The free category on a directed graph $\cG$ has the same objects as \cG, and its morphisms $A\to B$ are the derivations of $A\types B$ (or equivalently, the terms $\phi$ such that $\phi :(A\types B)$ is derivable) in the cut-ful type theory for categories under \cG, modulo the equivalence relation $\phi\equiv \psi:(A\types B)$.
\end{thm}
\begin{proof}
  Let $\F\bCat\cG$ be defined as described in the theorem; the identity and composition rules give it the structure necessary to be a category, and the transitivity and unitality relations make it a category.

  Now suppose \cA is any category and $P:\cG\to\cA$ is a map of directed graphs.
  Then $P$ extends uniquely to the objects of $\F\bCat\cG$, since they are the same as those of \cG.
  But unlike the case of posets, we have to define it on the morphisms of $\F\bCat\cG$ as well.

  If $\phi :(A\types B)$ is derivable, then by \cref{thm:category-tad} it has a unique derivation; thus we can define $P(\phi)$ by recursion on the derivation of $\phi$.
  Of course, if the derivation of $\phi$ ends with $f\in\cG(A,B)$, then we define $P(\phi)=P(f)$; if it ends with $\idfunc_A$ we define $P(\phi)=\idfunc_{P(A)}$; and if it ends with $\psi\comp{C}\chi$ we define $P(\phi) = P(\psi)\circ P(\chi)$.

  We also have to show that this definition respects the equivalence relation $\equiv$.
  This is clear since $\cA$ is a category; formally it would be another induction on the derivations of $\equiv$ judgments.

  Finally, we have to show that this $P:\F\bCat\cG\to\cA$ is a functor.
  This follows by definition of the category structure of $\F\bCat\cG$ and the action of $P$ on its arrows.
\end{proof}

Of course, once again very little seems to be happening; we are just using a complicated funny syntax to build a free algebraic structure.
Therefore, it is the second way to deal with the problem of associativity that is more interesting.

\subsection{Cut admissibility}
\label{sec:category-cutadm}

In this case what we do is \emph{remove the composition rule $\circ$ entirely}; instead we ``build (post)composition into the axioms''.
That is, the only rule independent of \cG is identities:
\[ \inferrule{ }{A\types A}\,\idfunc \]
while for every edge $f\in \cG(A,B)$ we take the following rule:
\[ \inferrule{X\types A}{X\types B} \,f \]
for any $X$.
Informally, one might say that we represent $f$ by its ``image under the Yoneda embedding''.

Note that we have made a choice to build in \emph{postcomposition}; we could also have chosen to build in precomposition.
In the current context, either choice would work just as well; but later on we will see that there were reasons to choose postcomposition here.
We will call this the \textbf{cut-free type theory for categories under \cG}.

In this theory, if we have $f\in\cG(A,B)$, $g\in\cG(B,C)$, and $h\in \cG(C,D)$ there is \emph{only one way} to derive $A\types D$:
\begin{mathpar}
  \inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{A\types A}
      }{
        A\types B
      }
    }{
      A\types C
    }
  }{
    A\types D
  }
\end{mathpar}
Thus, we no longer have to worry about distinguishing between $h\circ (g\circ f)$ and $(h\circ g)\circ f$.
Of course, we have a new problem: if we are trying to build a category, then we \emph{do} need to be able to compose arrows!
So we need the following theorem:

\begin{thm}\label{thm:category-cutadm}
  If we have derivations of $A\types B$ and $B\types C$ in the cut-free type theory for categories under \cG, then we can construct a derivation of $A\types C$.
\end{thm}
\begin{proof}
  We induct on the derivation of $B\types C$.
  If it ends with $\idfunc$, then it must be that $B=C$; so our given derivation of $A\types B$ is also a derivation of $A\types C$.
  Otherwise, we must have some $f\in\cG(D,C)$ and our derivation of $B\types C$ ends like this:
  \begin{mathpar}
    \inferrule*[right=$f$]{\inferrule*{\sD\\\\\vdots}{B\types D}}{B\types C}
  \end{mathpar}
  In particular, it contains a derivation \sD of $B\types D$.
  Thus, by the inductive hypothesis we have a derivation, say $\sD'$, of $A\types D$.
  Now we can simply follow this with the rule for $f$:
  \begin{equation*}
    \inferrule*[right=$f$]{\inferrule*{\sD'\\\\\vdots}{A\types D}}{A\types C}\qedhere
  \end{equation*}
\end{proof}

In type-theoretic lingo, \cref{thm:category-cutadm} says that \textbf{the cut rule is admissible} in the cut-free type theory for categories under \cG.
In other words, although the cut/composition rule
\begin{mathpar}
  \inferrule*[right=$\circ$]{A\types B \\ B\types C}{A\types C}
\end{mathpar}
is not \emph{part of the type theory} as defined, it is nevertheless true that whenever we have derivations of the premises of this rule, we can construct a derivation of its conclusion.

\begin{rmk}\label{rmk:admissible-derivable-1}
  This is what it means in general for a rule to be \textbf{admissible}: it is not part of the theory as defined (that is, it is not one of the \textbf{primitive rules}), but nevertheless if it were added to the theory it would not change the set of derivable sequents.
  In between primitive and admissible rules there are \textbf{derivable rules}: those that can be expanded out directly into a fragment of a derivation in terms of the primitive rules.
  For instance, if we have $f\in\cG(A,B)$ and $g\in \cG(B,C)$, then the left-hand rule below is derivable:
  \begin{mathpar}
    \inferrule*{X\types A}{X\types C}\and
    \inferrule*[Right=$g$]{\inferrule*[Right=$f$]{X\types A}{X\types B}}{X\types C}
  \end{mathpar}
  because we can expand it out into the right-hand derivation in terms of the primitive rules.
  Any derivable rule is admissible: if we have a derivation of $X\types A$ we can follow it with the $f$ and $g$ rules to obtain a derivation of $X\types C$.
  Note the difference with the proof of cut-admissibility: here we do not need to modify the given derivation, we only apply further primitive rules to its conclusion.
  We will return to this distinction in \cref{rmk:admissible-derivable-2}.
\end{rmk}

Closely related to cut-admissibility is \textbf{cut-elimination}, which in our theory takes the following form.

\begin{thm}\label{thm:category-cutelim}
  Consider the cut-free type theory for categories under \cG with the cut rule \emph{added} as primitive.
  If $A\types B$ has a derivation in this new theory, then it also has a derivation in the cut-free theory.
\end{thm}
\begin{proof}
  We induct on the derivation of $A\types B$.
  If it ends with $\idfunc$, it is already cut-free.
  If it ends like this for some $f\in\cG(C,B)$:
  \begin{mathpar}
    \inferrule*[right=$f$]{\inferrule*{\sD\\\\\vdots}{A\types C}}{A\types B}
  \end{mathpar}
  then by induction, $A\types C$ has a cut-free derivation, to which we can apply the $f$ rule to obtain a cut-free derivation of $A\types B$.
  Finally, if it ends with the cut rule:
  \begin{mathpar}
    \inferrule*[right=cut]{\inferrule*{\sD_1\\\\\vdots}{A\types C} \\ \inferrule*{\sD_2\\\\\vdots}{C\types B}}{A\types B}
  \end{mathpar}
  then by induction $A\types C$ and $C\types B$ have cut-free derivations, and thus by \cref{thm:category-cutadm} so does $A\types B$.
\end{proof}

Note that cut-elimination is a fairly straightforward consequence of cut-admissibility: the latter allows us to eliminate each cut one by one.
This will nearly always be true for our type theories, so we will usually just prove cut admissibility and rarely remark on the cut-elimination theorem that follows from it.
On the other hand, cut admissibility is a special case of cut-elimination, and sometimes people prove cut-elimination directly without explicitly using cut-elimination as a lemma.
Under this approach, the inductive step in cut-admissibility is viewed instead as a step of ``pushing cuts upwards'' through a derivation: given a derivation as on the left below in the theory with cut, we transform it into the derivation on the right in which the cut is higher up.
\begin{equation*}
  \inferrule*[right=cut]{\inferrule*{\sD_1\\\\\vdots}{A\types B} \\
    \inferrule*[Right=$f$]{\inferrule*{\sD_2\\\\\vdots}{B\types C}}{B\types D}}{A\types D}
  \quad\leadsto\quad
  \inferrule*[right=$f$]{\inferrule*[Right=cut]{\inferrule*{\sD_1\\\\\vdots}{A\types B} \\
    \inferrule*{\sD_2\\\\\vdots}{B\types C}}{A\types C}}{A\types D}
\end{equation*}
Because our derivation trees are finite (or, more generally, well-founded) this process must eventually terminate with all the cuts eliminated.

A more category-theoretic way to say what is going on is that the morphisms in the free category on a directed graph \cG have an explicit description as \emph{finite strings of composable edges} in \cG.
We have just given an inductive definition of ``finite string of composable edges'': there is a finite string (of length 0) from $A$ to $A$; and if we have such a string from $X$ to $A$ and an edge $f\in\cG(A,B)$, we can construct a string from $X$ to $B$.

We could prove the initiality theorem by appealing to this known fact about free categories, but as before, we prefer to give a more explicit proof to illustrate the patterns of type theory.
For this purpose, it is convenient to first introduce terms, as we did in the previous section for the cut-ful theory.
We can do this with terms directly constructed so that their parse tree will mirror the derivation tree, for instance writing the rules as
\begin{mathpar}
  \inferrule{ }{\idfunc_A:(A\types A)}\,\idfunc\and
  \inferrule{\phi:(X\types A)}{\postc f\phi:(X\types B)} \,f
\end{mathpar}
Then a term derivation and corresponding parse tree would look like
\begin{equation*}
\inferrule*[right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{\idfunc_A:(A\types A)}
      }{
        \postc f{\idfunc_A}:(A\types B)
      }
    }{
      \postc g{\postc f{\idfunc_A}}:(A\types C)
    }
  }{
    \postc h{\postc g{\postc f{\idfunc_A}}}:(A\types D)
  }
  \qquad\leadsto\qquad
  \raisebox{2.75cm}{\xymatrix@-1pc{
      \postcsym h \ar@{-}[d] \\
      \postcsym g \ar@{-}[d] \\
      \postcsym f \ar@{-}[d] \\
      \idfunc_A
    }}
\end{equation*}
However, now there is another option available to us, which begins to show more of the characteristic behavior of type-theoretic terms.
Rather than describing the entire judgment $A\types B$ with a term, the way we did for the cut-ful theory, we assign a \emph{formal variable} such as $x$ to the domain $A$, and then an expression containing $x$ to the codomain $B$.
For the theory of plain categories that we are working with here, the only possible expressions are repeated applications of function symbols to the variable, such as $h(g(f(x)))$.
We write this as
\[ x:A \types h(g(f(x))) : B\]
The identity and generator rules can now be written as
\begin{mathpar}
  \inferrule{ }{x:A\types x:A}\,\idfunc \and
  \inferrule{x:X\types M:A \\ f\in\cG(A,B)}{x:X\types f(M):B} \,f
\end{mathpar}
Here $M$ denotes an arbitrary term, which will of course involve the variable $x$.
Thus, for instance, the composite of $h$, $g$, and $f$ would be written like so:
\begin{mathpar}
  \inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{x:A\types x:A}
      }{
        x:A\types f(x):B
      }
    }{
      x:A\types g(f(x)): C
    }
  }{
    x:A\types h(g(f(x))):D
  }
\end{mathpar}
Of course, the term $h(g(f(x)))$ has essentially the same parse tree as the term $\postc h{\postc g{\postc f{\idfunc_A}}}$ shown above, so it can clearly represent the same derivation.
The main difference is that instead of $\idfunc_A$ we have the variable $x$ representing the identity rule.

This is our first encounter with how type theory permits a ``set-like'' syntax when reasoning about arbitrary categorical structures.
It is also one reason why we chose to build in postcomposition rather than precomposition.
If we used precomposition instead, then the analogous syntax would be backwards: we would have to represent $f:A\to B$ as $f(u):A \types u:B$ rather than $x:A \types f(x):B$.
At a formal level, there would be little difference, but it feels much more familiar to apply functions to variables than to co-apply functions to co-variables.
(We can still dualize at the level of the categorical models; we already mentioned in \cref{sec:intro} that we could apply the type theory of categories with finite products to the opposite of the category of commutative rings.)

Now we observe that terms are still derivations in this theory.

\begin{lem}\label{thm:category-tad}
  If $x:X \types M:B$ is derivable in the cut-free type theory for categories under \cG, then it has a unique derivation.
\end{lem}
\begin{proof}
  If $M$ is the variable $x$, then the only possible derivation is $\idfunc$.
  And if $M = f(N)$, where $f\in\cG(A,B)$, then it can only be obtained from the generator rule for $f$ applied to $x:X \types N:A$.
\end{proof}

Note that the terms in this theory are simpler than those in the cut-ful theory in that we don't need the type subscripts on the composition operation $\comp{A}$.
This is because each rule composes with only one generator $f$, and each such generator ``knows'' its domain, so the premise of the rule is determined by the conclusion.

Another difference between the two theories that instead of attaching a term to the entire derivation such as $(f\circ g): (A\types C)$, we now attach a variable to the antecedent and a more complex term to the consequent.
Really it is the pair of both of these that plays the role played by the terms in \cref{sec:category-cutful}; that is, we may regard $x:A \types M:B$ as a notational variation of something like\footnote{The period used for the pairing here is a ``variable binder''; we will return to it later on.} $x.M:(A\types B)$, and regard $x.M$ as the real ``term''.
However, everyone always refers to the non-variable part $M$ as the \emph{term}, and the separation into variable (or, later, variables) and term is responsible for much of the characteristic behavior of terms in type theory.

In particular, unlike in the cut-ful theory, it is no longer true that each derivation determines a \emph{unique} term (or more precisely, variable-term pair), because we have to choose a name for the variable.
As written on the page, the judgments $x:A \types f(x):B$ and $y:A \types f(y):B$ are distinct; but they represent the same derivation (if we remove the term annotations) and the same morphism:
\begin{mathpar}
  \inferrule*[right=$f$]{\inferrule*[Right=$\idfunc$]{ }{x:A\types x:A}}{x:A\types f(x):B}
  \and
  \inferrule*[right=$f$]{\inferrule*[Right=$\idfunc$]{ }{y:A\types y:A}}{y:A\types f(y):B}
\end{mathpar}

This should not really be overly worrisome.
Recall that we regard terms as merely \emph{notation} for derivations, which we introduced in order to talk about derivations (and, in particular, to describe an equivalence relation $\equiv$ on them) in a more concise and readable way.
Thus, we are really just saying that we have more than one notation for the same thing, which is of course commonplace in mathematics.
For instance, saying ``let $f(x)=x^2$'' and ``let $f(t)=t^2$'' are two notationally different ways to define exactly the same function $\dR\to\dR$.

To be sure, there is a different viewpoint on type theory that takes \emph{terms} as primary objects rather than derivations, regarding the derivability of a judgment such as $x:X\types M:B$ as a \emph{property} of the term $M$, rather than regarding (as we do) the term $M$ as a notation for a particular derivation of $X\types B$.
One reason for this is that terms are (by design) much more concise than derivations, and so if we want to represent type theory in a computer then it is attractive to use terms as the basic objects rather than derivations.
% And if we want to actually program a computer to manipulate terms, or if we want to construct a free category using terms rather than derivations, then we do need to somehow remove the redundancy involved in the choice of variable name.

We will not follow this route.
However, even though we maintain the viewpoint that derivations are primary, there are reasons to think a bit more carefully about the issue of variable names.
Most of these reasons will not arise until \cref{chap:simple}, so we will not say very much about the issue here; but we will at least introduce in our present simple context the two basic ways of dealing with the ambiguity in variable names.

The first method is to decide, once and for all, on a single variable name (say, $x$) to use for \emph{all} our derivations.
Then we cannot write $y:A \types f(y):B$ at all, and so every derivation does determine a unique term.
We call this the \textbf{de Bruijn method}.
(In theories with multiple variables this method becomes more complicated; we will return to this in \cref{chap:simple}.)

The second method is to allow arbitrary choices of variable names (from some standard alphabet), but be aware of the operation of variable renaming.
We say that two terms are \textbf{$\alpha$-equivalent} if they differ by renaming the variable; thus we can say that a derivation determines a unique $\alpha$-equivalence class of terms.
(In theories with ``variable binding'', the definition of $\alpha$-equivalence is likewise more complicated; we will return to this in \cref{sec:catcoprod} and discuss it formally in \cref{chap:dedsys}.)

Of these two methods, the de Bruijn method is theoretically cleaner, and better for implementation in a computer, but tends to detract from readability for human mathematicians.
We will return to discuss these two methods when we have more complicated theories where there is more interesting to say about them.
For now, we continue to use arbitrary variables, remembering that the particular choice of variable name is irrelevant, that derivations are primary, and that terms are just a convenient notation for derivations.

Now that we have such a convenient notation, we can observe that \cref{thm:category-cutadm} is not just a statement about derivability.
Indeed, the proof that we gave is ``constructive'', in the strong sense that it actually determines an \emph{algorithm} for transforming a pair of derivations of $A\types B$ and $B\types C$ into a derivation of $A\types C$.
The inductive nature of the proof means that this algorithm is recursive.
And because terms uniquely represent derivations (modulo $\alpha$-equivalence), it can equivalently be considered an operation on derivable term judgments.

For instance, suppose we start with $x:A \types f(x):B$ and $y:B\types h(g(y)):C$; then the construction proceeds in the following steps.
\begin{itemize}
\item The second derivation ends with an application of $h$, so we apply the inductive hypothesis to $x:A \types f(x):B$ and $y:B\types g(y):D$.
\item Now the second derivation begins with an application of $g$, so we recurse again on $x:A \types f(x):B$ and and $y:B\types y:B$.
\item This time the second derivation is just the identity rule, so the result is the first given derivation $x:A \types f(x):B$.
\item Backing out of the induction one step, we apply $g$ to this result to get $x:A\types g(f(x)):D$.
\item Finally, backing out one more time, we apply $h$ to the previous result to get $x:A\types h(g(f(x))):C$.
\end{itemize}
Intuitively, the result $h(g(f(x)))$ has been obtained by \emph{substituting} the term $f(x)$ for the variable $y$ in the term $h(g(y))$.
Thus, we refer to the operation defined by \cref{thm:category-cutadm} as \textbf{substitution}, and sometimes state \cref{thm:category-cutadm} and its analogues as \textbf{substitution is admissible}.
In general, given $x:A\types M:B$ and $y:B\types N:C$ we denote the substitution of $M$ for $y$ in $N$ by $N[M/y]$ (although unfortunately one also finds other notations in the literature; including, quite confusingly, $[M/y]N$ and $N[y/M]$).

The operation $N[M/y]$ this is ``meta-notation'': the square brackets are not part of the syntax of terms, instead they denote an operation \emph{on} terms.
The proof of \cref{thm:category-cutadm} \emph{defines} the notion of substitution recursively in the following way:
\begin{align}
  y[M/y] &= M\label{eq:category-sub-1}\\
  f(N)[M/y] &= f(N[M/y])\label{eq:category-sub-2}
\end{align}
When terms are regarded as objects of study in their own right, rather than just as notations for derivations, it is common to define substitution as an operation on terms first, and then to state \cref{thm:category-cutadm} as ``if $x:A\types M:B$ and $y:B\types N:C$ are derivable, then so is $x:A\types N[M/y]:C$''.
We instead consider \cref{thm:category-cutadm} as fundamentally an operation on derivations, which we call ``substitution'' especially when representing it using term notation.

Note, though, that because a derivation is represented by a term together with a variable for the antecedent (that is, $x:X\types M:B$ is a notational variant of $x.M:(X\types B)$), technically this operation on derivations has to specify the variables too.
The notation $N[M/y]$ represents only the term part; so the definitions~\eqref{eq:category-sub-1} and~\eqref{eq:category-sub-2} are only complete when combined with the statement that the variable of $N[M/y]$ is the same as that of $M$.

\begin{rmk}
  Substitution is already a place where the use of distinct named variables (and hence $\alpha$-equivalence) makes the exposition substantially clearer for a human reader.
  We even teach our calculus students (or, at least, the author does) that when composing functions $f$ and $g$, it is clearer to use different variables for the two functions, writing $y=f(x)$ but $z=g(y)$ and then plugging $f(x)$ in place of $y$ in the second equation to get $z = g(f(x))$.
  It is possible to get away with using the same variable for the inputs of all functions, as we do in de Bruijn style, but it is much easier to get confused that way.
\end{rmk}

Before proving the initiality theorem, let us first observe that substitution does, in fact, define a category:

\begin{lem}\label{thm:category-subassoc}
  Substitution is associative: given $x:A\types M:B$ and $y:B\types N:C$ and $z:C\types P:D$, we have $P[N/z][M/y] = P[N[M/y]/z]$.
  (This is a literal equality of derivations, or equivalently of terms modulo $\alpha$-equivalence.)
\end{lem}
\begin{proof}
  By induction on the derivation of $P$.
  If it ends with the identity, so that $P=z$, then
  \[P[N/z][M/y] = z[N/z][M/y] = N[M/y] = z[N[M/y]/z] = P[N[M/y]/z] \]
  If it ends with an application of a morphism $f$, so that $P = f(Q)$, then
  \begin{multline*}
    f(Q)[N/z][M/y] = f(Q[N/z])[M/y] = f(Q[N/z][M/y])\\
    = f(Q[N[M/y]/z]) = f(Q)[N[M/y]/z]
  \end{multline*}
  using the inductive hypothesis for $Q$ in the third step.
\end{proof}

\begin{thm}\label{thm:category-initial-2}
  The free category on a directed graph $\cG$ has the same objects as \cG, and its morphisms are the derivations $A\types B$ in the cut-free type theory for categories under \cG (or, equivalently, the derivable term judgments $x:A \types M:B$, modulo $\alpha$-equivalence).
\end{thm}
\begin{proof}
  Let $\F\bCat\cG$ be defined as in the statement, with composition given by substitution constructed as in \cref{thm:category-cutadm}.
  By \cref{thm:category-subassoc}, composition is associative.
  For unitality, we have $y[M/y] = y$ by definition, while $N[x/x] = N$ is another easy induction on the structure of $N$.
  Thus, $\F\bCat\cG$ is a category.

  Now suppose \cA is any category and $P:\cG\to\cA$ is a map of directed graphs.
  We define $P:\F\bCat\cG \to\cA$ by recursion on the rules of the type theory: the identity $x:A\types x:A$ goes to $\idfunc_{P(A)}$, while $x:A\types f(M):B$ goes to $P(f) \circ P(M)$, with $P(M)$ defined recursively.
  Since $x:A\types f(M):B$ is the composite of $x:A\types M:C$ and $y:C\types f(y):B$ in $\F\bCat\cG$, this is the only possible definition that could make $P$ a functor.
  It remains to check that it actually \emph{is} a functor, i.e.\ that it preserves \emph{all} composites; that is, we must show that $P(N[M/y]) = P(N) \circ P(M)$.
  This follows by yet another induction on the derivation of $N$.
\end{proof}

Note that we did not have to impose any equivalence relation on the derivations in this theory.
This suggests a second, more interesting, general statement about categorical logic: it is
{a syntax for generating free categorical structures using derivations from rules}
\emph{that yield elements in canonical form}, eliminating the need for quotients.
This statement is actually too narrow; as we will see later on, type theory is not \emph{just} about canonical forms.
However, canonical forms do play a very important role.

From the perspective of category theory, the reason for the importance of canonical forms is that we can easily decide whether two canonical forms are equal.
In the cut-free type theory for categories, two terms present the same morphism in a free category just when they are literally equal (modulo $\alpha$-equivalence); whereas to check whether two terms are equal in the cut-ful theory we have to remove the identities and reassociate them all to the left or the right.

In fact, a good algorithm for checking equality of terms in the cut-ful theory is to \emph{interpret them into the cut-free theory}!
That is, we note that every rule of the cut-ful theory is admissible in the cut-free theory, and hence eliminable; so any term (i.e.\ derivation) in the cut-ful theory yields a derivation in the cut-free theory.
For instance, to translate the cut-ful term $h\comp{C} ((\id_C\comp{C} g) \comp{B} f)$ into the cut-free theory, we first write it as a derivation
\begin{mathpar}
  \inferrule*[Right=$\circ$]{
    h:(C\types D)\\
    \inferrule*[Right=$\circ$]{
      \inferrule*[Right=$\circ$]{
        \id_C:(C\types C)\\
        g:(B\types C)}
      {(\id_C\comp{C} g):(B\types C)}\\
      f:(A\types B)
    }{((\id_C\comp{C} g) \comp{B} f):(A\types C)}
  }{(h\comp{C} ((\id_C\comp{C} g) \comp{B} f)):(A\types D)}
\end{mathpar}
and then annotate the same derivation by cut-free terms, using substitution for composition:
\begin{mathpar}
  \inferrule*[Right=$\circ$]{
    z:C\types h(z):D\\
    \inferrule*[Right=$\circ$]{
      \inferrule*[Right=$\circ$]{
        z:C\types z:C\\
        y:B\types g(y):C}
      {y:B\types g(y):C}\\
      x:A\types f(x):B
    }{x:A\types g(f(x)):C}
  }{x:A\types h(g(f(x))):D}
\end{mathpar}
Since, as we have proven, both the cut-ful and the cut-free theory present the same free structure, it follows that \emph{two terms in the cut-ful theory are equal modulo $\equiv$ exactly when their images in the cut-free theory are identical}.
Informally, we are just comparing two terms by  ``removing all the identities and the parentheses''; but in a more complicated theory much more can be going on.

In this sense, type theory can be considered to be about solving \emph{coherence problems} in category theory.
In general, the coherence problem for a categorical structure is to decide when two morphisms ``constructed from its basic data'' are equal (or isomorphic, etc.)
For instance, the classical coherence theorem of MacLane for monoidal categories says, informally, that two parallel morphisms constructed from the basic constraint isomorphisms of a monoidal category are \emph{always} equal; whereas the analogous theorem for braided monoidal categories says that they are equal if and only if they have the same underlying braid.
A type-theoretic calculus of canonical forms gives a way to answer this question, by translating a cut-ful theory into a cut-free one, and cut-elimination methods have frequently been used in the proof of coherence theorems; [TODO: some citations].
% ~\cite{blute:ll-coh},~\cite{bcst:natded-coh-wkdistrib}
% https://golem.ph.utexas.edu/category/2008/02/logicians_needed_now.html#c015266
% https://golem.ph.utexas.edu/category/2008/02/logicians_needed_now.html#c015167
% and other references therein
We will not explore this aspect here, however.

\label{sec:identifying-initial-objects}
A related remark is that categorical logic is about \emph{showing that two different categories have the same\footnote{Of course, technically, an object of one category is not generally also an object of another one.  So what we mean is that there is an easy way to transform the initial object of one category into the initial object of another.} initial object}.
The primitive rules of a type theory can be regarded as the ``operations'' of a certain algebraic theory, and the judgments that can be derived from these rules form the initial algebra for this theory, i.e.\ the initial object in a certain category.
(See \cref{chap:dedsys} for a precise statement along these lines.)
The initiality theorems we care about, however, show that these initial objects are \emph{also} initial in some other, quite different, category that is of more intrinsic categorical interest.

\begin{rmk}\label{rmk:admissible-derivable-2}\label{rmk:free}
  This point of view sheds further light on the distinction between derivable and admissible rules mentioned in \cref{rmk:admissible-derivable-1}.
  A derivable rule automatically holds in any model of the ``algebraic theory'' version of a type theory, whereas an admissible rules holds \emph{only in the initial algebra} (or, more generally, free algebras) for this algebraic theory.
  In particular, an arbitrary model of the algebraic rules of the cut-free type theory for categories is not even a category, e.g.\ it may not satisfy the cut rule.

  It can be tempting for a category theorist, upon learning that type theory is a presentation of a certain free structure, to conclude that the emphasis on \emph{free} structures is myopic or of only historical interest, and attempt to generalize to not-necessarily-free algebras over the same theory.
  This temptation should be resisted.
  At best, it leads to neglect of some of the most important and interesting features of type theory, such as cut-elimination, which holds only in free structures.
  At worst, it leads to nonsense, for central type-theoretic notions such as ``bound variable'' (see \cref{sec:catcoprod}) \emph{only make sense} in free structures.
  We will see in \cref{sec:unary-theories} that we can still use type theory to ``present'' categorical objects that are not themselves free (at least, not in the usual sense); but the syntax of types and terms/derivations must still itself be freely generated.
\end{rmk}


\subsection*{Exercises}

\begin{ex}\label{ex:categories-over}
  Let \sM be a fixed category; then we have an induced adjunction between $\bCat/\sM$ and $\bGr/\sM$.
  Describe a cut-free type theory for presenting the free category-over-\sM on a directed-graph-over-\sM, and prove the initiality theorem (the analogue of \cref{thm:category-initial-2}).
  Note that you will have to prove that cut is admissible first.
  \textit{(Hint: index the judgments by arrows in \sM, so that for instance $A\types_\alpha B$ represents an arrow lying over a given arrow $\alpha$ in $\sM$.)}
\end{ex}

\begin{ex}\label{ex:cat-2free}
  Category theorists are accustomed to consider \bCat as a 2-category, but our free category $\F\bCat\cG$ only has a 1-categorical universal property, expressed by the 1-categorical adjunction between $\bCat$ and $\bGr$.
  It is not immediately obvious how it could be otherwise, since unlike \bCat, \bGr is only a 1-category; but there is something along these lines that we can say.
  \begin{enumerate}
  \item Suppose \cG is a directed graph and \cC a category; define a category $\bGr(\cG,\cC)$ whose objects are graph morphisms $\cG\to\cC$ and whose morphisms are an appropriate kind of ``natural transformation''.
  \item Prove that $\bGr(\cG,-)$ is a 2-functor $\bCat\to\bCat$.
  \item Using the cut-free presentation of $\F\bCat\cG$, prove that it is a representing object for this 2-functor.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:nonfree-noadm}
  Regarding the cut-free type theory for categories as describing a multi-sorted algebraic theory, define a particular algebra for this theory that does not satisfy the cut rule.
  Then define another algebra that does admit a ``cut rule'', but in which the resulting ``composition'' is not associative.
\end{ex}


\section{Meet-semilattices}
\label{sec:mslat}

Moving gradually up the ladder of nontriviality, we now consider categories with finite products, or more precisely binary products and a terminal object.
In fact, let us revert back to the posetal world first and consider posets with binary meets and a top element, i.e.\ meet-semilattices.
We will make all this structure algebraic, so that our meet-semilattices are posets (which, recall, is not necessarily skeletal) \emph{equipped with} a chosen top element and an operation assigning to each pair of objects a meet.
We then have an adjunction relating the category \bmSLat of such meet-semilattices (and morphisms preserving all the structure strictly) with the category \bRelGr of relational graphs, and we want to describe the free meet-semilattice on a relational graph \cG.

One new feature this introduces is that the objects of $\F\bmSLat \cG$ will no longer be the same as those of \cG: we need to add a top element and freely apply the meet operation.
In order to describe this type-theoretically, we introduce a new judgment ``$\types A\type$'', meaning that $A$ will be one of the objects of the poset we are generating.
The rules for this judgment are
\begin{mathpar}
  \inferrule{ }{\types \top\type} \and
  \inferrule{\types A\type \\ \types B\type}{\types A\meet B\type}
\end{mathpar}
When talking about type theory under \cG, we additionally include ``axiom'' rules saying that each object of \cG is a type:
\begin{mathpar}
  \inferrule{A\in\cG}{\types A\type}
\end{mathpar}
Note that the premise $A\in \cG$ here is not a judgment; rather it is an ``external'' fact that serves as a precondition for application of thish rule.
Thus it might be more correct to write this rule as
\begin{mathpar}
  \inferrule{ }{\types A\type}\;\text{(if $A\in\cG$)}
\end{mathpar}
but we will generally write such conditions as premises for simplicity.

As an example of the application of these rules, if $A,B\in\cG$ we have a derivation
\begin{mathpar}
  \inferrule*{
    \inferrule*{A\in\cG}{\types A\type}\\
    \inferrule*{\inferrule*{ }{\types \top\type} \\ \inferrule*{B\in\cG}{\types B\type}}{\types \top\meet B\type}
  }{
    \types (A\meet (\top\meet B))\type
  }
\end{mathpar}
so that $A\meet (\top\meet B)$ will be one of the objects of $\F\bmSLat\cG$.

Now we need to describe the morphisms, i.e.\ the relation $\le$ in $\F\bmSLat\cG$.
The obvious thing to do is to assert the universal property of the meet and the top element:
\begin{mathpar}
  \inferrule{ }{A\types \top}\and
  \inferrule{ }{A\meet B \types A}\and
  \inferrule{ }{A\meet B \types B}\and
  \inferrule{A\types B \\ A\types C}{A\types B\meet C}
\end{mathpar}
This works, but it forces us to go back to asserting transitivity/cut.
For instance, if $A,B,C\in \cG$ we have the following derivation:
\begin{mathpar}
  \inferrule*{
    \inferrule*{ }{(A\meet B)\meet C \types A\meet B}\\
    \inferrule*{ }{A\meet B \types A}
  }{
    (A\meet B)\meet C \types A
  }
\end{mathpar}
but there is no way to deduce this without using the cut rule.
Thus, this \textbf{cut-ful type theory for meet-semilattices under \cG} works, but to have a better class of ``canonical forms'' for its relations we would also like a cut-free version.

What we need to do is to treat the ``projections'' $A\meet B \to A$ and $A\meet B\to B$ similarly to how we treated the edges of \cG in \cref{sec:categories}.
However, at this point we have to make a choice of whether to build in postcomposition or precomposition:
\[
\inferrule{A\types C}{A\meet B \types C} \qquad\text{or}\qquad
\inferrule{C\types A\meet B}{C\types A} \quad ?
\]
Both choices work (that is, they make cut admissible), and lead to different kinds of type theories with different properties.
The first leads to a kind of type theory called \textbf{sequent calculus}, and the second to a kind of type theory called \textbf{natural deduction}.
We consider each in turn.

\subsection{Sequent calculus for meet-semilattices}
\label{sec:seqcalc-mslat}

To be precise, for a relational graph \cG, the \textbf{unary sequent calculus for meet-semilattices under \cG} has the following rules (in addition to the rules for the judgment $\types A\type$ mentioned above).
We label each rule on the right to make them easier to refer to later on.
\begin{mathpar}
  \inferrule{A\in \cG}{A\types A}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ X\types A}{X\types B}\;fR\and
  \inferrule{\types A\type}{A\types \top}\;\top R\and
  \inferrule{A\types C \\ \types B\type}{A\meet B \types C}\;\meetL1\and
  \inferrule{B\types C \\ \types A\type}{A\meet B \types C}\;\meetL2\and
  \inferrule{A\types B \\ A\types C}{A\types B\meet C}\;\meetR
\end{mathpar}

There are several things to note about this.
The first is that we have included in the premises some judgments of the form $\types A\type$.
This ensures that whenever we can derive a sequent $A\types B$, that $A$ and $B$ are well-formed as types.
However, we don't need to assume explicitly as premises that \emph{all} types appearing in any sequent are well-formed, only those that are introduced without belonging to any previous sequents; this is sufficient for the following inductive proof.

\begin{thm}\label{thm:seqcalc-mslat-wftype}
  In the unary sequent calculus for meet-semilattices under \cG, if $A\types B$ is derivable, then so are $\types A\type$ and $\types B\type$.
\end{thm}
\begin{proof}
  By induction on the derivation of $A\types B$.
  \begin{itemize}
  \item If it is the $\idfunc$ rule, then $A\in\cG$ and so $\types A\type$.
  \item If it ends with the rule $f$ for some $f\in\cG(A,B)$, then $B\in \cG$ and so $\types A\type$, while $X\types A$ and so $\types X\type$ by the inductive hypothesis.
  \item If it ends with the rule $\top R$, then $\types A\type$ by assumption.
  \item If it ends with the rule $\meetL1$, then $\types B\type$ by assumption, while $\types A\type$ and $\types C\type$ by the inductive hypothesis; thus also $\types A\meet B\type$.
  \item The cases for $\meetL2$ and $\meetR$ are similar.\qedhere
  \end{itemize}
\end{proof}

The second thing to note is that we only assert the identity rule $A\types A$ when $A$ is a \emph{generating object} (also called a \emph{base type}), i.e.\ an object of \cG.
This is sufficient because in the sequent calculus, we can derive the identity rule for any type:

\begin{thm}\label{thm:seqcalc-mslat-idadm}
  In the unary sequent calculus for meet-semilattices under \cG, if $A$ is a type (that is, if $\types A\type$ is derivable), then $A\types A$ is derivable.
\end{thm}
\begin{proof}
  We induct on the derivation of $\types A\type$.
  There are three cases:
  \begin{enumerate}
  \item $A$ is in \cG.  In this case $A\types A$ is an axiom.
  \item $A=\top$.  In this case $\top\types\top$ is a special case of the rule that anything $\types\top$.
  \item $A=B\meet C$ and we have derivations $\sD_B$ and $\sD_C$ of $\types B \type$ and $\types C\type$ respectively.
    Therefore we have, inductively, derivations $\sD_1$ and $\sD_2$ of $B\types B$ and $C\types C$, and we can put them together like this:
    \begin{equation*}
      \inferrule*{
        \inferrule*{
          \inferrule*{\sD_1\\\\\vdots}{B\types B} \\
          \inferrule*{\sD_C\\\\\vdots}{\types C\type}
        }{
          B\meet C \types B
        }\\
        \inferrule*{
          \inferrule*{\sD_2\\\\\vdots}{C\types C}\\
          \inferrule*{\sD_B\\\\\vdots}{\types B\type}
        }{
          B\meet C\types C
        }
      }{
        B\meet C\types B\meet C
      }\qedhere
    \end{equation*}
  \end{enumerate}
\end{proof}

In other words, the general identity rule
\[ \inferrule{\types A\type}{A\types A} \]
is also \emph{admissible}.
This is a general characteristic of sequent calculi.

Next we prove that the cut rule is admissible for this sequent calculus too.

\begin{thm}\label{thm:seqcalc-mslat-cutadm}
  In the unary sequent calculus for meet-semilattices under \cG, if $A\types B$ and $B\types C$ are derivable, then so is $A\types C$.
\end{thm}
\begin{proof}
  By induction on the derivation of $B\types C$.
  \begin{enumerate}
  \item If it is $\idfunc$, then $B=C$.
    Now $A\types C$ is just $A\types B$ and we are done.
  \item If it is $f\in\cG(C',C)$, then we have a derivation of $B\types C'$.
    So by the inductive hypothesis we can derive $A\types C'$, whence also $A\types C$ by the rule for $f$.
  \item If it ends with $\top R$, then $C=\top$.
    Since $A\types B$ is derivable, by \cref{thm:seqcalc-mslat-wftype} $\types A\type$ is also derivable; thus by $\top R$ we have $A\types \top$.
  \item If it ends with $\meetR$, then $C=C_1\meet C_2$ and we have derivations of $B\types C_1$ and $B\types C_2$.
    By the inductive hypothesis we can derive both $A\types C_1$ and $A\types C_2$, to which we can apply $\meetR$ to get $A\types C_1\meet C_2$.
  \item If it ends with $\meetL1$, then $B=B_1\meet B_2$ and we can derive $B_1\types C$.
    We now do a secondary induction on the derivation of $A\types B$.
    \begin{enumerate}
    \item It cannot end with $\idfunc$ or $f$ or $\top R$, since $B=B_1\meet B_2$ is not in $\cG$ and not equal to $\top$.
    \item If it ends with $\meetL1$, then $A=A_1\meet A_2$ and we can derive $A_1\types B$.
      By the inductive hypothesis, we can derive $A_1 \types C$, and hence by $\meetL1$ also $A \types C$.
      The case of $\meetL2$ is similar.
    \item If it ends with $\meetR$, then we can derive $A\types B_1$ and $A\types B_2$.
      Recall that we are also assuming a derivation of $B_1\types C$.
      Thus, by the inductive hypothesis on $A\types B_1$ and $B_1\types C$, we can derive $A\types C$.
      \label{item:mslat-principal-cut}\qedhere
    \end{enumerate}
    The case when it ends with $\meetL2$ is similar.
  \end{enumerate}
\end{proof}

This simple proof already displays many of the characteristic features of a cut-admissibility argument.
The final case~\ref{item:mslat-principal-cut} is called the \textbf{principal case} for the operation $\meet$, when the type $B$ we are composing over (also called the \textbf{cut formula}) is obtained from $\meet$ and both sequents are also obtained from the $\meet$ rules.
In a direct argument for cut-elimination such as that sketched after \cref{thm:category-cutelim}, this case becomes the following transformation on derivations:
\begin{equation*}
  \let\mymeet\meet
  \def\meet{\mathord{\mymeet}}
  \inferrule*[right=cut]{
    \inferrule*[right=$\meetR$]{\labderivof{\sD_1}{A\types B_1} \\ \labderivof{\sD_2}{A\types B_2}}{A\types B_1\meet B_2}\\
    \inferrule*[Right=$\meetL1$]{\labderivof{\sD_3}{B_1\types C}}{B_1\meet B_2\types C}}{A\types C}
  \quad\leadsto\quad
  \inferrule*[right=cut]{\labderivof{\sD_1}{A\types B_1} \\ \labderivof{\sD_3}{B_1\types C}}{A\types C}
\end{equation*}

\begin{rmk}
  It may seem somewhat odd that we can prove the admissibility of all cuts (compositions), but we have to assert identities as a primitive rule for base/generating types.
  This is essentially because we chose to ``build a cut'' into the rule $fR$ that represents the generating arrows.
  If we had not, then we would have to assert ``cuts over base types'' (that is, where the cut formula is an object of \cG) as primitive rules, the way we did in the cut-ful theory of \cref{sec:category-cutful}.
  Put differently, building a cut into $fR$ is essentially the ``morphism version'' of asserting identities primitively for base types.
\end{rmk}

Finally, we have the initiality theorem:

\begin{thm}\label{thm:seqcalc-mslat-initial}
  For any relational graph $\cG$, the free meet-semilattice $\F\bmSLat \cG$ it generates is described by the unary sequent calculus for meet-semilattices under \cG: its objects are the $A$ such that $\types A\type$ is derivable, with $A\le B$ just when $A\types B$ is derivable.
\end{thm}
\begin{proof}
  \cref{thm:seqcalc-mslat-idadm,thm:seqcalc-mslat-cutadm} show that this defines a poset; let us denote it $F\cG$.
  The rule $\top R$ implies that $\top$ is a top element, while the rules $\meetL1$, $\meetL2$, and $\meetR$ imply that $A\meet B$ is a binary meet.
  Therefore, we have a meet-semilattice.
  Moreover, the rules $\idfunc$ and $f$ yield a map of posets $\cG\to F\cG$.

  Now suppose $\cM$ is any other meet-semilattice with a map $P:\cG\to\cM$.
  Recall that a meet-semilattices is equipped with a chosen top element and meet function.
  We extend $P$ to a map from the objects of $F\cG$ by recursion on the construction of the latter, sending $\top$ to the chosen top element of \cM, and $A\meet B$ to the chosen meet in \cM of the (recursively defined) images of $A$ and $B$.
  This is clearly the only possible meet-semilattice map extending $P$, and it clearly preserves the chosen meets and top element, so it suffices to check that it is a poset map.
  This follows by a straightforward induction over the rules for deriving the judgment $A\types B$.
\end{proof}

To finish, we observe that this sequent calculus has another important property.
Inspecting the rules, we see that the operations $\meet$ and $\top$ only ever appear in the \emph{conclusions} of rules.
Each operation $\meet$ and $\top$ has zero or more rules allowing us to introduce it on the right of the conclusion, and likewise zero or more rules allowing us to introduce it on the left.
(Specifically, $\meet$ has two left rules and one right rule, while $\top$ has zero left rules and one right rule.)
This is convenient if we are given a sequent $A\types B$ and want to figure out whether it is derivable: we can choose rules to apply ``in reverse'' by breaking down $A$ and $B$ according to their construction out of $\meet$ and $\top$.

It also tells us nontrivial things about derivations.
For instance, all the primitive rules have the property that every type appearing in their premises also appears as a sub-expression of some type in their conclusion.
Thus, any (cut-free) \emph{derivation} of a sequent $A\types B$ must involves only types appearing as sub-expressions of $A$ and $B$.
This is called the \textbf{subformula property}.

The phrase \emph{sequent calculus}, like \emph{type theory}, is difficult to define precisely, but sequent calculi generally exhibit the properties we have observed in this subsection: admissibility of the identity rule (based on an axiom applying only to base types), admissibility of cut, type operations appearing only in the conclusions of rules, and the subformula property.

\subsection{Natural deduction for meet-semilattices}
\label{sec:natded-mslat}

Now suppose we make the other choice about how to treat projections.
We call this the \textbf{unary natural deduction for meet-semilattices under \cG}; its rules (in addition to those for $\types A\type$) are
\begin{mathpar}
  \inferrule{\types X\type}{X\types X}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ X\types A}{X\types B}\;fI\and
  \inferrule{\types X\type}{X\types \top}\;\top I\and
  \inferrule{X\types B\meet C}{X \types B}\;\meetE1\and
  \inferrule{X\types B\meet C}{X \types C}\;\meetE2\and
  \inferrule{X\types B \\ X\types C}{X\types B\meet C}\;\meetI
\end{mathpar}

We observe first that this theory has the same well-formedness property as the sequent calculus:

\begin{thm}\label{thm:natded-mslat-wftype}
  In the unary natural deduction for meet-semilattices under \cG, if $A\types B$ is derivable, then so are $\types A\type$ and $\types B\type$.\qed
\end{thm}

Unlike the sequent calculus, however, the general identity rule is not admissible: there is no way to derive $A\meet B \types A\meet B$ from $A\types A$ and $B\types B$ without it.
Thus, we assert the $\idfunc$ for all types, not just those coming from \cG.

Cut, however, is still admissible:

\begin{thm}\label{thm:natded-mslat-cutadm}
  In the unary natural deduction for meet-semilattices under \cG, if $A\types B$ and $B\types C$ are derivable, then so is $A\types C$.
\end{thm}
\begin{proof}
  We induct on the derivation of $B\types C$.
  \begin{enumerate}
  \item The cases when it ends with $\idfunc$, $f$, $\top I$, and $\meetI$ are just like those in \cref{thm:seqcalc-mslat-cutadm} for $\idfunc$, $f$, $\top R$, and $\meetR$.
  \item If it ends with $\meetE1$, then we have $B\types C\meet D$ for some $D$.
    Thus, $A\types C\meet D$ by the inductive hypothesis, so $A\types C$ by $\meetE1$.
    The case of $\meetE2$ is similar.\qedhere
  \end{enumerate}
\end{proof}

The proof is noticeably simpler than that of \cref{thm:seqcalc-mslat-cutadm}; we don't need the secondary inner induction.
This is essentially due to the fact that all the rules of this theory involve an \emph{arbitrary} type $X$ on the left (rather than one built using operations such as $\meet$).
Thus, instead of the rules of sequent calculus that introduce operations like $\meet$ and $\top$ on the left and right, we have rules like $\top I$ and $\meetI$ that introduce them on the right, and also rules that \emph{eliminate} them on the right like $\meetE1$ and $\meetE2$.
These properties are characteristic of \emph{natural deduction} theories.
(Later on, in \cref{sec:natded-logic}, we will be able to give a more convincing explanation of the origin of the phrase ``natural deduction''.)

\begin{rmk}
  Because the proof of cut admissibility for natural deduction theories is so much simpler than that for sequent calculus, some people say that the former is ``trivial''.
  Triviality is subjective; but what seems inarguable is that cut-admissibility for natural deduction is \emph{saying something different} than cut-admissibility for sequent calculus.
  The content of cut-admissibility for sequent calculus corresponds more closely to $\beta$-reduction in natural deduction (see \cref{sec:beta-eta}).
  % or to the cut-elimination (``hereditary substitution'') of a canonical/atomic natural deduction (see \cref{cha:canonical}).
  Similarly, the admissibility of the identity rule for sequent calculus corresponds to the $\eta$-conversion rule in natural deduction (see \cref{sec:beta-eta}).
\end{rmk}

Of course, we should also prove the initiality theorem:

\begin{thm}\label{thm:natded-mslat-initial}
  For any relational graph $\cG$, the free meet-semilattice $\F\bmSLat \cG$ it generates is described by the unary natural deduction for meet-semilattices under \cG: its objects are the $A$ such that $\types A\type$ is derivable, with $A\le B$ just when $A\types B$ is derivable.
\end{thm}
\begin{proof}
  Almost exactly like \cref{thm:seqcalc-mslat-initial}.
\end{proof}

\subsection*{Exercises}

\begin{ex}\label{ex:mslat-idem}
  Using the unary sequent calculus for meet-semilattices, prove that $A\meet A \cong A$ for any object $A$ of any meet-semilattice.
  (Recall that meet-semilattices are categories with at most one morphism in each hom-set, so for two objects to be isomorphic it suffices to have a morphism in each direction.)
  Then prove the same thing using the natural deduction.
\end{ex}

\begin{ex}\label{ex:mslat-monoid}
  Using either the sequent calculus or the natural deduction for meet-semilattices (your choice), prove that in any meet-semilattice we have
  \begin{mathpar}
    A\meet \top \cong A\and
    \top \meet A \cong A\and
    A\meet B \cong B\meet A\and
    A\meet (B\meet C) \cong (A\meet B)\meet C\and
  \end{mathpar}
\end{ex}

\begin{ex}\label{ex:mslat-invertible}
  Prove that the rules $\top R$ and $\meetR$ in the unary sequent calculus for meet-semilattices are \emph{invertible}, in the sense that whenever we have a derivation of their conclusions, we also have a derivation of all their premises.
\end{ex}

\begin{ex}\label{ex:jslat}
  Describe a sequent calculus for \emph{join-semilattices} (posets with a bottom element and binary joins), and prove the admissibility and initiality theorems for it.
  The rules for $\bot$ and $\join$ should be exactly dual to the rules for $\top$ and $\meet$.
\end{ex}

\begin{ex}\label{ex:lattices}
  By putting together the rules for meet- and join-semilattices, describe a sequent calculus for \emph{lattices} (posets with a top and bottom element and binary meets and joins), and prove the admissibility and initiality theorems for it.
\end{ex}

\begin{ex}\label{ex:lattices-invertible}
  Prove that in your sequent calculus for lattices from \cref{ex:lattices}, the rules $\top R$, $\meet R$, $\bot L$, and $\join L$ are all invertible in the sense of \cref{ex:mslat-invertible}.
\end{ex}

\begin{ex}\label{ex:seqcalc-poset-fib}
  A map of posets $P:\sA\to\sM$ is called a \emph{(cloven) fibration} if whenever $b\in\sA$ and $x\le P(b)$, there is a chosen $a\in \sA$ such that $P(a)=x$ and $a\le b$ and moreover for any $c\in\sA$, $c\le b$ and $P(c)\le x$ together imply $c\le a$.
  The object $a$ can be written as $x^*(b)$.
  \begin{enumerate}
  \item Given a fixed poset \sM, describe a sequent calculus for fibrations over \sM by adding rules governing the operations $x^*$ to the cut-free theory of \cref{ex:categories-over}.
  \item Prove the initiality theorem for this sequent calculus.
  \item Use this sequent calculus to prove that in any fibration $P:\sA\to\sM$, if we have $b\in \sA$ and $x\le y\le P(b)$, then $x^*(y^*(b))\cong x^*(b)$.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:natded-poset-fib}
  Now describe instead a natural deduction for fibrations over \sM, prove the initiality theorem, and re-prove that $x^*(y^*(b))\cong x^*(b)$ using this theory.
\end{ex}

\begin{ex}\label{ex:mslat-fib}
  Suppose we augment your sequent calculus for fibrations over \sM from \cref{ex:seqcalc-poset-fib} with the following additional rules for ``fiberwise meets''.
  Here $\types A\type_x$ is a judgment indicating that $A$ will be an object of our fibration in the fiber over $x\in\sM$.
  \begin{mathpar}
    \inferrule{ }{\types \top_x \type_x} \and
    \inferrule{\types A\type_x \\ \types B\type_x}{\types A\meet_x B\type_x} \and
    \inferrule{\types A\type_x}{A\types_{x\le y} \top_y}\and
    \inferrule{A\types_{x\le y} C \\ \types B\type_x}{A\meet_x B \types_{x\le y} C}\and
    \inferrule{B\types_{x\le y} C \\ \types A\type_x}{A\meet_x B \types_{x\le y} C}\and
    \inferrule{A\types_{x\le y} B \\ A\types_{x\le y} C}{A\types_{x\le y} B\meet_y C}\and
  \end{mathpar}
  Consider the sequents
  \begin{align*}
    x^*(A\meet_y B) &\types_{x\le x} x^*A \meet_x x^*B\\
    x^*A \meet_x x^*B &\types_{x\le x} x^*(A\meet_y B)
  \end{align*}
  for $x\le y$, $\types A\type_y$, and $\types B\type_y$.
  \begin{enumerate}
  \item Construct derivations of these sequents in the above sequent calculus.
  \item Write down an analoguous natural deduction and derive the above sequents therein.
  \item What categorical structure do you think these type theories construct the initial one of?
    If you feel energetic, prove the initiality theorem.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:poset-bifib}
  A map of posets $P:\sA\to\sM$ is called an \emph{opfibration} if $P\op:\sA\op\to\sM\op$ is a fibration.
  The analogous operation takes $a\in \sA$ and $P(a)\le y$ to a $b\in \sA$ with $P(b)=y$ and $a\le b$ and a universal property; we write this $b$ as $y_!(a)$.
  We say $P$ is a \emph{bifibration} if it is both a fibration and an opfibration.
  Describe a sequent calculus for bifibrations over a fixed \sM, and prove the initiality theorem.
\end{ex}

\begin{ex}\label{ex:poset-bifib-adj}
  Use your sequent calculus from \cref{ex:poset-bifib} to prove that in a bifibration of posets, if $x\le y$ in $\sM$, we have an adjunction $y_! \dashv x^*$.
\end{ex}

\begin{ex}\label{ex:poset-bifib-iso}
  Use your sequent calculus from \cref{ex:poset-bifib} to prove that in a bifibration of posets, if $x\cong y$ in $\sM$, we have an isomorphism $x_! \cong x^*$ (that is, for any $a$ in the fiber over $y$, we have $x_!(a) \cong x^*(a)$).
\end{ex}


\section{Categories with products}
\label{sec:catprod}\label{sec:beta-eta}

Now we move back from posets to categories.
For brevity, by a \textbf{category with products} we will mean a category with specified binary products and a specified terminal object.
Let \bPrCat be the category of such categories with products, and functors preserving them strictly.
Then we have an adjunction relating \bPrCat to \bGr, and we want to describe the left adjoint with a type theory.

As in \cref{sec:natded-mslat}, we could take either the sequent calculus route or the natural deduction route.
Unfortunately, even if we build in enough composition to make cut admissible, in \emph{both} cases we need to impose a further equivalence relation on the derivations, as there are single morphisms that can be derived in multiple ways.
However, the ways in which this happens in the two cases are different.

On one hand, if we have an arrow $f:A\to C$ in a directed graph \cG, then there is a morphism $A\times B \to A \to A\times C$ in the free category-with-products on \cG.
In a sequent calculus, there are two distinct derivations of this morphism:
\begin{mathpar}
  \inferrule*{
    \inferrule*{A\types A}{A\times B \types A}\\
    \inferrule*{\inferrule*[Right=$f$]{A\types A}{A\types C}}{A\times B \types C}
  }{
    A\times B \types A\times C
  }\and
  \inferrule*{
    \inferrule*{A\types A \\ \inferrule*[Right=$f$]{A\types A}{A\types C}}{A\types A\times C}
  }{
    A\times B \types A\times C
  }\and
\end{mathpar}
whereas in a natural deduction there will be only one:
\begin{mathpar}
  \inferrule*{
    \inferrule*{A\times B\types A\times B}{A\times B \types A}\\
    \inferrule*[Right=$f$]{\inferrule*{A\times B\types A\times B}{A\times B\types A}}{A\times B \types C}
  }{
    A\times B \types A\times C
  }
\end{mathpar}
This sort of thing is true quite generally.
A sequent calculus includes both left and right rules, so to derive a given sequent we must choose whether a left or a right rule is to be applied last.
By contrast, both kinds of rules in a natural deduction (introduction and elimination) act on the right, so there is less choice about what rule to apply last.

On the other hand, if we have an arrow $A\to B$ in \cG, then in a natural deduction there are (at least) two derivations of the identity $A\to A$:
\begin{equation}
  \inferrule*{\inferrule*{A\types A \\ \inferrule*{A\types A}{A\types B}}{A\types A\times B}}{A\types A}\qquad\text{and}\qquad
  \inferrule*{ }{A\types A}
  \label{eq:beta-deriv}
\end{equation}
while in a sequent calculus there is only one:
\begin{mathpar}
  \inferrule*{ }{A\types A}
\end{mathpar}
This is also true quite generally.
A natural deduction includes both introduction and elimination rules, so we will always be able to introduce a type and then eliminate it, essentially ``doing nothing''.
By contrast, in a sequent calculus we have ``only introduction rules'' (of both left and right sorts), so this cannot happen.

\begin{rmk}
  In \cref{sec:intro} we mentioned that by presenting free categorical structures without explicit reference to composition, type theory enables us to \emph{define} composition in such a way as to ensure that various desirable properties hold as exact equalities.
  The point here is just that sequent calculus and natural deduction make \emph{different} choices of which properties to ensure by their definitions of composition.
  Roughly speaking, sequent calculus chooses to make the \emph{defining equations} of universal properties hold exactly (e.g.\ the composites of a paired morphism $\pair f g : X \to A\times B$ with $\pi_1$ and $\pi_2$ are exactly $f$ and $g$ respectively); this is what the ``principal case'' of its cut-admissibility proof does.
  On the other hand, natural deduction chooses to make the \emph{naturality}\footnote{This is not actually the origin of the term ``natural deduction'' --- see \cref{sec:natded-logic} for that --- but it serves as a useful mnemonic.} of universal properties hold exactly; the equality between the two distinct sequent calculus derivations above expresses the naturality of pairing $\pair f g : X \to A\times B$ with respect to precomposition by $\pi_1$.
\end{rmk}

In fact, in the simple case of categories with products, there are tricks enabling us to eliminate {both} kinds of redundancy, and thereby do without any ``$\equiv$'' for both the sequent calculus and the natural deduction.
(For sequent calculus the trick is called ``focusing'', and for natural deduction the trick is called ``canonical/atomic terms''.)
However, in even slightly more complicated theories the analogous tricks do not eliminate $\equiv$ completely (though they do reduce its complexity).
Thus, we will not spend any time on these tricks, but instead bite the bullet and deal with $\equiv$.

As was the case in \cref{sec:category-cutful}, it is easier to describe the rules for $\equiv$ if we first introduce terms for derivations.
Thus, we generalize the abstract-variable term syntax of \cref{sec:category-cutadm} with terms for the $\times$ and $\unit$ rules, as shown in \cref{fig:catprod-terms}.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\types X\type}{x:X\types x:X}\;\idfunc\and
    \inferrule{f\in \cG(A,B) \\ x:X\types M:A}{x:X\types f(M):B}\;fI\and
    \inferrule{\types X\type}{x:X\types \ttt:\unit}\;\unit I\and
    \inferrule{x:X\types M:B\times C}{x:X \types \pr1BC(M):B}\;\timesE1\and
    \inferrule{x:X\types M:B\times C}{x:X \types \pr2BC(M):C}\;\timesE2\and
    \inferrule{x:X\types M:B \\ x:X\types N:C}{x:X\types \pair MN:B\times C}\;\timesI
  \end{mathpar}
  \caption{Terms for categories with products}
  \label{fig:catprod-terms}
\end{figure}

Since terms are just a notation for derivations, the general principle for naming derivations by terms is that each rule (being an operation on derivations) should correspond to a ``term operation'' that indicates unambiguously what rule is being applied, what the premises are (by including terms for them), and how they are combined.
This is no different from any other mathematical notation for any other operation.
For instance, in \cref{sec:category-cutful} the composition rule was represented by a (subscripted) binary operation $\circ_B$ combining a pair of terms for the premises.

However, the use of abstract variables complicates matters a bit, because we are only free to describe a notation for the term part (attached to the consequent), whereas the term part only describes a derivation when combined with a variable (attached to the antecedent).
For instance, the two premises $x:X\types M:A$ and $x:X\types N:B$ of the rule $\timesI$ are really $x.M:(X\types A)$ and $x.N\types (X\types B)$, and so the term for the induced derivation $X\types A\times B$ ought to ``pair up'' $x.M$ and $x.N$ somehow; but how would it then be associated to a variable in its own antecedent?

In our present case, we can solve this by using the fact that both premises have the same antecedent \emph{and the same variable}, so we can ``pull that variable out'' of the pair and write $x.\pair M N : (X\types A\times B)$, or $x:X\types \pair M N :A\times B$.
(In other cases, such as \cref{sec:catcoprod}, a more complicated solution is needed.)
But why \emph{do} they have the same variable?
Of course, if we use the de Bruijn method, then they must always have the same variable because all judgments have the same variable.
But otherwise, they might in principle have different variables, and so we might have to rename the variable in one of them (that is, apply an $\alpha$-equivalence) before we can use this notation for $\timesI$.
This is an important general principle.

\begin{princ}\label{princ:term-der-alpha}
  A rule applies equally to any derivations of its premises, but the abstract-variable term notation for that rule may require certain compatibilities between the variables occurring in the premises, which can always be ensured by $\alpha$-equivalence.
\end{princ}

The terms for the other rules $\timesE1,\timesE2,\unit I$ are relatively straightforward.
The superscript type annotations on the projections $\pr1BC$ and $\pr2BC$ are necessary to make type-checking possible, since otherwise it would not be clear from a term such as $x:A \types \pi_1(M):B$ what the type of $M$ should be in the premise.
However, in practice we often omit them.
It is then straightforward to prove the analogue of \cref{thm:category-tad}.

Note how well the natural-deduction choice of ``all rules acting on the right'' matches the use of abstract variables: in all cases we can think of ``applying functions to arguments'' in a familiar way.
It is possible to describe sequent calculus derivations using terms as well, but they are less pretty.
For this reason, \emph{we will henceforth use sequent calculus only for posetal theories}.
% with a brief exception in \cref{sec:focusing}.
The need to impose the identity rule for all types (not just those coming from \cG) also makes perfect sense from the abstract variable standpoint: a variable in any type is also a term of that type.

Now that we have our terms, the desired equivalence between the two derivations~\eqref{eq:beta-deriv} of the identity $A\to A$ can be written as
\begin{equation}
  \pr1AB(\pair M N) \equiv M\label{eq:beta-prodcat-1}
\end{equation}
and of course we should also have
\begin{equation}
  \pr2AB(\pair M N) \equiv N\label{eq:beta-prodcat-2}
\end{equation}
Note that in these equalities we allow $M$ and $N$ to be arbitrary terms.
Categorically speaking, therefore, we are asserting that the maps $X\to A\times B$ induced by the universal property of the product (the $\timesI$ rule) do in fact have the desired composites with the projections.

The other half of the universal property is the uniqueness of maps into a product.
This corresponds to a dual family of simplifications: we want to identify the following derivations of $A\times B\to A\times B$.
\begin{mathpar}
  \inferrule*{
    \inferrule*{\inferrule*{ }{A\times B\types A\times B}}{A\times B \types A} \\
    \inferrule*{\inferrule*{ }{A\times B\types A\times B}}{A\times B \types B}
  }{
    A\times B \types A\times B
  }\and
  \inferrule*{ }{A\times B\types A\times B}
\end{mathpar}
In term syntax, this means that
\begin{equation}
  \pair{\pr1AB(M)}{\pr2AB(M)} \equiv M\label{eq:eta-prodcat}
\end{equation}
In type-theoretic lingo, the equalities~\eqref{eq:beta-prodcat-1} and~\eqref{eq:beta-prodcat-2} are called \textbf{$\beta$-conversion}\footnote{Presumably $\beta$-conversion is so named because it is the ``second most basic'' equivalence relation on terms, with $\alpha$-equivalence (renaming of variables) being the first.
However, there is a significant difference between the two: $\alpha$-equivalent terms represent the \emph{same} derivation, while $\beta$-conversion relates \emph{distinct derivations} (though we generally notate them with terms).} while the equality~\eqref{eq:eta-prodcat} is called an \textbf{$\eta$-conversion}.

For $\unit$ there is no $\beta$-conversion rule, while the $\eta$-conversion rule is
\begin{equation}\label{eq:eta-unit}
  \ttt \equiv M
\end{equation}
for any term $M:\unit$.
These conversions generate an equivalence relation on terms, which we also require to be a congruence for everything else.

We can describe this more formally with an additional judgment ``$x:X\types M\equiv N :A$'', with rules shown in  \cref{fig:catprod-equality}.
Note that in addition to the $\beta$- and $\eta$-conversions, we assert reflexivity, symmetry, and transitivity, and also that all the previous rules preserve equality.
As remarked in \cref{sec:category-cutful}, all our equality judgments $\equiv$ will be equivalence relations with such a congruence property for all the primitive rules.
In general we will not bother to state these ``standard'' rules for $\equiv$, but since this is our first encounter with such a relation involving ``abstract variable'' term syntax we have included them explicitly.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{x:X\types M:A \\ x:X\types N:B}{x:X\types \pr1AB(\pair M N) \equiv M :A}\and
    \inferrule{x:X\types M:A \\ x:X\types N:B}{x:X\types \pr2AB(\pair M N) \equiv N :B}\and
    \inferrule{x:X\types M:A\times B}{x:X\types \pair{\pr1AB(M)}{\pr2AB(M)} \equiv M:A\times B}\and
    \inferrule{x:X\types M:\unit}{x:X\types \ttt\equiv M:\unit}\and
    \inferrule{x:X\types M:A}{x:X \types M\equiv M:A}\and
    \inferrule{x:X\types M\equiv N:A}{x:X\types N\equiv M:A}\and
    \inferrule{x:X\types M\equiv N:A \\ x:X\types N\equiv P:A}{x:X\types M\equiv P:A}\and
    \inferrule{f\in \cG(A,B) \\ x:X\types M\equiv N:A}{x:X\types f(M)\equiv f(N):B}\\
    \inferrule{x:X\types M\equiv N:B\times C}{x:X \types \pr1BC(M)\equiv \pr1BC(N):B}\and
    \inferrule{x:X\types M\equiv N:B\times C}{x:X \types \pr2BC(M)\equiv \pr2BC(N):B}\and
    \inferrule{x:X\types M\equiv M':B \\ x:X\types N\equiv N':C}{x:X\types \pair MN\equiv \pair{M'}{N'}:B\times C}
  \end{mathpar}
  \caption{Equality rules for categories with products}
  \label{fig:catprod-equality}
\end{figure}

This completes the definition of the \textbf{unary type theory for categories with products under \cG}.

\begin{rmk}\label{rmk:beta-reduction}
  Note that unlike the equalities $h\circ (g\circ f) \equiv (h\circ g)\circ f$ from \cref{sec:category-cutful}, the $\beta$- and $\eta$-conversions are intutively ``directional'', with one side being ``simpler'' than the other.
  This suggests that we should be able to ``reduce'' an arbitrary term to a ``simplest possible form'' by successively applying $\beta$- and $\eta$-conversions.
  Such is indeed the case (although for technical reasons the $\eta$-conversion is usually applied in the less intuitive right-to-left direction and called an ``expansion'' rather than a ``reduction'').
  This \emph{process of reduction} (and expansion) belongs to the ``computational'' side of type theory, which (though of course important in its own right) is somewhat tangential to our category-theoretic emphasis, so we will not discuss it in detail.
  % (However, in \cref{cha:canonical} we will directly characterize the \emph{result of reduction}, and thereby implicitly the process thereof.)
\end{rmk}

\begin{thm}\label{thm:catprod-subadm}
  Substitution is admissible in the unary type theory for categories with products under \cG.
  That is, if we have derivations of $x:A\types M:B$ and $y:B \types N:C$, then we have a derivation of $x:A \types N[M/y]:C$.
\end{thm}
\begin{proof}
  The proof is essentially the same as that of \cref{thm:natded-mslat-cutadm}, but we write it out again explicitly with terms present.
  As always, we induct on the derivation of $y:B \types N:C$.
  \begin{enumerate}
  \item If it ends with $\idfunc$, then we can use the given derivation $M$.
  \item If it ends swith $fI$ for $f\in \cG(C',C)$, then we have $y:B \types N':C'$, so by induction we have $x:A\types N'[M/y]:C'$ and hence $x:A\types f(N'[M/y]):C'$.
  \item If it ends with $\unit I$, then by $\unit I$ we have $x:A \types \ttt:\unit$ as well.
  \item If it ends with $\timesE1$, then we have $y:B\types N':C\times C'$, so by induction we have $x:A \types N'[M/y]:C\times C'$, hence $x:A \types \pi_1(N'[M/y]):C$ by $\timesE1$.
    The case for $\timesE2$ is similar.
  \item Finally, if it ends with $\timesI$, we have $y:B\types N_1:C_1$ and $y:B\types N_2:C_2$, so by induction we have $x:A \types N_1[M/y]:C_1$ and $x:A \types N_2[M/y]:C_2$, hence $x:A \types \pair{N_1[M/y]}{N_2[M/y]}:C_1\times C_2$.\qedhere
  \end{enumerate}
\end{proof}

As with \cref{thm:category-cutadm}, this proof can be regarded as \emph{defining} recursively what it means to ``substitute'' $M$ for $y$ in $N$.
The defining clauses are
\begin{align*}
  N[M/y] &= M\\
  (f(N))[M/y] &= f(N[M/y])\\
  \ttt[M/y] &= \ttt\\
  (\pi_1(N))[M/y] &= \pi_1(N[M/y])\\
  (\pi_2(N))[M/y] &= \pi_2(N[M/y])\\
  \pair{N_1}{N_2}[M/y] &= \pair{N_1[M/y]}{N_2[M/y]}
\end{align*}

Similarly, we also have:

\begin{thm}\label{thm:catprod-subcong}
  The relation $\equiv$ is a congruence for substitution in the unary type theory for categories with products under \cG.
  In other words, if we have derivations of $x:X \types M\equiv M':B$ and $y:B \types N\equiv N':C$, then we can derive $x:A \types N[M/y] \equiv N'[M'/y]:C$.\qed
\end{thm}

\begin{thm}\label{thm:catprod-subassoc}
  Substitution is associative in the unary type theory for categories with products under \cG: we have $P[N/z][M/y] = P[N[M/y]/z]$.\qed
\end{thm}

\begin{thm}\label{thm:catprod-initial}
  For any directed graph \cG, the free category-with-products $\F\bPrCat \cG$ it generates is described by the unary type theory for categories with products under \cG: its objects are the $A$ such that $\types A\type$ is derivable, and its morphisms $A\to B$ are the terms $M$ such that $x:A \types M:B$ is derivable, modulo the equivalence relation $\equiv$.\qed
\end{thm}

The proofs of these theorems are left to the reader in \cref{ex:catprod-subcong,ex:catprod-subassoc,ex:catprod-initial}.

\subsection*{Exercises}

\begin{ex}\label{ex:catprod-functor-uniq}
  Suppose we have
  \begin{mathpar}
    f\in\cG(A,B)\and
    g\in\cG(A,C)\and
    h\in\cG(B,D)\and
    k\in\cG(C,E)
  \end{mathpar}
  Consider the following two derivations of $A\types D\times E$.
  Note that both use the admissible cut/substitution rule.
  \begin{mathpar}
    \let\mytimes\times
    \def\times{\mathord{\mytimes}}
    \inferrule*[Right=$\timesI$]{
      \inferrule*[right=cut]{\inferrule*[Right=$f$]{\inferrule*{ }{A\types A}}{A\types B}\\
        \inferrule*[Right=$h$]{\inferrule*{ }{B\types B}}{B\types D}}{A\types D}\\
      \inferrule*[Right=cut]{\inferrule*[Right=$g$]{\inferrule*{ }{A\types A}}{A\types C}\\
        \inferrule*[Right=$k$]{\inferrule*{ }{C\types C}}{C\types E}}{A\types E}
    }{A\types D\times E}\and
    \inferrule*[Right=cut]{
      \inferrule*[right=$\timesI$]{
        \inferrule*[Right=$f$]{\inferrule*{ }{A\types A}}{A\types B}\\
        \inferrule*[Right=$g$]{\inferrule*{ }{A\types A}}{A\types C}
      }{A\types B\times C}\\
      \inferrule*[right=$\timesI$]{
        \inferrule*[Right=$h$]{\inferrule*[Right=$\timesE1$]{
            \inferrule*{ }{B\times C\types B\times C}
          }{B\times C\types B
          }}{B\times C\types D}\\
        \inferrule*[Right=$k$]{\inferrule*[Right=$\timesE2$]{
            \inferrule*{ }{B\times C\types B\times C}
          }{B\times C\types C
          }}{B\times C\types E}
      }{B\times C\types D\times E}
    }{A\types D\times E}\and
  \end{mathpar}
  Write down the terms corresponding to these two derivations and show directly that they are related by $\equiv$.
\end{ex}

\begin{ex}\label{ex:catprod-monoidal}
  Use the type theory for categories with products to prove that in any category with products we have
  \begin{mathpar}
    A\times B \cong B\times A\and
    A\times (B\times C) \cong (A\times B)\times C\and
    A\times \unit \cong A\and
    \unit \times A \cong A.
  \end{mathpar}
  Note that since we are in categories now rather than posets, to show that two types $A$ and $B$ are isomorphic we must derive $x:A\types M:B$ and $y:B\types N:A$ and also show that their substitutions in both orders are equal (modulo $\equiv$) to identities.
\end{ex}

\begin{ex}\label{ex:catprod-subcong}
  Prove \cref{thm:catprod-subcong} ($\equiv$ is a congruence in the unary type theory for categories with products).
\end{ex}

\begin{ex}\label{ex:catprod-subassoc}
  Prove \cref{thm:catprod-subassoc} (substitution is associative in the unary type theory for categories with products).
\end{ex}

\begin{ex}\label{ex:catprod-initial}
  Prove \cref{thm:catprod-initial} (initiality for the unary type theory for categories with products).
\end{ex}

\begin{ex}\label{ex:catfib}
  A functor $P:\sA\to \sM$ is called a \textbf{fibration} if for any $b\in \sA$ and $f:x\to P(b)$, there exists a morphism $\phi:a\to b$ in \sA such that $P(\phi)=f$ and $\phi$ is \emph{cartesian}, meaning that for any $\psi:c\to b$ and $g:P(c)\to x$ such that $P(\psi)=fg$, there exists a unique $\chi:c\to a$ such that $P(\chi)=g$ and $\phi\chi=\psi$.
  The object $c$ is denoted $f^*(b)$.
  \begin{enumerate}
  \item Generalize your natural deduction for fibrations of posets from \cref{ex:natded-poset-fib} to a type theory for fibrations of categories over a fixed base category \sM, with $\beta$- and $\eta$-conversion $\equiv$ rules.
  \item Prove the initiality theorem for this type theory.
  \item Use this type theory to prove that in any fibration $P:\sA\to\sM$:
    \begin{enumerate}
    \item For any $f:x\to y$ in \sM, $f^*$ is a functor from the fiber over $y$ to the fiber over $x$.
    \item For any $B\in \sA$ and $x\xto{f} y \xto{g} P(B)$ in \sM, we have $f^*(g^*(B)) \cong (gf)^*(M)$.
    \end{enumerate}
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:catprod-fib}
  Generalize \cref{ex:mslat-fib} from posets to categories, combining your type theory from \cref{ex:catfib} with the one for categories with products from \cref{sec:beta-eta}.
\end{ex}

\begin{ex}\label{ex:catprod-2free}
  The category $\bPrCat$ is a 2-category whose 2-cells are arbitrary natural transformations (that is, there is no nonvacuous notion of a ``product-preserving natural transformation'').
  Let \cG be a directed graph; as in \cref{ex:cat-2free}, define a 2-functor $\bGr(\cG,-):\bPrCat\to\bCat$, and show that $\F\bPrCat\cG$ is a representing object for it.
  \textit{(Use induction over the derivations of the judgments in its type-theoretic description.)}
\end{ex}

\begin{ex}\label{ex:catprod-flexible}
  \cref{ex:cat-2free,ex:catprod-2free} address one worry that a category theorist might have about the strictness of our constructions.
  Another such worry is that the morphisms in \bPrCat preserve specified products \emph{strictly}, while it is usually more natural in category theory to preserve products only up to isomorphism.
  This is not a problem if our main purpose is to have a syntax to describe objects and morphisms in particular categories with products; indeed, it is exactly what we would want.
  However, for abstract reasons it may be nice to also be able to say something about less strict functors.

  With this in mind, prove that for any \cG, the category with products $\F\bPrCat\cG$ is \emph{semi-flexible} in the sense of~\cite{bkp:2dmonads}: that is, if \cM has chosen products, then every functor $\F\bPrCat\cG \to \cM$ that preserves products in the usual up-to-isomorphism sense is naturally isomorphic to a functor that preserves the chosen products strictly.
  \textit{(Again, use induction over derivations in the type-theoretic description.)}
  Deduce that $\F\bPrCat\cG$ satisfies a universal property relative to the 2-category of categories with products and functors that preserve them up to isomorphism.
\end{ex}

\begin{ex}\label{ex:catprod-induction}
  Here is another way to prove the result of \cref{ex:catprod-flexible}.
  \begin{enumerate}
  \item Use the initiality of $\F\bPrCat\cG$ to show that if \cM has finite products and $Q:\cM\to \F\bPrCat\cG$ preserves finite products strictly, then any map of directed graphs $\cG \to \cM$ that lifts the inclusion $\cG \to \F\bPrCat\cG$ extends to a section $\F\bPrCat\cG \to \cM$ of $Q$ in \bPrCat.\label{item:catprod-induction}
  \item The results of~\cite{bkp:2dmonads} imply that the 2-category of categories with products and functors that preserve products in the usual up-to-isomorphism sense has 2-categorical limits called products, inserters, and equifiers, and the projections of these limits preserve products strictly.
    Use this, and~\ref{item:catprod-induction}, to prove that $\F\bPrCat\cG$ satisfies a universal property relative to this 2-category.
  \end{enumerate}
\end{ex}


\section{Categories with coproducts}
\label{sec:catcoprod}

% [TODO: I'm unsure whether this should be here at all.
% It seems a natural thing to expect, but on the other hand it involves variable binding in terms, which might be easier to introduce first with simply-typed $\lambda$-calculus in \cref{chap:simple}.
% Note that we also need variable binding for monoidal categories, so if we want to do those before cartesian monoidal categories (as also seems natural, since multicategories are simpler than cartesian multicategories) we need to discuss variable binding sooner than $\lambda$-calculus.]

In \cref{ex:jslat}, you obtained a {sequent calculus} for join-semilattices by dualizing the sequent calculus for meet-semilattices.
However, {natural deductions} don't dualize as straightforwardly, due to the insistence that all rules act only on the right.
(Of course, we could dualize them to ``co-natural deductions'' in which all rules act only on the \emph{left}, but that would destroy the familiar behavior of terms on variables, as well as make it tricky to combine left and right universal properties, such as for lattices.)
To describe joins in a natural deduction, we need to ``build an extra cut'' into their universal property:
\begin{mathpar}
  \inferrule{X\types A}{X\types A\join B}\;\joinI1\and
  \inferrule{X\types B}{X\types A\join B}\;\joinI2\and
  \inferrule{X\types A\join B \\ A\types C \\ B\types C}{X\types C}\;\joinE
\end{mathpar}
Note that $\joinE$ is precisely the result of cutting $\joinL$ with an arbitrary sequent:
\begin{mathpar}
  \inferrule*[Right=cut]{X\types A\join B \\ \inferrule*[Right=$\joinL$]{A\types C \\ B\types C}{A\join B\types C}}{X\types C}
\end{mathpar}
We treat the bottom element similarly:
\begin{mathpar}
  \inferrule{X\types \bot \\\types C\type}{X\types C}
\end{mathpar}

Rather than take the time to study join-semilattices, we skip directly to a \textbf{unary type theory for categories with coproducts} in which we care about distinct derivations.
As usual, for this purpose we annotate the judgments with terms, as shown in \cref{fig:catcoprod}.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\types X\type}{x:X\types x:X}\;\idfunc\and
    \inferrule{f\in \cG(A,B) \\ x:X\types M:A}{x:X\types f(M):B}\;fI\and
    \inferrule{x:X \types M:\zero \\ \types C\type}{x:X \types \abort(M):C}\;\zeroE\\
    \inferrule{x:X\types M:A}{x:X\types \inl(M):A + B}\;\plusI1\and
    \inferrule{x:X\types N:B}{X\types \inr(N):A + B}\;\plusI2\and
    \inferrule{x:X\types M:A + B\\u:A\types P:C \\ v:B\types Q:C}{x:X\types \acase AB(M,u.P,v.Q):C}\;\plusE
  \end{mathpar}
  \caption{Unary type theory for categories with coproducts}
  \label{fig:catcoprod}
\end{figure}

Most of the term operations are easy to guess.
The two injections $A\to A+B$ and $B\to A+B$ are named $\inl$ and $\inr$ (``in-left'' and ``in-right''), while the unique morphism $\zero \to C$ is called $\abort$.
However, the term notation for $\plusE$ merits some discussion.

Recall from \cref{sec:catprod} that a term notation should indicate derivations of the premises of the rule by including terms for them, and that in general this can be tricky when using abstract variables because a derivation is only determined by a term together with its variable.
The premises of $\plusE$ are, when paired explicitly with their variables, $x.M:(X\types A+B)$ and $u.P:(A\types C)$ and $v.Q:(B\types C)$, so the term notation should operate on all three of these.
But unlike the case of $\timesI$, now all three premises have different antecendents (and to emphasize this, we have used three different variables as well), so we cannot pull the same variable out of all of them.

However, since the antecedent of the conclusion is $X$, which is also the antecedent of the first premise, we can pull that variable (namely $x$) out to be the variable of the conclusion, leaving the other variables $u$ and $v$ paired with their terms.
This leads us to $x.\acase AB(M,u.P,v.Q)$.
Note that the periods in $u.P$ and $v.Q$ bind more tightly than the commas, so this should be parsed as $x.\acase AB(M,(u.P),(v.Q))$.
The annotation by $A+B$ is to make type-checking possible (but often we will simplify it by writing just $\case$).

The idea behind the name ``$\match$'' is that to ``evaluate'' $\case(M,u.P,v.Q)$ the term $M:A+B$ should be compared to the ``patterns'' $\inl(u)$ and $\inr(v)$, and according to which one it ``matches'' (looks like), we branch into either $P$ or $Q$.
The notation $\abort(M)$ is the nullary case of this: the term $M:\zero$ is matched against all possible ways that a term of type $\zero$ could be constructed --- of which there are none, and so there are no cases to consider.

Categorically, $\plusE$ expresses the universal property of the coproduct as follows.
Recall that morphisms from $A$ to $B$ are (being derivations) represented by variable-term pairs.
Thus the morphisms $A\to C$ and $B\to C$ are represented by the pairs $u.P$ and $v.Q$, while their copairing is a morphism $A+B\to C$ that should be represented by a term involving a variable $y:A+B$.
This is exactly the data included in $y:A+B\types \case(y,u.P,v.Q):C$; the general version with $x:X\types M:A+B$ just comes from building in a cut.

% If we use the de Bruijn method for variables, with unique variable $x$, then we would have $x:A\types P:C$ and $x:B\types Q:C$, and so we \emph{could}, without technical ambiguity, just write $[P,Q]$ leaving the variables as-is.
% However, this would be rather confusing and contrary to mathematical intuition.
% For instance, if $P=f(x)$ and $Q=g(k(x))$ while $M=h(x)$, then $\plusE$ would give us $x:X\types [f(x),g(k(x))](h(x)):C$; but the expressions appearing in the copair $[-,-]$ do not actually ``depend on $x$'' in the same sense that the argument $h(x)$ does.
% If we apply $\alpha$-equivalence instead, we could use different variables for these, such as $x:X\types [f(u),g(k(v))](h(x)):C$, but this is still confusing because it appears to depend on the values of $u$ and $v$.
% (Moreover, when we generalize away from unary type theories in \cref{chap:simple} such a notation would become actually ambiguous.)

% \begin{rmk}
%   In unary type theory it is not strictly necessary to add the prefixes ``$u.$'' and ``$v.$'' --- as long as variables are a syntactic group disjoint from all other symbols, the variable $u$ is determined as the only variable occurring in $P$, and similarly for $v$ in $Q$ (and if no variables occur in $P$, it doesn't matter what variable $u$ we mean).
%   However, starting in \cref{chap:simple} it will be important to keep track of which variable is being bound, so we establish good habits now.
% \end{rmk}

Note that the variables $u$ and $v$, though they ``appear in the term'', are not the variable associated to the antecedent;
we say they are \textbf{bound} variables.
By contrast, the antecedent variable $x$ is \textbf{free}.
Textually each bound variable is associated to a subterm called its \textit{scope}, delimiting the places where it can be referred to;
in $\plusE$ the scope of $u$ is $P$ and the scope of $v$ is $Q$.
In terms of rules, this just means that the variable (or more precisely, its type) may appear as the antecedent of some, but not all, of the premises.

Bound variables are familiar in ordinary mathematics as well.
For instance, the integration variable $x$ in a definite integral $\int_0^2 x^2\,\mathrm{d}x$ is bound, because the value of the expression ``doesn't depend on $x$''; its scope is the expression being integrated (but not the bounds of integration).
Bound variables also occur in function definitions: given an expression such as $x^2$ depending on an unspecified variable $x$, we may write something like $(x\mapsto x^2)$ for ``the function that squares its argument'',\footnote{A more common notation for $(x\mapsto x^2)$ in type theory is $\lambda x.x^2$; see \cref{sec:stlc}.} which is a fully defined object containing $x$ as a bound variable.

It is a common convention in type theory that a prefixed variable followed by a period is bound.
Our writing $x:X\types M:B$ as $x.M:(X\types B)$ follows this convention; the variable $x$ is free in $M$, but bound in $x.M$, since when the term $M$ is paired with its variable $x$ it represents a derivation of $X\types B$ that does not depend on $x$.

If we were using de Bruijn variables, then all the variables $x,u,v$ here would actually be the same, giving for instance $\acase AB(M,x.f(x),x.g(x))$.
Although technically fine, this can be confusing since the same unique variable $x$ also occurs in $M$ itself, but unrelatedly (and with a different type) than its occurrences in the case branches.
In general, a bound variable ``shadows'' any free variable (or ``outer'' bound variable) of the same name, so that in $\acase AB(M,x.f(x),x.g(x))$ the $x$ in $f(x)$ refers to the prefix variable (called $x$) in $x.f(x)$, not to the outer free variable occurring in $M$ (also called $x$).
This makes a precise and general definition of $\alpha$-equivalence in the presence of bound variables rather technically involved; one such definition is given in \cref{sec:alpha}.

However, it is arguably bad mathematical style to use the same name for distinct variables, even if there is technically no ambiguity.
For instance, we encourage calculus students to write $F(x) = \int_0^x f(t)\,\mathrm{d}t$ rather than $F(x) = \int_0^x f(x)\,\mathrm{d}x$, even though technically they mean the same thing.
If we adhere to this informal convention, then we rarely need to worry about technical definitions of $\alpha$-equivalence (unless we are trying to implement mathematics in a computer).

With all of this out of the way, we can now consider the appropriate $\beta$- and $\eta$-conversion rules.
However, we pause first to prove admissibility of cut, i.e.\ to construct substitution, as we will need substitution to state the $\beta$ and $\eta$ rules.

\begin{lem}\label{thm:catcoprod-subadm}
  Substitution is admissible in the unary type theory for categories with coproducts under \cG.
  That is, if we have derivations of $x:A\types M:B$ and $y:B\types N:C$, we can construct a derivation of $x:A\types N[M/y]:C$.
\end{lem}
\begin{proof}
  By induction over derivations.
  As usual for natural deduction theories, there is not much happening: for each rule that might appear last in the derivation of $y:B\types N:C$, we apply the inductive hypothesis to its premises and then re-apply the final rule.
  The defining equations of the substitution operation are:
  \begin{align*}
    y[M/y] &= M\\
    f(N)[M/y] &= f(N[M/y])\\
    \abort(N)[M/y] &= \abort(N[M/y])\\
    \inl(N)[M/y] &= \inl(N[M/y])\\
    \inr(N)[M/y] &= \inr(N[M/y])\\
    \case(N,u.P,v.Q)[M/y] &= \case(N[M/y],u.P,v.Q)\qedhere
  \end{align*}
\end{proof}

Now we proceed to $\beta$- and $\eta$-conversion.
Dualizing the $\beta$-conversion rules for products, the $\beta$-conversion rules for coproducts should say that the map $A+B\to C$ induced by $f:A\to C$ and $g:B\to C$ yields $f$ and $g$ when composed with the coproduct injections.
Recalling that composition is given by substitution, this leads us to write down
\begin{align*}
  \case(\inl(y),u.P,v.Q) &\equiv P[y/u]\\
  \case(\inr(z),u.P,v.Q) &\equiv Q[z/v]
\end{align*}
%The substitutions on the right are just $\alpha$-equivalences, replacing the variables $u,v$ by $y,z$; in a de Bruijn style nothing at all would be happening.
Similarly, the $\eta$-conversion rule\footnote{In fact, for types such as coproducts with a left universal property, there is no consensus on exactly what equality ``$\eta$'' refers to.\label{fn:weak-eta}
  From a categorical point of view this equality is the most natural, since like the $\eta$-conversion rule for products it expresses the uniqueness aspect of the universal property.
  But sometimes $\eta$ is used to refer only to the special case $\case(y,u.\inl(u),v.\inr(v))\equiv y$, which is also analogous to the $\eta$ rule for products in that it says that elements \emph{of the type in question} have a canonical form (a pair in a product or a case-split in a coproduct).
  The stronger property is equivalent to the weaker property combined with ``commuting conversions'' such as $\inl(\case(M,u.P,v.Q)) \equiv \case(M,u.\inl(P),v.\inl(Q))$.%
  % The relationship between these two, which involves ``commuting conversions'', is discussed further in \cref{cha:canonical}.
} should say that morphisms out of a coproduct are determined uniquely by their composites with the projections:
\begin{equation}
  \case(y,u.P[\inl(u)/y],v.P[\inr(v)/y]) \equiv P\label{eq:catcoprod-eta}
\end{equation}
and similarly that morphisms out of the initial object are unique:
\[ \abort(y) \equiv P. \]
In fact, to ensure that $\equiv$ is a congruence, we should ``build in a cut'' to all of these rules, so that the antecedent of the conclusion is an arbitrary type.
Thus the actual generating $\equiv$ rules are those shown in \cref{fig:catcoprod-equiv}.
\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{u:A\types P:C \\ v:B\types Q:C \\ x:X\types M:A}{x:X \types \case(\inl(M),u.P,v.Q) \equiv P[M/u] : C}
    \and
    \inferrule{u:A\types P:C \\ v:B\types Q:C \\ x:X\types N:B}{x:X \types \case(\inr(N),u.P,v.Q) \equiv Q[N/v] : C}
    \and
    \inferrule{x:X \types M:A+B \\ y:A+B \types P:C}{x:X \types \case(M,u.P[\inl(u)/y],v.P[\inr(v)/y]) \equiv P[M/y] : C}
    \and
    \inferrule{x:X\types M:\zero \\ y:\zero \types P:C}{x:X \types \abort(M) \equiv P[M/y] :C}
  \end{mathpar}
  \caption{$\beta$- and $\eta$-conversions for categories with coproducts}
  \label{fig:catcoprod-equiv}
\end{figure}
As usual, we also require $\equiv$ to be an equivalence relation and a congruence for substitution.
This completes the definition of our \textbf{unary type theory for categories with coproducts under \cG}.


As in the dual case, by a \textbf{category with coproducts} we mean a category with specified binary coproducts and a specified initial object, and in the category of such the functors preserve the specified structure strictly.

\begin{thm}\label{thm:catcoprod-initial}
  For any directed graph \cG, the free category with coproducts generated by \cG can be described by the unary type theory for categories with coproducts under \cG: its objects are the derivations of $\types A\type$, and its morphisms $A\to B$ are the derivations of $A\types B$ (or equivalently the derivable term judgments $x:A \types M:B$ modulo $\alpha$-equivalence), modulo the equivalence relation $\equiv$.
\end{thm}
\begin{proof}
  \cref{thm:catcoprod-subadm} defines substitution, and the same sort of induction proves it associative and unital; thus we have a category $\F\bCoprCat\cG$.
  The rules are defined just so as to give this category the structure of coproducts.

  Now if \cM is a category with coproducts and $P:\cG\to\cM$ is a map of graphs, we extend it to the objects of $\F\bCoprCat\cG$ in the obviously unique way (by induction on derivations of $\types A\type$), and to its morphisms by induction on derivations of $x:A\types M:B$.
  All the non-identity rules are defined by a cut with a generator or one of the coproduct structure maps; thus their images in \cM are uniquely determined by the need for the extension to be a functor and to preserve the specified coproducts.
  Finally, we show by a further induction that this extension is actually a functor (preserves all composites).
\end{proof}

% At this point the reader is free (after doing some exercises) to move on directly to \cref{chap:simple}.
% However, if you want to understand $\equiv$ a little more deeply, you may also continue with the somewhat digressive \cref{cha:canonical}.

This seems an appropriate place to introduce some more type-theoretic lingo.
Roughly speaking, types corresponding to objects with ``mapping out'' universal properties, such as $A+B$, are called \textbf{positive}, while types corresponding to objects with ``mapping in'' universal properties, such as $A\times B$, are called \textbf{negative}.
The precise meanings of these terms relate to ``focusing'' and are more directly applicable to sequent calculus than to natural deduction, but they are often used informally in this broad sense, and we will sometimes do the same.
% We have already seen some of the different type-theoretic behavior exhibited by both ``polarities'':
% \begin{itemize}
% \item Negative types have more direct introduction and elimination rules, while positive types are eliminated by a $\match$ that binds variables (possibly zero of them).
% \item In sequent calculus, positive types have invertible left rules.
% \end{itemize}


\subsection*{Exercises}

\begin{ex}\label{ex:catcoprod-functor-uniq}
  This is the dual of \cref{ex:catprod-functor-uniq}, though of course its proof is not dual.
  Suppose we have
  \begin{mathpar}
    f\in\cG(A,C)\and
    g\in\cG(B,D)\and
    h\in\cG(C,E)\and
    k\in\cG(D,E)
  \end{mathpar}
  Here is one (cut-free) derivation of $A+B\types E$.
  \begin{mathpar}
    \inferrule*[Right=$\plusE$]{
      \inferrule*{ }{A+B\types A+B}\\
      \inferrule*[Right=$h$]{\inferrule*[Right=$f$]{\inferrule*{ }{A\types A}}{A\types C}}{A\types E}\\
      \inferrule*[Right=$k$]{\inferrule*[Right=$g$]{\inferrule*{ }{B\types B}}{B\types D}}{B\types E}
    }{A+B\types E}\and
  \end{mathpar}
  Write down another derivation of $A+B\types E$ that ends with the following cut:
  \begin{mathpar}
    \inferrule*[Right=cut]{
      \inferrule*{\vdots}{A+B\types C+D}\\
      \inferrule*{\vdots}{C+D\types E}
    }{A+B\types E}\and
  \end{mathpar}
  Then write down the terms corresponding to the two derivations and show directly that they are related by $\equiv$.
\end{ex}

\begin{ex}\label{ex:cat-prod-coprod}
  Combine the type theories of \cref{sec:catprod,sec:catcoprod} to obtain a unary type theory for categories with both products and coproducts.
  Prove the initiality theorem.
  (Note in particular that no compatibility between the products and coproducts is required or ensured.)
\end{ex}

\begin{ex}\label{ex:cat-opfib}
  A functor $P:\sA\to\sB$ is called an \textbf{opfibration} if $P\op:\sA\op\to\sM\op$ is a fibration (as in \cref{ex:catfib}).
  The dual of $f^*(b)$ is written $f_!(a)$.
  \begin{enumerate}
  \item Write down a type theory for opfibrations and prove the initiality theorem.
    (Remember that we always use natural deduction style when dealing with categories rather than posets, so you can't just dualize \cref{ex:catfib} or categorify \cref{ex:poset-bifib}.
    You will probably want a term syntax such as ``$\match_!$''.)
  \item Use this type theory to prove that $f_!$ is always a functor.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:cat-prod-coprod-uniq}
  Suppose we have
  \begin{mathpar}
    f\in\cG(A,C)\and
    g\in\cG(A,D)\and
    h\in\cG(B,C)\and
    k\in\cG(B,D)
  \end{mathpar}
  Write down two different derivations of $A+B\types C\times D$ in the type theory of \cref{ex:cat-prod-coprod}, one that ends with $\timesI$ and one that ends with $\plusE$.
  Then write down the corresponding terms and show directly that they are identified by $\equiv$.
\end{ex}

\begin{ex}\label{ex:cat-bifib}
  A functor $P:\sA\to\sB$ is called a \textbf{bifibration} if it is both a fibration and an opfibration.
  \begin{enumerate}
  \item Combine the theories of \cref{ex:catfib,ex:cat-opfib} to obtain a type theory for bifibrations.
  \item If you aren't tired of proving initiality theorems yet, do it for this type theory.
  \item Use this type theory to prove that in any bifibration, $f_!$ is left adjoint to $f^*$.
  \end{enumerate}
\end{ex}

% Is this worth it?  I think there's not much to say beyond \label{ex:catprod-2free}.
% \begin{ex}\label{ex:2free-catcoprod}
%   As in \cref{ex:cat-2free,ex:catprod-2free} show that $\F\bCoprCat\cG$ is a representing object for a suitable 2-functor $\bGr(\cG,-):\bCoprCat\to\bCat$.
% \end{ex}

\section{Unary Theories}
\label{sec:unary-theories}

Now that we have a type theory for categories with products, we might hope that we could express in it the proof from \cref{sec:intro} of uniqueness of inverses for monoid objects.
However, the theory as presented in \cref{sec:catprod} is inadequate even to \emph{talk} about monoid objects!

Let us recall briefly how we use initiality theorems to deduce categorical facts.
Suppose we want to prove a theorem of the form ``for any objects $A,B,C,\dots$ and morphisms $f,g,h,\dots$ in a category with products, we have \dots''.
Then we let \cG be a directed graph with one vertex for each object appearing in the theorem and one edge for each morphism, with appropriate source and target, and we reason in the type theory for categories with products under \cG.
Then whenever we have objects and morphisms of the sort described in the theorem in any other category \cM with products, we get a map $\cG\to\cM$, which therefore extends to the free category $\F\bPrCat\cG$ presented by the type theory, and this extension carries our type-theoretic constructions and proofs to categorical ones.

However, although theorems about monoid objects are intuitively of the form ``for any object $A$ and morphisms $m,e$, \dots'', it does not fit into this picture, because the data cannot be described as a directed graph.
The problem is that the source and target of $m$ and $e$ are not \emph{single} objects mentioned in the theorem, but products of them (specifically, $m:A\times A\to A$ and $e:\unit\to A$).
Furthermore, a monoid object contains not just objects and morphisms, but \emph{axioms} about composites of those morphisms, which the type theory of \cref{sec:catprod} is also unable to deal with.

We now present an extension of this theory which does suffice to discuss monoid objects.
It is not the final word on the matter --- as we will see, the type theories of \cref{chap:simple} can do somewhat better --- but it is a first step.

The two problems mentioned above are in fact quite similar.
To talk about a morphism $m:A\times A\to A$, say, using type theory, we need to replace directed graphs with some more general structure including ``arrows'' whose source and target can be products of ``generating objects'' rather than just single ones.
Similarly, to talk about an axiom such as $m(x,m(y,z)) = m(m(x,y),z)$, we need to also include ``generating equalities'' that relate ``pairs of parallel morphisms'', and these morphisms could also be products and composites of generating ones.
(This is an instance of the higher-categorical philosophy that equalities are just a kind of higher morphism.)

We call this structure a \textbf{unary $\times$-theory}.
Its full definition is somewhat involved; we break it up into several levels, each with their own type theory.

\begin{defn}
  A \textbf{unary 0-$\times$-theory} is a set, $\cG_0$.
\end{defn}

The type theory of a unary 0-$\times$-theory consists of the rules for type judgments from \cref{sec:catprod}:
\begin{mathpar}
  \inferrule{A\in\cG_0}{\types A\type}\and
  \inferrule{ }{\types \unit\type} \and
  \inferrule{\types A\type \\ \types B\type}{\types A\times B\type}
\end{mathpar}
We write $\F{\times,0}\cG_0$ for the set of derivable types in this theory.

\begin{defn}
  A \textbf{unary 1-$\times$-theory} consists of a unary 0-$\times$-theory $\cG_0$ together with a set of \emph{arrows} $\cG_1$, each of which is assigned a source and a target that are types in the type theory of $\cG_0$.
\end{defn}

Thus, for instance, a unary 1-$\times$-theory could contain objects $A,B$ and arrows $f:A\times B\to B\times B$ and $g:\unit \to A$.
The type theory of a unary 1-$\times$-theory consists of the rules for term judgments $x:A\types M:B$ from \cref{sec:catprod}.
The generator rule 
\[ \inferrule{x:X\types M:A \\ f\in\cG_1(A,B)}{x:X\types f(M):B} \,f \]
looks the same as before, but now it no longer implies as a side condition that $A$ and $B$ must be base types.
We prove exactly as before that terms have unique derivations and that substitution is admissible and associative.
We write $\F{\times,1}\cG_1(A,B)$ for the set of derivations/terms $x:A\types M:B$ in this theory.

\begin{defn}
  A \textbf{unary 2-$\times$-theory} (or just a \textbf{unary $\times$-theory}, since this is the last step) consists of a unary 1-$\times$-theory $\cG_{\le 1}$ together with a set of \emph{equalities} $\cG_2$, each of which is assigned a source and a target that are derivable terms with the same antecedent and consequent in the type theory of $\cG_{\le 1}$.
\end{defn}

The type theory of a unary 2-$\times$-theory consists of the rules for the equality judgment $\equiv$ from \cref{sec:catprod}, together with a generator rule for equalities:
\begin{equation}
  \inferrule{x:X\types M:A \\ y:A\types P:B \\ y:A\types Q:B \\ e\in \cG_2(P,Q)}{x:X \types P[M/y] \equiv Q[M/y] : B}\label{eq:equality-intro}
\end{equation}
Note that we have built in a substitution on the left.
This ensures that $\equiv$ remains a congruence for substitution on both sides as in \cref{thm:catprod-subcong} (the other side is ensured by the primitive congruence rules for $\equiv$).

To state an initiality theorem for $\times$-theories, we need a category of them.

\begin{defn}\ 
  \begin{enumerate}
  \item A morphism of unary 0-$\times$-theories is just a function $K_0:\cG_0 \to \cH_0$.
    This induces a function $\Kbar_0:\F{\times,0}\cG_0 \to \F{\times,0}\cH_0$ by a simple induction.
  \item A morphism of unary 1-$\times$-theories is a morphism of their underlying unary 0-$\times$-theories together with a function $K_1:\cG_1 \to \cH_1$ respecting sources and targets (relative to $\Kbar_0$).
    This induces maps $\Kbar_1 : \F{\times,1}\cG_1(A,B) \to \F{\times,1}\cH_1(\Kbar_0(A),\Kbar_0(B))$ by another simple induction.
  \item Finally, a morphism of unary 2-$\times$-theories is a morphism of their underlying unary 1-$\times$-theories together with a function $K_2:\cG_2 \to \cH_2$ respecting sources and targets (relative to $\Kbar_1$).
  \end{enumerate}
  We write $\timesthy k$ for the category of $k$-$\times$-theories.
\end{defn}

We also need a forgetful functor $\bPrCat \to \timesthy2$, but this is easiest to construct in stages along with the proof of the initiality theorem.

\begin{defnthm}\label{thm:catprod-thy-initial}\ 
  \begin{enumerate}
  \item The forgetful functor $\fU_0 : \bPrCat \to \timesthy0$ takes a category with products to its set of objects.\label{item:catprod-thy-initial-U0}
  \item Any function $P_0:\cG_0 \to \fU_0\cM$ extends canonically to a function $\Pbar_0:\F{0}\cG_0\to \fU_0\cM$.
    In particular, for any $\cM\in\bPrCat$ there is a canonical map $\ep_0:\F{0}\fU_0\cM\to \fU_0\cM$.\label{item:catprod-thy-initial-F0}
  \item The forgetful functor $\fU_1 : \bPrCat \to \timesthy1$ takes \cM to $\fU_0\cM$ with arrows defined by $\fU_1\cM(A,B) = \cM(\ep_0(A),\ep_0(B))$.\label{item:catprod-thy-initial-U1}
  \item Any morphism $P_1:\cG_1 \to \fU_1\cM$ extends canonically to a morphism $\Pbar_1 : \F{1}\cG_1 \to \fU_1\cM$.
    In particular, for any $\cM\in\bPrCat$ there is a canonical map $\ep_1:\F{1}\fU_1\cM\to \fU_1\cM$.\label{item:catprod-thy-initial-F1}
  \item The forgetful functor $\fU_2 : \bPrCat \to \timesthy2$ takes \cM to $\fU_1\cM$ with an equality in $\fU_2\cM(M,N)$ just when $\ep_1(M)=\ep_1(N)$.\label{item:catprod-thy-initial-U2}
  \item Any morphism $P_2:\cG_2 \to \fU_2\cM$ extends canonically to a morphism $\Pbar_2 : \F{2}\cG_2 \to \fU_2\cM$.
    Moreover, $\F{2}\cG_2$ is a category with products, and there is a unique such extension that lies in \bPrCat.
    Therefore, $\F{2}\cG_2$ is the free category with products generated by $\cG_2$, i.e.\ $\F{2}$ is a left adjoint to $\fU_2$.\label{item:catprod-thy-initial-F2}
  \end{enumerate}
\end{defnthm}
\begin{proof}
  Items~\ref{item:catprod-thy-initial-U0},~\ref{item:catprod-thy-initial-U1}, and~\ref{item:catprod-thy-initial-U2} are just definitions and require no proof.
  Items~\ref{item:catprod-thy-initial-F0}, \ref{item:catprod-thy-initial-F1}, and the first sentence of~\ref{item:catprod-thy-initial-F2} are straightforward inductions on derivations: all the generator rules are interpreted by the given morphism $P_k$, while we know (as in \cref{sec:catprod}) how to interpret all the other rules in a category with products.
  The rest of~\ref{item:catprod-thy-initial-F2} follows as in \cref{sec:catprod}: $\F{2}\cG_2$ is a category with products due to substitution and the $\beta$- and $\eta$-conversions, the fact that $\Pbar_2$ is a morphism in \bPrCat is an induction on derivations, and its uniqueness follows because the interpretations of all the rules are defined in terms of the category-with-products structure.
\end{proof}

This is a bit abstract, so let's consider our motivating example.
The unary $\times$-theory for monoid objects should have one object $A\in\cG_0$ and two arrows, $m\in \cG_1(A\times A,A)$ and $e\in\cG_1(\unit,A)$.
It should have three equalities, for associativity and the two unit laws; but what terms do they relate?
Consider associativity: we need two terms $x:(A\times A)\times A \types M:A$ expressing the two ways to multiply the three components of $x$.
(Note that we also had to chose, arbitrarily, a particular way to associate the triple cartesian product.)
First, of course we have to extract those components using $\pi_1$ and $\pi_2$.
Then we have to multiply them in pairs, noting that since the source of $m$ is $A\times A$ we have to pair things up before we can apply $m$ to them.
This leads us to the terms
\begin{align*}
  x:(A\times A)\times A &\types m(\pair{m(\pair{\pi_1(\pi_1(x))}{\pi_2(\pi_1(x))})}{\pi_2(x)}) : A\\
  x:(A\times A)\times A &\types m(\pair{\pi_1(\pi_1(x))}{m(\pair{\pi_2(\pi_1(x))}{\pi_2(x)})}) : A
\end{align*}
so one of our generating equalities will relate these two terms.
The unit laws are simpler; one relates
\[ x:A \types m(\pair{x}{e(\ttt)}):A \qquad\text{and}\qquad x:A \types x:A \]
and the other relates
\[ x:A \types m(\pair{e(\ttt)}{x}):A \qquad\text{and}\qquad x:A \types x:A \]
This completes the definition of the \textbf{unary $\times$-theory of monoids}.
For brevity we may write its axioms as
\begin{align*}
  x:(A\times A)\times A &\types
                          \begin{multlined}[t]
                            m(\pair{m(\pair{\pi_1(\pi_1(x))}{\pi_2(\pi_1(x))})}{\pi_2(x)}) \\[\jot]
                            \equiv
                            m(\pair{\pi_1(\pi_1(x))}{m(\pair{\pi_2(\pi_1(x))}{\pi_2(x)})}) : A
                          \end{multlined}\\
  x:A &\types m(\pair{x}{e(\ttt)}) \equiv x:A\\
  x:A &\types m(\pair{e(\ttt)}{x}) \equiv x:A
\end{align*}
as long as we don't forget that the actual rule~\eqref{eq:equality-intro} builds in a substitution.

\cref{thm:catprod-thy-initial} implies that a morphism from this theory to $\fU_2\cM$, for any category with products \cM, is exactly a monoid object in \cM.
More explicitly, first we choose to interpret the base type $A$ as some object of \cM, say $\m A$.
Then we extend this interpretation inductively to other types, so that for instance $A\times A$ and $\unit$ are sent to the product $\m A \times \m A$ and the terminal object $1$ in \cM.
Now we choose to interpret the generating arrows $m$ and $e$ as morphisms $\m A \times \m A \to \m A$ and $1\to \m A$ in \cM.
Then we extend this interpretation to other terms, such as those appearing in the above, are sent to other morphisms in \cM.
Unwinding the definitions shows that the two associativity terms really are sent to the two morphisms $(\m A\times \m A)\times \m A \to \m A$ that the associativity of a monoid object should equate, and similarly for the unit terms.
Thus, when in the final step we assert that the generating equalities map to actual equalities, we have precisely specified a monoid object in \cM.

If \cG denotes this theory, then $\F{2}\cG$ is the free category with products it generates.
(In fact it is a special kind of category with products called a \emph{Lawvere theory}; see \cref{sec:cartmoncat}.)
Thus, a monoid object in \cM also induces a functor $\F{2}\cG\to\cM$.
Since derivations of equalities yield equal morphisms in $\F2\cG$, it follows that any equation we can derive in this theory will be true of any monoid object in any category with products.

If we want to reproduce the uniqueness-of-inverses proof from \cref{sec:intro}, we need to further augment our $\times$-theory with two inverse operations $i,j \in \cG_1(A,A)$ and equalities such as
\[ x:X \types m(\pair{x}{i(x)}) \equiv e(\ttt) : A \]
The proof of uniqueness now looks like:
\begin{align*}
  i(x)
  &\equiv m(\pair{i(x)}{e(\ttt)})\\
  &\equiv m(\pair{i(x)}{m(\pair{x}{j(x)})})\\
  &\equiv m(\pair{\pi_1(\pi_1(\pair{\pair{i(x)}{x}}{j(x)}))}{m(\pair{\pi_2(\pi_1(\pair{\pair{i(x)}{x}}{j(x)}))}{\pi_2(\pair{\pair{i(x)}{x}}{j(x)})})})\\
  &\equiv m(\pair{m(\pair{\pi_1(\pi_1(\pair{\pair{i(x)}{x}}{j(x)}))}{\pi_2(\pi_1(\pair{\pair{i(x)}{x}}{j(x)}))})}{\pi_2(\pair{\pair{i(x)}{x}}{j(x)})})\\
  &\equiv m(\pair{m(\pair{i(x)}{x})}{j(x)})\\
  &\equiv m(\pair{e(\ttt)}{j(x)})\\
  &\equiv j(x).
\end{align*}
If it weren't for those two horrific-looking terms in the middle, the rest of the calculation looks pretty much like the argument as we gave it in \cref{sec:intro}.
The horrific-looking terms are there because we can only apply the associativity of $m$ to a single term belonging to $(A\times A)\times A$, so we need to tuple up the terms $i(x)$, $x$, and $j(x)$ and replace them by the projections from that tuple (using $\beta$-conversion).
In \cref{chap:simple} we will remedy this problem by introducing a type theory that allows us to state associativity (and $m$ itself) without tupling.

The construction of unary $\times$-theories, given the ordinary unary type theory for categories with products under directed graphs, while somewhat involved, is quite general\footnote{We will not attempt to make this generality precise, but the approach is similar to the general theory of ``globular computads'' in~\cite{batanin:cptd-fin}.} and can be applied to pretty much any type theory with an initiality theorem.
For instance, by starting from the type theory for categories with coproducts instead, we obtain a notion of ``unary $+$-theory''; while starting from the theory of \cref{ex:cat-prod-coprod} for categories with both products and coproducts, we obtain a notion of ``unary $(\times,+)$-theory''.
We will discuss some more important special cases in \cref{sec:stlc,sec:symmoncat}.

Before leaving this subject, it is worth making a few more remarks about $\times$-theories (all of which generalize to other kinds of theories).
First of all, unlike the case of directed graphs, we can show that \emph{every} category is, up to equivalence, ``freely generated by a theory'' --- specifically, by its underlying theory.

\begin{thm}\label{thm:catprod-thy-esssurj}
  For any category with products \cM, the functor $\F2\fU_2\cM \to \cM$ is an equivalence of categories.
\end{thm}
\begin{proof}
  The definitions of $\fU_0$, $\fU_1$, and $\fU_2$ are cooked up precisely so as to make this functor surjective on objects, full, and faithful respectively.
  That is, $\fU_0\cM$ contains all the objects of \cM, so that $\ep_0:\F{0}\fU_0\cM\to \fU_0\cM$ is surjective --- though not injective, since $\F{0}\fU_0\cM$ contains new types such as ``$A\times B$'' for $A,B\in \cM$, which is distinct from the specified product of the objects $A$ and $B$ in \cM.
  Then for any types $A,B$, $\fU_1\cM(A,B)$ contains all the morphisms of \cM between their images in \cM, so that $\ep_1:\F{1}\fU_1\cM\to \fU_1\cM$ is ``full'' (in the obvious sense) --- though not ``faithful'', since $\F{1}\fU_1\cM$ also contains new terms such as $x:A\types g(f(x)):C$ for $f:A\to B$ and $g:B\to C$ in \cM, which is distinct from $x:A\types (g\circ f)(x):C$ for the composite $g\circ f$ of morphisms in \cM.
  Finally, $\fU_2\cM$ equates all terms whose images in \cM are equal, so that when we quotient by $\equiv$ the functor $\ep_2$ becomes faithful as well.
\end{proof}

In other words, the functor $\F2:\timesthy2 \to \bPrCat$ is bicategorically essentially surjective.
It follows that if we define a 2-category whose objects are unary $\times$-theories and whose hom-categories are induced from those of \bPrCat (regarded as a 2-category) via $\F2$, the result will be biequivalent to \bPrCat.
The morphisms in this 2-category are sometimes called ``translations'' of one theory into another.

Finally, it is worth noting that $\times$-theories as defined here have a minor problem from a type-theoretic standpoint.
If we try to formulate a sequent calculus version of them, we find that cut is no longer admissible, because there is no way to simplify a cut like the following:
\begin{mathpar}
  \inferrule*[right=cut]{\inferrule*[right=$f$]{X\types A}{X\types B\times C} \\
    \inferrule*[Right=$\timesL$]{B\types Y}{B\times C\types Y}}
  {X\types Y}
\end{mathpar}
where $f\in\cG_1(A,B\times C)$.
That is, if we have generating morphisms whose codomains are products, then their composites with projections cannot be ``simplified''.
In a natural deduction theory, this problem manifests as a lack of ``canonical forms'' relative to $\beta$- and $\eta$-conversion.
This is another advantage of the multiple-antecedent type theories to be presented in \cref{chap:simple}: they enable us to describe important theories (such as monoids) while restricting the generating morphisms to have only base types in their domain and codomain.
(Of course we still do require generating equalities relating complex terms rather than just generators.)


\subsection*{Exercises}
\label{sec:exercises}

\begin{ex}\label{ex:catcoprod-comon}
  Write down a $+$-theory for \emph{comonoids} in categories with coproducts: objects $A$ equipped with morphisms $\Delta:A\to A+A$ and $e:A\to \zero$ satisfying coassociativity and counitality axioms.
\end{ex}

\begin{ex}\label{ex:catprodcoprod-field}
  Write down a $\times$-theory for ring objects.
  Then extend it to a $(\times,+)$-theory for field objects.
\end{ex}

\begin{ex}\label{ex:catprod-eckmann-hilton}
  Write down a $\times$-theory for objects having two monoid structures with the same unit and satisfying an internalized version of the ``interchange law'' $m_1(m_2(x,y),m_2(z,w)) = m_2(m_1(x,z),m_1(y,w))$.
  Prove in the resulting type theory that $m_1=m_2$ (this is called the \emph{Eckmann-Hilton argument}).
\end{ex}

\begin{ex}\label{ex:catprod-nearring}
  A ``distributive near-ring'' is like a ring but without the assumption that addition is commutative; thus we have a monoid structure $(\cdot,1)$ and a group structure $(+,0)$ such that $\cdot$ distributes over $+$ on both sides.
  Write down a $\times$-theory for internal distributive near-ring objects in a category with products.
  Then use the resulting type theory to prove that every distributive near-ring object is in fact a ring object, i.e.\ the addition is automatically commutative.
  \textit{(For this reason, in an unqualified ``near-ring'' only one side of distributivity is assumed.)}
\end{ex}


\ChapterExercises


\chapter{Simple type theories}
\label{chap:simple}

At this point we have done about all we can with \emph{unary} type theories, where the antecedent and consequent of each sequent consist only of a single type.
(In fact, most introductions to type theory skip over the unary case altogether, but I find it clarifying to start with cases that are as simple as possible.)
The most common type theories allow finite \emph{lists} of types as the antecedent.
These are the object of study in this chapter; we call them \emph{simple type theories}.
This term is more common in the literature than ``unary'', but perhaps not with the exact meaning we are giving it; the word ``simple'' is primarily used to contrast with ``dependent'' type theories.


\section{Towards multicategories}
\label{sec:why-multicats}

As motivation for the generalization away from unary type theories, we consider a few problems with unary type theory, from a categorical perspective, that all turn out to have this as their solution.
Let's begin by stating some general principles of type theory.
Looking back at the rules of all our type theories, we see that they can be divided into two groups.
On the one hand, there are rules that don't refer specifically to any operation, such as the identity rule $x:X \types x:X$ and the cut rule.
On the other hand, there are rules that introduce or eliminate a particular operation on types, such as product, coproduct, and so on --- and each such rule refers to only \emph{one} operation  (such as $\times,+,f^*$, etc.).

This ``independence'' between the rules for distinct operations is important for the good behavior of type theory.
For instance, notice that many of the exercises have involved combining the rules for multiple previously-studied operations.
If you did some of these exercises, you hopefully got a sense for how these operations tend to coexist ``without interacting'' in the metatheory: e.g.\
when proving the cut-admissibility theorems we essentially just commute the rules for different operations past each other.
This ``modularity'' means that we are always free to add new structure to a theory without spoiling the structure we already had.
We formulate this as a general principle:
\begin{equation}\label{princ:independence}
  \text{Each rule in a type theory should refer to only one operation}.\tag{$\ast$}
\end{equation}
(Like any general principle,~\eqref{princ:independence} is not always strictly adhered to.
For instance, we haven't discussed the $\equiv$ relation that have to be imposed on sequent calculus derivations to present non-posetal free categories, but these turn out to involve ``commutativity'' relations between \emph{different} type operations.
This is arguably another advantage of natural deduction.)

Note that despite~\eqref{princ:independence}, we can often obtain nontrivial results about the interaction of operations.
For instance, in \cref{ex:mslat-monoid} you showed that $A\meet \top\cong A$, even though $\meet$ and $\top$ are distinct operations with apparently unrelated rules.
Similarly, in \cref{ex:mslat-fib,ex:catprod-fib} you showed that $f^*$ preserves $\meet$ and $\times$.
In general, this tends to happen when relating operations whose universal properties all have the same ``handedness''.
For instance, all the operations $\meet,\top,\times,\unit,f^*$ have ``mapping in'' or ``from the right'' universal properties.
Thus, we can expect to compare two objects built using more than one of them by showing that they have the same universal property, and this is essentially what type theory does.

We also observe that in all cases we were able to extract the rules for a given operation from the universal property of the objects it was intended to represent in category theory.
The left and right rules in a sequent calculus, or the introduction and elimination rules in a natural deduction, always expressed the ``two sides'' of a universal property: one of them ``structures the object'' and the other says that it is universal with this structure.
The ``principal case'' of cut admissibility for a sequent calculus, and the $\beta$-conversion equality rule for a natural deduction, both express the fact that morphisms defined by the universal property ``compose with the structure'' to the inputs, e.g.\ a map $X\to A\times B$ defined from $f:X\to A$ and $g:X\to B$ gives back $f$ and $g$ when composed with the product projections.
Similarly, the proof of identity admissibility for a sequent calculus, and the $\eta$-conversion rule for a natural deduction, express the uniqueness half of the universal property.
This leads us to formulate another general principle:
\begin{equation}\label{princ:ump}
  \parbox{3.5in}{\centering The operations in a type theory should correspond categorically to objects with universal properties.}\tag{$\dagger$}
\end{equation}

The point is that from the perspective of unary type theory, these two principles \emph{seem} overly restrictive.
For instance, we remarked above that by expressing universal properties in type theory we can compare operations whose universal properties have the same handedness; but often we are interested in categorical structures satisfying nontrivial relations between objects with universal properties of different handedness.
For instance, in any category with both products and coproducts, there is a canonical map $(A\times B)+(A\times C) \to A\times (B+C)$, and the category is said to be \emph{distributive} if this map is always an isomorphism.
(When the category is a poset, we call it a \emph{distributive lattice}.)
However, as you saw in \cref{ex:lattices,ex:cat-prod-coprod}, if we simply combine the unary type theoretic rules for $\times$ and $+$, we get a type theory for categories with products and coproducts, but no interaction between them.
So unary type theory cannot deal with distributive categories while adhering to~\eqref{princ:independence} and~\eqref{princ:ump}.

Perhaps surprisingly, there \emph{is} a way to present a type theory for distributive categories.
The idea is to move into a world where the product $A\times B$ \emph{also} has a ``mapping out'' universal property, so that we can compare $(A\times B)+(A\times C)$ and $A\times (B+C)$ by saying they have the same universal property.
As we will see, this requires moving to a type theory with multiple antecedents.

This is one motivation.
Another is that we might want to talk about operations whose universal property can't be expressed in unary type theory while adhering to~\eqref{princ:independence}.
For instance, a \emph{cartesian closed category} has exponential objects such that morphisms $X\to Z^Y$ correspond to morphisms $X\times Y\to Z$; but how can we express this without referring to $\times$?
It turns out that the solution is the same.

We might also want to talk about operations that \emph{have} no obvious universal property, obviously violating~\eqref{princ:ump}.
For instance, what about monoidal categories?
In the usual presentation of a monoidal category, the tensor product $A\otimes B$ has no universal property.
It turns out that there is a way to give it a universal property, and this also leads us to higher-ary antecedents.

Finally, there is also another motivation not having anything to do with~\eqref{princ:independence} and~\eqref{princ:ump}: we may want to generalize the ``input data'' \cG from which we generate our free objects.
The unary type theory for categories with products allows us to prove theorems of the form ``in any category with products, for any morphisms $f:A\to B$, $g:C\to D$ \dots'', but it doesn't permit these quantifications to include ``for any $f:A\times B\to C$''.
This is because the type theory generates a free category-with-products from a directed graph \cG whose vertices and edges are the objects and morphisms we hypothesize; but there is no way to express $f:A\times B\to C$ as an edge of \cG, since \cG has no products of objects yet.
This problem turns out to have the same solution as well.

As already mentioned, on the type-theoretic side what we will do in this chapter is allow multiple types in the antecedent of a judgment (but still, for now, only one type in the consequent); we call these \emph{simple type theories}.
In a simple type theory the antecedent is often called the \emph{context}.

On the categorical side, what we will study are \emph{multicategories} of various sorts.
An ordinary multicategory is like a category but whose morphisms can have any finite list of objects as their domain, say $f:(A_1,\dots,A_n) \to B$, with a straightforward composition law.
There are many possible variations on this definition: in a symmetric multicategory the finite lists can be permuted, in a cartesian multicategory we can add unrelated objects and collapse equal ones, and so on.
All of these categorical structures are known as \emph{generalized multicategories}.
There is an abstract theory of generalized multicategories~\cite{cs:multicats,hermida:coh-univ,leinster:higher-opds,burroni:t-cats} that includes these examples and many others, but (at least in the current version of this chapter) we will simply work with concrete examples.

Our approach to the semantics of simple type theory can be summed up in the following additional principle:
\begin{equation}\label{princ:structural}
  \parbox{4.3in}{\centering The shape of the context and the structural rules in a simple type theory should mirror the categorical structure of a generalized multicategory.}\tag{$\ddagger$}
\end{equation}
The \emph{structural rules} are the rules that don't refer to any operation on types, such as identity and cut.
(In this chapter we will meet other structural rules, such as exchange --- corresponding to permutation of domains in a symmetric multicategory -- and contraction and weakening --- corresponding to diagonals and projections in a cartesian multicategory.)
Principle~\eqref{princ:ump} then tells us that the \emph{non-structural} rules (which are sometimes called \emph{logical} rules) should all correspond to objects with universal properties in a generalized multicategory, and principle~\eqref{princ:independence} tells us that each non-structural rule should involve only \emph{one} such object.

In sum, we have the following table of correspondences:
\begin{center}
  \begin{tabular}{c|c}
    Type theory & Category theory\\\hline
    Structural rules & Generalized multicategory\\
    Logical rules & Independent universal properties
  \end{tabular}
\end{center}
Here by ``independent universal properties'' I mean that the universal property of each object can be defined on its own without reference to any other objects defined by universal properties (unlike, for instance, the exponential in a cartesian closed category).

We might formulate one further principle based on our experience in \cref{chap:unary}:
\begin{equation}\label{princ:adm}
  \parbox{4in}{\centering Insofar as possible, structural rules should\\ be admissible rather than primitive.}\tag{\S}
\end{equation}
It is not generally possible to make \emph{all} the structural rules admissible; for instance, we have seen that for sequent calculus we need a primitive identity rule at base types, while for natural deduction we need a primitive identity rule at all types.
However, in \cref{chap:unary} we were always able to make the substitution/cut/composition rule admissible rather than primitive.
That will continue to be the case in this chapter, and we will also strive for admissibility of the new structural rules we introduce (exchange, weakening, and contraction).

Note that together our four principles say that insofar as possible, the ``algebraic operations'' in a categorical structure (such as composition and identities in a category or multicategory, permutation of domains in a symmetric multicategory, and so on) are exactly what we do \emph{not} include as primitive rules in type theory!
To put this differently, recall from the end of \cref{sec:identifying-initial-objects} that the initiality theorems for type theory are about showing that two different categories have the same initial object; we might then say that the effect of the above principles is to ensure that these two categories are \emph{as different as possible}.
This may seem strange, but to paraphrase John Baez\footnote{``Every interesting equation is a lie.''~\cite{baez:why-ncats}}, a proof that two things are the same is more interesting (and more useful) the more different the two things appear to begin with.

Another way to say it is that in category theory we take the algebraic structure of a category as primitive, and use them to define and characterize objects with universal properties; whereas in type theory we take the universal properties as primitive and deduce that the algebraic structure is admissible.
Put this way, one might say that type theory is even more category-theoretic than category theory; for what is more category-theoretic than universal properties?

This all been very abstract, so I recommend the reader come back to this section again at the end of this chapter.
However, for completeness let me point out now that this general correspondence is particularly useful when designing new type theories and when looking for categorical semantics of existing type theories.
On one hand, any type theory that adheres to~\eqref{princ:independence} should have semantics in a kind of generalized multicategory that can be ``read off'' from the shape of its contexts and its structural rules.
On the other hand, to construct a type theory for a given categorical structure, we should seek to represent that structure as a generalized multicategory in which all the relevant objects have independent universal properties; then we can ``read off'' from the domains of morphisms in those multicategories the shape of the contexts and the structural rules of our desired type theory.

We will not attempt to make this correspondence precise in any general way, and in practice it has many tweaks and variations that would probably be exceptions to any putative general theorem; but it is a very useful heuristic.


\section{Introduction to multicategories}
\label{sec:multicats-catth}

From a categorical point of view, a multicategory can be regarded as an answer to the question ``in what kind of structure does a tensor product have a universal property?''
The classical tensor product of vector spaces (or, more generally, modules over a commutative ring) does have a universal property: it is the target of a universal bilinear map.
That is, there is a function $m:V\times W \to V\tensor W$ that is bilinear (i.e.\ $m(x,-)$ and $m(-,y)$ are linear maps for all $x\in V$ and $y\in W$), and any other bilinear map $V\times W \to U$ factors uniquely through $m$ by a linear map $V\tensor W \to U$.
Put differently, $V\tensor W$ is a representing object for the functor $\mathrm{Bilin}(V,W;-) : \bVect \to \bSet$.

Of course, this property determines the tensor product up to isomorphism (though of course one still needs some more or less explicit construction to ensure that such a representing object exists).
However, unlike many universal properties, it is not quite sufficient on its own to show that the tensor product behaves as desired.
In particular, to show that the tensor product is associative, we would naturally like to show that $V\tensor (W\tensor U)$ and $(V\tensor W)\tensor U$ are both representing objects for the functor of \emph{trilinear} maps $\mathrm{Trilin}(V,W,U;-)$, and hence isomorphic.
But this is not an abstract consequence of the fact that each binary tensor product represents bilinear maps;
what we need is a sort of ``relative representability'' such as $\mathrm{Trilin}(V,W,U;-) \cong \mathrm{Bilin}(V\tensor W,U;-)$.

Finally, when we come to prove that these associativity isomorphisms satisfy the pentagon axiom of a monoidal category, we need analogous facts about quadrilinear maps, at which point it is clear that we should be talking about $n$-linear maps for a general $n$.
A multicategory is the categorical context in which to do this: in addition to ordinary morphisms like an ordinary category (e.g.\ linear maps) it also contains $n$-ary maps for all $n\in\dN$ (e.g.\ multilinear maps).

% \subsection{Multicategories and representability}
% \label{sec:multicat-repr}

Formally, just as a category is a directed graph with composition and identities, a multicategory is a \emph{multigraph} with composition and identities.

\begin{defn}\label{defn:multigraph}
  A \textbf{multigraph} \cG consists of a set $\cG_0$ of \emph{objects}, together with for every object $B$ and every finite list of objects $(A_1,\dots,A_n)$ a set of \emph{arrows} $\cG(A_1,\dots,A_n;B)$.
\end{defn}

Note that $n$ can be $0$.
We say that an arrow in $\cG(A_1,\dots,A_n;B)$ is \textbf{$n$-ary}; the special cases $n=0,1,2$ are \emph{nullary}, \emph{unary}, and \emph{binary}.

\begin{defn}
  A \textbf{multicategory} \cM is a multigraph together with the following structure and properties.
  \begin{itemize}
  \item For each object $A$, an identity arrow $\idfunc_A\in\cM(A;A)$.
  \item For any object $C$ and lists of objects $(B_1,\dots,B_m)$ and $(A_{i1},\dots,A_{in_i})$ for $1\le i\le m$, a composition operation
    \begin{align*}
      \cM(B_1,\dots,B_m;C) \times \prod_{i=1}^m \cM(A_{i1},\dots,A_{in_i};B_i) &\too \cM(A_{11},\dots,A_{mn_m};C)\\
      (g,(f_1,\dots,f_m)) &\mapsto g\circ (f_1,\dots,f_m)
    \end{align*}
    [TODO: Picture]
  \item For any $f\in\cG(A_1,\dots,A_n;B)$ we have
    \begin{mathpar}
      \idfunc_B \circ (f) = f\and
      f\circ (\idfunc_{A_1},\dots,\idfunc_{A_n}) = f.
    \end{mathpar}
  \item For any $h,g_i,f_{ij}$ we have
    \begin{mathpar}
      (h\circ (g_1,\dots,g_m))\circ (f_{11},\dots,f_{mn_m}) =
      h \circ (g_1\circ (f_{11},\dots,f_{1n_1}), \dots, g_m \circ (f_{m1},\dots,f_{mn_m}))
    \end{mathpar}
  \end{itemize}
\end{defn}

The objects and unary arrows in a multicategory form a category; indeed, a multicategory with only unary arrows is exactly a category.
Vector spaces and multilinear maps, as discussed above, are a good example to build intuition.

While the above definition is the most natural one from a certain categorical perspective, there is another equivalent way to define multicategories.
If in the ``multi-composition'' $g\circ (f_1,\dots,f_m)$ all the $f_j$'s for $j\neq i$ are identities, we write it as $g \circ_i f_i$.
We may also write it as $g\circ_{B_i} f_i$ if there is no danger of ambiguity (e.g.\ if none of the other $B_j$'s are equal to $B_i$).
Thus we have \textbf{one-place composition} operations
\begin{multline*}
  \circ_i : \cM(B_1,\dots,B_n;C) \times \cM(A_1,\dots,A_m;B_i) \\
  \too \cM(B_1,\dots,B_{i-1},A_1,\dots,A_m,B_{i+1},\dots,B_n;C)
\end{multline*}
that satisfy the following properties:
\begin{itemize}
\item $\idfunc_B \circ_1 f = f$ (since $\idfunc_B$ is unary, $\circ_1$ is the only possible composition here).
\item $f\circ_i \idfunc_{B_i} = f$ for any $i$.
\item If $h$ is $n$-ary, $g$ is $m$-ary, and $f$ is $k$-ary, then
  \[ (h \circ_i g) \circ_{j} f=
  \begin{cases}
    (h\circ_j f)\circ_{i+k-1} g &\quad \text{if } j < i\\
    h\circ_i (g\circ_{j-i+1} f) &\quad \text{if } i \le j < i+m\\
    (h\circ_{j-m+1} f)\circ_{i} g &\quad \text{if } j \ge i+m
  \end{cases}
  \]
  [TODO: Picture]
  We refer to the second of these equations as \emph{associativity}, and to the first and third as \emph{interchange}.
\end{itemize}
Conversely, given one-place composition operations satisfying these axioms, one may define
\[ g\circ (f_1,\dots,f_m) = (\cdots((g \circ_m f_m) \circ_{m-1} f_{m-1}) \cdots \circ_2 f_2) \circ_1 f_1 \]
to recover the original definition of multicategory.
The details can be worked out by the interested reader (\cref{ex:multicat-defns}) or looked up in a reference such as~\cite{leinster:higher-opds}.

With multicategories in hand, we can give an abstract version of the characterization of the tensor product of vector spaces using multilinear maps.

\begin{defn}\label{defn:multicat-tensor}
  Given objects $A_1,\dots,A_n$ in a multicategory \cM, a \textbf{tensor product} of them is an object $\bigtensor_i A_i$ with a morphism $\chi:(A_1,\dots,A_n) \to \bigtensor_i A_i$ such that all the maps $(-\circ_i \chi)$ are bijections
  \[ \cM(B_1,\dots,B_k,\textstyle\bigtensor_i A_i,C_1,\dots,C_m;D) \toiso \cM(B_1,\dots,B_k,A_1,\dots,A_n,C_1,\dots,C_m;D). \]
\end{defn}

When $n=2$ we write a binary tensor product as $A_1\tensor A_2$.
When $n=0$ we call a nullary tensor product a \textbf{unit object} and write it as $\one$.
When $n=1$ a unary tensor product is just an object isomorphic to $A$.

In keeping with the usual ``biased'' definition of monoidal category (which has a binary tensor product and a unit object, with all other tensors built out of those), we will say that a multicategory is \textbf{representable} if it is equipped with a chosen unit object and a chosen binary tensor product for every pair of objects.
Let $\bRepMCat$ denote the category of representable multicategories and functors that preserve the chosen tensor products strictly.

\begin{thm}\label{thm:multicat-repr}
  The category \bRepMCat is equivalent to the category \bMonCat of monoidal categories.
\end{thm}
\begin{proof}
  It is easy to show that $(A_1\tensor A_2)\tensor A_3$ and $A_1 \tensor (A_2\tensor A_3)$, if they both exist, are both a ternary tensor product $\bigtensor_{i=1}^3 A_i$, and hence canonically isomorphic.
  Similarly, $A\tensor \one$ and $\one\tensor A$ are unary tensor products, hence canonically isomorphic to $A$.
  The coherence axioms follow similarly; thus any representable multicategory gives rise to a monoidal category.

  Conversely, any monoidal category \cM has an underlying multicategory defined by $\cM(A_1,\dots,A_n;B) = \cM(\bigtensor_i A_i;B)$, where $\bigtensor_i A_i$ denotes some tensor product of the $A_i$'s such as $(\cdots((A_1\tensor A_2)\tensor A_3)\cdots )\tensor A_n$.
  The coherence theorem for monoidal categories implies that the resulting hom-sets $\cM(A_1,\dots,A_n;B)$ are independent, up to canonical isomorphism, of the choice of bracketing.
  We can similarly use the coherence theorem to define the composition of this multicategory, and to show that the given tensor product and unit make it representable.
  Finally, the constructions are clearly inverse up to natural isomorphism.
\end{proof}

There are plenty of good references on multicategories, such as~\cite{hermida:multicats,leinster:higher-opds}.
We end this section by discussing limits and colimits in multicategories, which are a bit less well-known.

% \subsection{Limits and colimits in multicategories}
% \label{sec:lim-colim-multicat}

We say that an object $\unit$ of a multicategory is \textbf{terminal} if for any $A_1,\dots, A_n$ there is a unique morphism $(A_1,\dots, A_n)\to \unit$.
Similarly, a \textbf{binary product} of $A$ and $B$ in a multicategory is an object $A\times B$ with projections $A\times B \to A$ and $A\times B\to B$, composing with which yields bijections
\[ \cM(C_1,\dots,C_n;A\times B) \too \cM(C_1,\dots,C_n;A) \times \cM(C_1,\dots,C_n;B)\]
for any $C_1,\dots,C_n$.
We will say a multicategory \textbf{has products} if it has a specified terminal object and a specified binary product for each pair of objects.
The following is entirely straightforward.

\begin{thm}\label{thm:multicat-prod}
  A monoidal category has products (in the sense of \cref{sec:catprod}) if and only if its underlying multicategory has products, and this yields an equivalence of categories.\qed
\end{thm}

Colimits in a multicategory are a bit more subtle.
We define a \textbf{binary coproduct} in a multicategory \cM to be an object $A+B$ with injections $A\to A+B$ and $B\to A+B$ composing with which induces bijections
\begin{multline*}
  \cM(C_1,\dots,C_n,A+B,D_1,\dots,D_m;E) \toiso\\
  \cM(C_1,\dots,C_n,A,D_1,\dots,D_m;E) \times \cM(C_1,\dots,C_n,B,D_1,\dots,D_m;E).
\end{multline*}
for all $C_1,\dots,C_n$ and $D_1,\dots, D_M$ and $E$.
Similarly, an \textbf{initial object} is an object $\zero$ such that for any $C_1,\dots,C_n$ and $D_1,\dots, D_M$ and $E$, there is a unique morphism $(C_1,\dots,C_n,\zero,D_1,\dots,D_m)\to E$.
We say a multicategory \textbf{has coproducts} if it has a specified binary coproduct for each pair of objects and a specified initial object.

By a \textbf{distributive monoidal category}, we mean a monoidal category thath has coproducts (in the sense of \cref{sec:catcoprod}) and such that the canonical maps
\begin{mathpar}
(A\tensor B)+(A\tensor C)\to A\tensor(B+C)\and
(B\tensor A)+(C\tensor A)\to (B+C)\tensor A\and
\zero \to A\tensor \zero\and
\zero\to \zero\tensor A
\end{mathpar}
are isomorphisms.
(A \textbf{distributive category} is a distributive cartesian monoidal category.)

\begin{thm}\label{thm:multicat-coprod}
  A monoidal category is distributive if and only if its underlying representable multicategory has coproducts, and this yields an equivalence of categories.
\end{thm}
\begin{proof}
  If \cM is distributive, then by induction we have
  \[ \textstyle
  \Big((\bigotimes_i C_i)\tensor A \tensor (\bigotimes_j D_j)\Big) +
  \Big((\bigotimes_i C_i)\tensor B \tensor (\bigotimes_j D_j)\Big)
  \;\cong\;
  (\bigotimes_i C_i)\tensor (A+B) \tensor (\bigotimes_j D_j)
  \]
  and similarly
  \[ \zero \;\cong\; \textstyle (\bigotimes_i C_i)\tensor \zero \tensor (\bigotimes_j D_j).\]
  Since the morphisms in the underlying multicategory of \cM are maps out of iterated tensor products in \cM, these isomorphisms imply that the latter has coproducts.

  Conversely, if the underlying multicategory of \cM has coproducts, then taking $n=m=0$ in their universal property we see that the ordinary category \cM has coproducts.
  Moreover, the universal property with $n=1$ and $m=0$ applied to the composites
  \begin{mathpar}
    (C,A) \to C\tensor A \to (C\tensor A)+(C\tensor B)\and
    (C,B) \to C\tensor B \to (C\tensor A)+(C\tensor B)
  \end{mathpar}
  gives a map $(C,A+B) \to (C\tensor A)+(C\tensor B)$, and the universal property of $\tensor$ then yields a map
  \[C\tensor(A+B) \to (C\tensor A)+(C\tensor B). \]
  It is straightforward to show that this is an inverse to the canonical map that exists in any monoidal category with coproducts, and similarly in the other cases; thus \cM is distributive.
  Finally, one can check that these constructions are inverse.
\end{proof}


\subsection*{Exercises}

\begin{ex}\label{ex:multicat-defns}
  Prove that the definitions of multicategory in terms of multi-composition and one-place composition are equivalent, in the strong sense that they yield isomorphic categories of multicategories.
\end{ex}

\begin{ex}\label{ex:multicat-repr}
  Fill in the details in the proof of \cref{thm:multicat-repr}.
\end{ex}

\begin{ex}\label{ex:mcat-lax-func}
  Show that the category whose objects are representable multicategories but whose morphisms are \emph{arbitrary} functors of multicategories is equivalent to the category of monoidal categories and \emph{lax} monoidal functors.
\end{ex}

\begin{ex}\label{ex:mcat-strong-func}
  Show that the category of representable multicategories and functors that ``preserve tensor products'', in the sense that if $\chi:(A_1,\dots,A_n) \to \bigtensor_i A_i$ is a tensor product then $F(\chi)$ is also \emph{a} tensor product, is equivalent to the category of monoidal categories and \emph{strong} monoidal functors.
\end{ex}

\begin{ex}\label{ex:multicat-coprod}
  Fill in the details in the proof of \cref{thm:multicat-coprod}.
\end{ex}


\section{Multiposets and monoidal posets}
\label{sec:multiposets-monpos}

\subsection{Multiposets}
\label{sec:multiposets}

We begin our study of type theory for multicategories with the posetal case.
A \textbf{multiposet} is a multicategory in which each set $\cM(A_1,\dots,A_n;B)$ has at most one element.
We consider the adjunction between the category \bMPos of multiposets and the category \bRelMGr of \emph{relational multigraphs}, i.e.\ sets of objects equipped with an $n$-ary relation ``$(a_1,\dots,a_{n-1})\le b$'' for all integers $n\ge 1$.
We would like to construct the free multiposet on a relational multigraph \cG using a type theory.

Its objects, of course, will be those of \cG, so we do not yet need a type judgment.
We represent its relations using a judgment written
\[A_1,A_2,\dots,A_n \types B.\]
As is customary, we use capital Greek letters such as $\Gamma$ and $\Delta$ to stand for finite lists (possibly empty) of types; thus the above general judgment can also be written $\Gamma\types B$.
We write ``$\Gamma_1,\Gamma_2,\dots,\Gamma_n$'' for the concatenation of such lists, and we also write for instance ``$\Gamma,A,\Delta$'' to indicate a list containing the type $A$ somewhere the middle.

At the moment, the only rules for this judgment will be identities and those coming from \cG.
Based on the lessons we learned from unary type theory, we represent the latter in Yoneda-style.
\begin{mathpar}
  \inferrule{ }{A\types A}\and
  \inferrule{(A_1,\dots,A_n \le B)\in\cG \\ \Gamma_1\types A_1 \\ \dots \\ \Gamma_n \types A_n}{\Gamma_1,\dots,\Gamma_n\types B}
\end{mathpar}
We call this the \textbf{cut-free type theory for multiposets under \cG}.
Note that we use the ``multi-composition'' in Yoneda-ifying the relations in \cG; this is absolutely necessary for the admissibility of cut.
By contrast, it is traditional to formulate the cut rule itself in terms of the one-place compositions:

\begin{thm}\label{thm:multiposet-cutadm}
  In the cut-free type theory for multiposets under \cG, the following cut rule is admissible: if we have derivations of $\Gamma\types A$ and of $\Delta,A,\Psi\types B$, then we can construct a derivation of $\Delta,\Gamma,\Psi\types B$.
\end{thm}
\begin{proof}
  We induct on the derivation of $\Delta,A,\Psi\types B$.
  If it is the identity rule, then $A=B$ and $\Delta$ and $\Psi$ are empty, so our given derivation of $\Gamma\types A$ is all we need.
  Otherwise, it comes from some relation $A_1,\dots,A_n \le B$ in \cG, where we have derivations of $\Gamma_i \types A_i$.
  Since then $\Delta,A,\Psi = \Gamma_1,\dots,\Gamma_n$, there msut be an $i$ such that $\Gamma_i = \Gamma_i',A,\Gamma_i''$, while $\Delta = \Gamma_1,\dots,\Gamma_{i-1},\Gamma_i'$ and $\Psi = \Gamma_i'',\Gamma_{i+1},\dots,\Gamma_n$.
  Now by the inductive hypothesis, we can construct a derivation of $\Gamma_i',\Gamma,\Gamma_i''\types A_i$.
  Applying the rule for $A_1,\dots,A_n \le B$ again, with this derivation in place of the original $\Gamma_i \types A_i$, gives the desired result.
\end{proof}

However, we can also prove admissibility of ``multi-cut'' directly:

\begin{thm}\label{thm:multiposet-multicutadm}
  In the cut-free type theory for multiposets under \cG, the following multi-cut rule is admissible: if we have derivations of $\Psi_i\types A_i$ for $1\le i\le n$, and also $A_1,\dots,A_n \types B$, then we can construct a derivation of $\Psi_1,\dots,\Psi_n\types B$.
\end{thm}
\begin{proof}
  If $A_1,\dots,A_n \types B$ ends with the identity rule, then $n=1$ and $A_1=B$, whence $\Psi_1\types A_1$ is what we want.
  Otherwise, it comes from some relation $C_1,\dots,C_m \le B$, where we have a partition $A_1,\dots,A_n = \Gamma_1,\dots,\Gamma_m$ and derivations of $\Gamma_j \types C_j$.
  Let $\Phi_j$ be the concatenation of all the $\Psi_i$ such that $A_i \in \Gamma_j$; then by the inductive hypothesis we can get $\Phi_j\types C_j$.
  Applying the generator rule again, we get $\Phi_1,\dots,\Phi_m \types B$, which is the desired result.
\end{proof}

The notation is certainly a bit heavier when constructing multi-cuts directly.
However, as we will see later on, in more complicated situations there are definite advantages to the latter.

\begin{thm}\label{thm:multiposet-initial}
  For any relational multigraph \cG, the free multiposet it generates has the same objects, and the relation $(A_1,\dots,A_n)\le B$ holds just when the sequent $A_1,\dots,A_n\types B$ is derivable in the cut-free type theory for multiposets under \cG.
\end{thm}
\begin{proof}
  \cref{thm:multiposet-cutadm}, together with the identity rule, tells us that this defines a multiposet $\F\bMPos\cG$.
  If $\cM$ is any other multiposet with a map $P:\cG\to\cM$ of relational multigraphs, then since the objects of $\F\bMPos\cG$ are those of \cG, there is at most one extension of $P$ to $\F\bMPos\cG$.
  It suffices to check that the relations in $\F\bMPos\cG$ hold in $\cM$; but this is clear since $\cM$ is a multiposet and the only rules are an identity and a particular multi-transitivity.
\end{proof}

Now we augment the type theory for multiposets with operations representing a tensor product.
Since the tensor product now has a universal property, this is essentially straightforward.
First of all, we need a type judgment $\types A\type$, with unsurprising rules:
\begin{mathpar}
  \inferrule{A\in\cG}{\types A\type}\and
  \inferrule{ }{\types \one\type}\and
  \inferrule{\types A\type \\ \types B\type}{\types A\tensor B\type}
\end{mathpar}

Second, in addition to the rules from \cref{sec:multiposets}, we have rules for $\tensor$.
Once again we need to make a choice between sequent calculus and natural deduction; we treat these one at a time.

\subsection{Sequent calculus for monoidal posets}
\label{sec:seqcalc-monpos}

The additional rules for the \textbf{sequent calculus for monoidal posets under \cG} are shown in \cref{fig:seqcalc-monpos}.
Since $A\tensor B$ has a ``mapping out'' universal property like a coproduct, the left rule expresses this universal property.
The right rule should be the universal relation $A,B\types A\tensor B$, but we have to Yoneda-ify it using the multicomposition.
The rules for $\one$ are similar.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\Gamma,A,B,\Delta\types C}{\Gamma,A\tensor B,\Delta\types C}\;\tensorL\and
    \inferrule{\Gamma\types A \\ \Delta\types B}{\Gamma,\Delta\types A\tensor B}\;\tensorR\and
    \inferrule{\Gamma,\Delta\types A}{\Gamma,\one,\Delta\types A}\;\one L\and
    \inferrule{ }{\types \one}\;\one R
  \end{mathpar}
  \caption{Sequent calculus for monoidal posets}
  \label{fig:seqcalc-monpos}
\end{figure}

Note the presence of the additional contexts $\Gamma$ and $\Delta$ in $\tensorL$ and $\one L$, which corresponds to the strong universal property of a tensor product in a multicategory referring to $n$-ary arrows for all $n$.

\begin{thm}\label{thm:monpos-identity}
  The general identity rule is admissible in the sequent calculus for monoidal posets under \cG: if $\types A\type$ is derivable, then so is $A\types A$.
\end{thm}
\begin{proof}
  By induction on the derivation of $\types A\type$.
  If $A\in \cG$, then $A\types A$ is an axiom.
  If $A=\one$, then $\one\types \one$ has the following derivation:
  \begin{mathpar}
    \inferrule*[Right=$\one L$]{\inferrule*[Right=$\one R$]{ }{\types \one}}{\one\types \one}
  \end{mathpar}
  And if $A=B\tensor C$, by the inductive hypothesis we have derivations $\sD_B$ and $\sD_C$ of $B\types B$ and $C\types C$, which we can put together like so:
  \begin{equation*}
    \inferrule*[Right=$\tensorL$]{
      \inferrule*[Right=$\tensorR$]{
        \inferrule*{\sD_B\\\\\vdots}{B\types B}\\
        \inferrule*{\sD_C\\\\\vdots}{C\types C}
      }{
        B,C\types B\tensor C
      }
    }{
      B\tensor C\types B\tensor C
    }\qedhere
  \end{equation*}
\end{proof}

The proof of cut-admissibility in this case has two new features we have not seen before.

\begin{thm}\label{thm:monpos-cutadm}
  Cut is admissible in the sequent calculus for monoidal posets under \cG: if we have derivations of $\Gamma\types A$ and of $\Delta,A,\Psi\types B$, then we can construct a derivation of $\Delta,\Gamma,\Psi\types B$.
\end{thm}
\begin{proof}
  % As always, we induct on the derivation of $\Delta,A,\Psi\types B$.
  If the derivation of $\Delta,A,\Psi\types B$ ends with the identity rule or a generating relation from \cG, we proceed just as in \cref{thm:multiposet-cutadm}.
  It cannot end with a $\one R$.
  If it ends with a $\tensorR$, we use the inductive hypotheses on its premises and apply $\tensorR$ again.

  The cases when it ends with a left rule introduce one new feature.
  Suppose it ends with a $\one L$.
  If $A$ is the $\one$ that was introduced by this rule, then we proceed basically as before: if $\Gamma\types A$ is $\one R$, so that $\Gamma$ is empty, then we are in the principal case and we can simply use the given derivation of $\Delta,\Psi\types B$; while if it is a left rule then we can apply a secondary induction.
  But it might also happen that $A$ is a different type, with the introduced $\one$ appearing in $\Delta$ or $\Psi$.
  However, this case is also easily dealt with by applying the inductive hypothesis to $\Gamma\types A$ and the given $\Delta,\Psi\types B$ (with $A$ appearing somewhere in its antecedents).
  In a direct argument for cut-elimination, we are transforming
  \begin{equation*}
    \inferrule*[right=cut]{
      \derivof{\Gamma\types A}\\
      \inferrule*[Right=$\one L$]{\derivof{\Delta_1,\Delta_2,A,\Psi\types B}}{\Delta_1,\one,\Delta_2,A,\Psi \types B}
    }{\Delta_1,\one,\Delta_2,\Gamma,\Psi\types B}
    \quad\leadsto\quad
    \inferrule*[right=$\one L$]{\inferrule*[Right=cut]{
      \derivof{\Gamma\types A}\\
      \derivof{\Delta_1,\Delta_2,A,\Psi\types B}
    }{\Delta_1,\Delta_2,\Gamma,\Psi\types B}}{\Delta_1,\one,\Delta_2,\Gamma,\Psi\types B}
  \end{equation*}

  The case when $\Delta,A,\Psi\types B$ ends with $\tensorL$ has a similar ``commutativity'' possibility.
  However, in this case there is also something new in the principal case, where $\Delta,A_1\tensor A_2,\Psi\types B$ is derived from $\Delta,A_1,A_2,\Psi\types B$, while $\Gamma\types A_1\tensor A_2$ is derived using $\tensorR$ from $\Gamma_1\types A_1$ and $\Gamma_2\types A_2$ (so that necessarily $\Gamma = \Gamma_1,\Gamma_2$).
  We would like to apply the inductive hypothesis twice to transform
  \begin{equation}\label{eq:monpos-cutam-1}
    \inferrule*[right=cut]{
      \inferrule*[right=$\tensorR$]{
        \derivof{\Gamma_1\types A_1}\\
        \derivof{\Gamma_2\types A_2}
      }{\Gamma_1,\Gamma_2\types A_1\tensor A_2}\\
      \inferrule*[Right=$\tensorL$]{
        \derivof{\Delta,A_1,A_2,\Psi\types B}
      }{\Delta,A_1\tensor A_2,\Psi\types B
      }}{\Delta,\Gamma_1,\Gamma_2,\Psi\types B}
  \end{equation}
  into
  \begin{equation}\label{eq:monpos-cutam-2}
    \inferrule*[Right=cut]{
      \derivof{\Gamma_2\types A_2}\\
      \inferrule*[Right=cut]{
        \derivof{\Gamma_1\types A_1}\\
        \derivof{\Delta,A_1,A_2,\Psi\types B}
      }{
        \Delta,\Gamma_1,A_2,\Psi\types B
      }
    }{
      \Delta,\Gamma_1,\Gamma_2,\Psi\types B
    }
  \end{equation}
  However, this is a problem for our usual style of induction.
  We can certainly apply the inductive hypothesis to $\Gamma_1\types A_1$ and $\Delta,A_1,A_2,\Psi\types B$ to get a derivation of $\Delta,\Gamma_1,A_2,\Psi\types B$.
  But this resulting derivation need not be ``smaller'' than our given derivation of $\Delta,A_1\tensor A_2,\Psi\types B$, so our inductive hypothesis does not apply to it.

  Probably the most common solution to this problem is to formulate the induction differently.
  Rather than inducting directly on the derivation of $\Delta,A,\Psi\types B$, we induct first on the type $A$ (the ``cut formula'')), and then do an ``inner'' induction on the derivation.
  All the ``commutativity'' cases do not change the cut formula, so there the inner inductive hypothesis continues to apply.
  But in the principal case for $\tensor$, both of the cuts we want to do inductively have smaller cut formulas than the one we started with ($A_1$ and $A_2$ versus $A_1\tensor A_2$), so they can be handled by the outer inductive hypothesis regardless of how large of derivations we need to apply them to.
\end{proof}

A different approach, however, is to prove the admissibility of multi-cut directly:

\begin{thm}\label{thm:monpos-multicutadm}
  Multi-cut is admissible in the sequent calculus for monoidal posets under \cG: if we have derivations of $\Psi_i\types A_i$ for $1\le i\le n$, and also $A_1,\dots,A_n \types B$, then we can construct a derivation of $\Psi_1,\dots,\Psi_n\types B$.
\end{thm}
\begin{proof}
  In this case we can return to inducting directly on the derivation of $A_1,\dots,A_n \types B$.
  The cases of identity and generator rules are just like in \cref{thm:multiposet-multicutadm}, and $\tensorR$ is just like the generator case.
  Unlike in \cref{thm:monpos-cutadm} it \emph{could} end with $\one R$, but in this case $n=0$ and there is nothing to be done.

  If it ends with $\one L$, then some $A_i = \one$, and we can forget about the corresponding $\Psi_i\types A_i$ and proceed inductively with the rest of them.
  (Note how even this case is simpler than in \cref{thm:monpos-cutadm}.)

  Finally, if it ends with $\tensorL$, then some $A_i = C\tensor D$, say, and we perform our secondary induction on $\Psi_i\types A_i$.
  Since $A_i=C\tensor D$ is not a base type, this derivation cannot end with the identity or generator rules, and of course it cannot end with $\one R$.
  If it ends with a left rule, we inductively cut with the premise of that rule and then apply it afterwards.
  The remaining case is when it ends with $\tensorR$, so that we have derivations of $\Gamma\types C$ and $\Delta\types D$ with $\Psi_i = \Gamma,\Delta$.
  But now we can inductively cut our given premise $A_1,\dots,A_{i-1},C,D,A_{i+1},\dots,A_n \types B$ with these two and also the given $\Psi_j\types A_j$ for $j\neq i$.
\end{proof}

That is, instead of transforming~\eqref{eq:monpos-cutam-1} into~\eqref{eq:monpos-cutam-2}, where we have to feed the output of one inductive cut into another inductive cut (which is what creates the problem), we transform
\begin{equation*}
  \inferrule*[right=cut]{
    \derivof{\Psi_j \types A_j} \\ % \;(A_j\neq C\tensor D)\\
    \inferrule*[right=$\tensorR$]{
      \derivof{\Gamma\types C}\\
      \derivof{\Gamma\types D}
    }{\Gamma,\Delta \types C\tensor D}\\
    \inferrule*[Right=$\tensorL$]{
      \derivof{A_1,\dots,C,D,\dots,A_n\types B}
    }{A_1,\dots,C\tensor D,\dots, A_n\types B
    }}{\Psi_1,\dots,\Gamma,\Delta,\dots,\Psi_n\types B}
\end{equation*}
into
\begin{equation*}
  \inferrule*[right=cut]{
    \derivof{\Psi_j \types A_j} \\ % \;(A_j\neq C\tensor D)\\
    \derivof{\Gamma\types C}\\
    \derivof{\Gamma\types D}\\
    \derivof{A_1,\dots,C,D,\dots,A_n\types B}
    }{\Psi_1,\dots,\Gamma,\Delta,\dots,\Psi_n\types B}
\end{equation*}
Thus, the multicategorical perspective leads to a simpler inductive proof of cut admissibility.
(Note, though, that to recover the one-place cut from the multi-cut requires composing with identities, hence invoking \cref{thm:monpos-identity} as well.)

In any case, we are ready to prove the initiality theorem, relating to an adjunction between the categoris \bRelMGr of relational multigraphs and \bMonPos of monoidal posets.
As always, the morphisms in our categories will be completely strict: so in particular the morphisms in \bMonPos are \emph{strict} monoidal functors.

\begin{thm}\label{thm:monpos-initial}
  For any relational multigraph \cG, the free monoidal poset generated by \cG is described by the sequent calculus for monoidal posets under \cG: its objects are the $A$ such that $\types A\type$ is derivable, while the relation $(A_1,\dots,A_n)\le B$ holds just when the sequent $A_1,\dots,A_n\types B$ is derivable.
\end{thm}
\begin{proof}
  As before, \cref{thm:monpos-identity,thm:monpos-cutadm} show that this defines a multiposet $\F\bMonPos\cG$.
  Moreover, the rules for $\tensor$ and $\one$ tell us that it is representable, hence monoidal.

  Now suppose $P:\cG\to\cM$ is a map into the underlying multiposet of any other monoidal poset.
  We can extend $P$ uniquely to a function from the objects of $\F\bMonPos\cG$ to those of $\cM$ preserving $\tensor$ and $\one$ on objects, so it remains to check that this is a map of multiposets and preserves the universal properties of $\tensor$ and $\one$.
  However, $\tensorR$ and $\one R$ are preserved by the universal data of $\tensor$ and $\one$ in \cM, while the universal properties of these data mean that $\tensorL$ and $\one L$ are also preserved.
\end{proof}

\subsection{Natural deduction for monoidal posets}
\label{sec:natded-monpos}

In natural deduction, the introduction rules $\tensorI$ and $\one I$ will coincide with the right rules $\tensorR$ and $\one R$ from the sequent calculus, but now we need elimination rules.
Since $\tensor$ and $\one$ in a multicategory have a ``mapping out'' universal property, these elimination rules will be reminiscent of the ``case analysis'' rules from \cref{sec:catcoprod}.
Formally speaking, they can be obtained by simply cutting $\tensorL$ and $\one L$ with an arbitrary sequent (thereby ``building in cuts'' to make the cut-admissiblity theorem easier).
\begin{mathpar}
  \inferrule*[Right=cut]{
    \Psi \types A\tensor B \\
    \inferrule*[Right=$\tensorL$]{\Gamma,A,B,\Delta\types C}{\Gamma,A\tensor B,\Delta\types C}
  }{
    \Gamma,\Psi,\Delta \types C
  }\and
  \inferrule*[Right=cut]{
    \Psi\types \one \\
    \inferrule*[Right=$\one L$]{\Gamma,\Delta\types A}{\Gamma,\one,\Delta\types A}
  }{
    \Gamma,\Psi,\Delta\types C
  }
\end{mathpar}
As usual in a natural deduction, we also need to assert the identity rule for all types.
Thus our complete \textbf{natural deduction for monoidal posets under \cG} consists of (the rules for $\types A\type$ and):
\begin{mathpar}
  \inferrule{\types A\type}{A\types A}\and
  \inferrule{(A_1,\dots,A_n \le B)\in\cG \\ \Gamma_1\types A_1 \\ \dots \\ \Gamma_n \types A_n}{\Gamma_1,\dots,\Gamma_n\types B}\and
  \inferrule{\Gamma\types A \\ \Delta\types B}{\Gamma,\Delta\types A\tensor B}\;\tensorI\and
  \inferrule{
    \Psi \types A\tensor B \\
    \Gamma,A,B,\Delta\types C
  }{
    \Gamma,\Psi,\Delta \types C
  }\;\tensorE\\
  \inferrule{ }{()\types \one}\;\one I\and
  \inferrule{
    \Psi\types \one \\
    \Gamma,\Delta\types A
  }{
    \Gamma,\Psi,\Delta\types C
  }\;\one E
\end{mathpar}

We leave the metatheory of this as an exercise (\cref{ex:natded-monpos}); it is also subsumed by the categorified version discussed in more detail in the next section.

\begin{rmk}\label{rmk:context-splitting-1}
  In \cref{sec:natded-mslat} we remarked that in (unary) natural deductions, the conclusions of rules always have an arbitrary type as antecedent.
  For simple type theories, the corresponding property is that the conclusions of rules should have an arbitrary \emph{context} on the left.
  This is not quite true for the above presentation of the rules, since most of their conclusions have an antecedent obtained by concatenating two or more contexts.
  However, such a rule is always equivalent to one whose conclusion involves an arbitrary context that is decomposed as a concatenation by an additional premise.
  For instance, the rule $\tensorI$ could equivalently be formulated as
  \begin{mathpar}
    \inferrule{\Psi = \Gamma,\Delta \\ \Gamma\types A \\ \Delta\types B}{\Psi\types A\tensor B}\;\tensorI\and
  \end{mathpar}
  while $\one I$ could be written
  \[ \inferrule{\Gamma=()}{\Gamma\types \one}\;\one I \]
  This is the appropriate point of view when reading rules ``bottom-up'' for type-checking or proof search, as discussed at the end of \cref{sec:rules}: to type-check or prove $\Psi\types A\tensor B$ we need to find an appropriate decomposition $\Psi = \Gamma,\Delta$ for which we can type-check or prove $\Gamma\types A$ and $\Delta\types B$.
  However, because this transformation is so straightforward, when writing informally one generally uses the simpler form with concatenated contexts in the conclusion.
\end{rmk}

\subsection*{Exercises}

\begin{ex}\label{ex:natded-monpos}
  Prove the well-formedness, cut-admissibility, and initiality theorems for the natural deduction for monoidal posets.
\end{ex}

\begin{ex}\label{ex:monpos-invertible}
  Prove that the rules $\tensorL$ and $\one L$ in the sequent calculus for monoidal posets are invertible in the sense of \cref{ex:mslat-invertible}: whenever we have a derivation of their conclusions, we also have derivations of their premises.
\end{ex}

\begin{ex}\label{ex:monpos-mslat}
  Write down either a sequent calculus or a natural deduction for monoidal posets that are also meet-semilattices, and prove its initiality theorem.
\end{ex}

\begin{ex}\label{ex:monpos-jslat}
  Let us augment the sequent calculus for monoidal posets by the following versions of the rules for join-semilattices:
  \begin{mathpar}
    \inferrule{\types A\type \\ \types B\type}{\types A\join B\type}\and
    \inferrule{ }{\types \bot\type}\and
    \inferrule{\Gamma \types A}{\Gamma\types A\join B}\and
    \inferrule{\Gamma \types B}{\Gamma\types A\join B}\and
    \inferrule{\Gamma,A,\Delta \types C\\\Gamma,B,\Delta \types C}{\Gamma,A\join B,\Delta\types C}\and
    \inferrule{ }{\Gamma,\bot,\Delta\types C}
  \end{mathpar}
  \begin{enumerate}
  \item Construct derivations in this calculus of the following sequents:
    \begin{align*}
      (A\tensor B)\join (A\tensor C) &\types  A\tensor (B\join C)\\
      A\tensor (B\join C) &\types (A\tensor B)\join (A\tensor C)
    \end{align*}
  \item Prove that this sequent calculus constructs the initial distributive monoidal poset (see \cref{thm:multicat-coprod}).
  \end{enumerate}
\end{ex}


\section{Multicategories and monoidal categories}
\label{sec:multicat-moncat}

Now we are ready to move back up from posets to categories; but here we encounter a bit of an expositional conundrum.
We have started with ordinary (non-symmetric, non-cartesian) multicategories since they are simpler from a category-theoretic perspective; in \cref{sec:cartmulti} we will introduce symmetric and cartesian multicategories by adding extra structure.
However, in type theory there are some ways in which the \emph{cartesian} case is the simplest.
This is essentially because our intuition tells us that ``variables can be used anywhere'', whereas in a non-cartesian type theory we have to control how many times a variable is used (and, in the non-symmetric case, what order they are used in).
Nevertheless we begin in this section (and the next) with a type theory for ordinary multicategories, as it introduces several important ideas that are clearer without the symmetric and cartesian structure to worry about; but we encourage the reader not to get too bogged down in details.


\subsection{Multicategories}
\label{sec:multicats}

Categorically, we begin with the adjunction between the category $\bMCat$ of multicategories and the category $\bMGr$ of multigraphs.
Let \cG be a multigraph; we augment the cut-free theory of \cref{sec:multiposets} with terms that represent the structure of derivations, as we did in \cref{sec:categories,sec:catprod,sec:catcoprod}.

Since our antecedents are now lists of formulas, we assign an abstract variable to \emph{each} of them, and we assign a single term involving these variables to the consequent.
Of course, we must assign distinct variables to distinct types in the list (or, more precisely, to distinct \emph{occurrences} of types, since the same type might occur more than once, and these occurrences should be assigned distinct variables).

Thus, for instance, we might have a judgment such as
\[ x:A, y:B, z:C \types f(x,g(y,z)):E \]
where $f\in\cG(A,D;E)$ and $g\in\cG(B,C;D)$.
Note that as always, the symbol $\types$ is the ``outermost''.
Moreover, the comma between abstract variable assignments binds more loosely than the typing colons; the above judgment should be read as
\[ ((x:A), (y:B), (z:C)) \types (f(x,g(y,z)):E). \]
As before, the derivation is actually determined by the term associated to the consequent \emph{together with} all the free variables in the context, which we can emphasize by writing
\[ xyz.f(x,g(y,z)) : (A, B, C \types E). \]

Since we now have multiple formal variables appearing in one sequent, it becomes important to keep track of which is which.
As in unary type theory, there are two ways to name variables.
In \textbf{de Bruijn style} we choose a fixed countably infinite set of variables, say $x_1,x_2,x_3,\dots$, and demand that any sequent with $n$ types in its context use the first $n$ of these variables \emph{in order}.
In fact there are two choices for this order; we might write
\[ x_1:A_1, x_2:A_2, \dots,x_n:A_n \types M:B \quad\text{or}\quad
x_n:A_n,\dots, x_2:A_2, x_1:A_1 \types M:B
\]
The first is called using \textbf{de Bruijn levels} and the second \textbf{de Bruijn indices}.

The second way to name variables is to allow arbitrary variables (perhaps taken from some fixed infinite set of variables), but keep track of $\alpha$-equivalence.
This now means that we can rename each variable independently, as long as we rename all of its occurrences at the same time and we don't try to rename any two variables to the same thing.
For instance, if $f\in\cG(A,A;A)$ then we can write four sequents
\[
\begin{array}{c@{\qquad}c}
  x:A, y:A \types f(x,y):A &
  x:A, y:A \types f(y,x):A \\\\
  y:A, x:A \types f(y,x):A &
  y:A, x:A \types f(x,y):A
\end{array}
\]
The two in the left column are the same by $\alpha$-equivalence, and similarly the two in the right column are identical; but the columns are distinct from each other.
(In fact, in the type theory of the present section, the sequents in the right-hand column are impossible; but in the theories to be considered in \cref{sec:cartmoncat,sec:symmoncat} they will be possible.)

\begin{rmk}
The intent of $\alpha$-equivalence is that the names or labels of variables are themselves meaningless, but they carry the information of which variable occurrences in a term refer to which variables in the context (or, later, to which variable binding sites).
Bourbaki attempted to do away with variable labels entirely, writing all variable occurrences as $\Box$ and drawing connecting links to denote these references; thus the two columns above would be written
\[
\tikz[remember picture] \node[rectangle, inner sep=0pt] (a1) {$A$};,
\tikz[remember picture] \node[rectangle, inner sep=0pt] (a2) {$A$}; \types
f(\tikz[remember picture] \node[rectangle, inner sep=0pt] (b1) {$\Box$};,
\tikz[remember picture] \node[rectangle, inner sep=0pt] (b2) {$\Box$};)
\qquad
\tikz[remember picture] \node[rectangle, inner sep=0pt] (a1p) {$A$};,
\tikz[remember picture] \node[rectangle, inner sep=0pt] (a2p) {$A$}; \types
f(\tikz[remember picture] \node[rectangle, inner sep=0pt] (b2p) {$\Box$};,
\tikz[remember picture] \node[rectangle, inner sep=0pt] (b1p) {$\Box$};)
\]
\begin{tikzpicture}[remember picture,overlay]
  \draw (b2) -- +(0,-.6) -| (a2);
  \draw[white,ultra thick] (b1) -- +(0,-.4) -| (a1);
  \draw (b1) -- +(0,-.4) -| (a1);
  \draw (b2p) -- +(0,-.4) -| (a2p);
  \draw (b1p) -- +(0,-.6) -| (a1p);
\end{tikzpicture}

\noindent
However, this notation seems unlikely to catch on.
\end{rmk}

In \cref{sec:multiposets-monpos} we used capital Greek letters such as $\Gamma$ to denote finite lists of types.
As is also conventional, when we incorporate formal variables we use $\Gamma$ represent a finite list of types \emph{with variables attached} (with, of course, distinct variables attached to distinct occurrences of types), which is also called a \textbf{context}.
In general, $\Gamma$ represents ``the sort of thing that can go on the left of $\types$''.
%Thus, if $\Gamma = (x:A, y:B, z:C)$ then the above sequent would be $\Gamma\types f(x,g(y,z)):E$.

Now, the rules for multiposets and monoidal posets from \cref{sec:multiposets-monpos} involve, among other things, concatenation of such lists, which we wrote as $\Gamma,\Delta$.
But when $\Gamma$ and $\Delta$ contain variables, simple concatenation would not preserve the invariant that distinct occurrences of types are labeled by distinct variables, so something else must be going on.
If we use de Bruijn style, then the variable numbers in $\Gamma$ or $\Delta$ have to be incremented; we leave the details of this to the interested reader (\cref{ex:debruijn-context-concat}).
If we instead use arbitrary named variables, as we will generally do, then we simply need to apply $\alpha$-equivalences to $\Gamma$ and/or $\Delta$ to make their variable names disjoint.
(This is an instance of \cref{princ:term-der-alpha} that term notations for rules can require applying $\alpha$-equivalences to some premises for compatibility.
In \cref{sec:catprod} ``compatibility'' meant using the \emph{same} variable, while here it means using \emph{different} variables.)

From now on we will write simply $\Gamma,\Delta$ (and similarly $\Gamma, x:A, \Delta$, and so on) for the concatenation of two given contexts, modified to ensure variable distinctness in whatever way is appropriate.
Of course, any variable incrementing or $\alpha$-equivalence that happens in $\Gamma$ or $\Delta$ must also be applied to the consequents of any sequents they appear in.
On the other hand, if in some situation we \emph{assume} a sequent and write its context as $\Gamma,\Delta$, then no such operation is being applied; we are simply choosing a partition of that context into two parts.
When applying a rule ``top-down'', this applies to its premises, while when applying it ``bottom-up'', this applies to its conclusion (recall \cref{rmk:context-splitting-1}).

All this futzing around with variables may seem quite tedious and uninteresting.
It does matter in some situations; for instance, if mathematics is to be implemented in a computer, then all these technical issues must be dealt with carefully.
However, from our point of view these are all just different tricks to ensure that the terms with formal variables (modulo $\alpha$-equivalence) remain exact representations of derivation trees.
The terms where we have to rename variables and so on are only a \emph{notation} for the mathematical objects of real interest, namely derivations.
Remember this if you are ever in doubt about the meaning of variables or what sorts of renamings are possible.

With all of that out of the way, we can anticlimactically state the rules for the \textbf{cut-free type theory for multicategories under \cG}:
\begin{mathpar}
  \inferrule{A\in\cG}{x:A\types x:A}\and
  \inferrule{f\in \cG(A_1,\dots,A_n;B) \\ \Gamma_1\types M_1:A_1 \\ \dots \\ \Gamma_n \types M_n:A_n}{\Gamma_1,\dots,\Gamma_n\types f(M_1,\dots,M_n):B}\and
\end{mathpar}
We note that this theory has the following property.

\begin{lem}\label{thm:multicat-linear}
  If $\Gamma\types M:B$ is derivable, then every variable in $\Gamma$ appears exactly once in $M$.
\end{lem}
\begin{proof}
  By induction on the derivation.
  The identity rule $x:A\types x:A$ clearly has this property.
  And in the conclusion of the generator rule each variable appears in exactly one $\Gamma_i$, hence can only appear in one of the $M_i$'s, and by induction it appears exactly once there; hence it appears exactly once in $f(M_1,\dots,M_n)$.
\end{proof}

In type-theoretic lingo, \cref{thm:multicat-linear} says that our current type theory is \textbf{linear} (just like a linear polynomial uses each variable exactly once, a ``linear type theory'' uses each variable exactly once).
Note that linearity is a property of a system that we \emph{prove}, not a requirement that we impose from outside.
It is useful when proving that terms are derivations.

\begin{lem}\label{thm:multicat-tad}
  If $\Gamma\types N:B$ is derivable in the cut-free type theory for multicategories under \cG, then it has a unique derivation.
\end{lem}
\begin{proof}
  If $N=x$, then the derivation can only be $\idfunc$.
  And if $N=f(M_1,\dots,M_n)$, then by linearity each variable in $\Gamma$ must occur in exactly one of the subterms $M_1,\dots,M_n$.
  If $\Gamma\types N:B$ is derivable, then it must be that this partition of $\Gamma$ is ordered, $\Gamma=\Gamma_1,\dots,\Gamma_n$, and this (together with the known domain $(A_1,\dots,A_n)$ of $f$) determines the premises $\Gamma_i \types M_i:A_i$ that must be recursively checked (c.f.\ \cref{rmk:context-splitting-1})
\end{proof}

Linearity also has content as a statement about derivations rather than just their terms: it says that each occurrence of a type in the antecedent of a derivable sequent can be ``traced back up'' exactly one branch of the derivation tree.
For instance, in the following derivation
\begin{mathpar}
  \inferrule*{\inferrule*{ }{x:A\types x:A}\\
    \inferrule*{\inferrule*{ }{y:B\types y:B}\\
      \inferrule*{ }{z:A\types z:A
      }}{y:B,z:A \types g(y,z):X}\\
    \inferrule*{\inferrule*{ }{w:C\types w:C}}{w:C \types h(w):Y}
  }{x:A,y:B,z:A,w:C \types f(x,g(y,z),h(w)):Z}
\end{mathpar}
we can trace the occurrences of types in the antecedent of the conclusion as follows (omitting the variables and terms for brevity):
\begin{mathpar}
  \inferrule*{\inferrule*{ }{\tikz[remember picture] \node[red,rectangle,inner sep=0pt] (a2) {$A$};\types A}\\
    \inferrule*{\inferrule*{ }{\tikz[remember picture] \node[blue,rectangle,inner sep=0pt] (b3) {$B$};\types B}\\
      \inferrule*{ }{\tikz[remember picture] \node[green,rectangle,inner sep=0pt] (aa3) {$A$};\types A
      }}{\tikz[remember picture] \node[blue,rectangle,inner sep=0pt] (b2) {$B$};,
      \tikz[remember picture] \node[green,rectangle,inner sep=0pt] (aa2) {$A$}; \types X}\\
    \inferrule*{\inferrule*{ }{\tikz[remember picture] \node[purple,rectangle,inner sep=0pt] (c3) {$C$};\types C
      }}{\tikz[remember picture] \node[purple,rectangle,inner sep=0pt] (c2) {$C$}; \types Y}
  }{\tikz[remember picture] \node[red,rectangle,inner sep=0pt] (a1) {$A$};,
    \tikz[remember picture] \node[blue,rectangle,inner sep=0pt] (b1) {$B$};,
    \tikz[remember picture] \node[green,rectangle,inner sep=0pt] (aa1) {$A$};,
    \tikz[remember picture] \node[purple,rectangle,inner sep=0pt] (c1) {$C$};
    \types Z}
\begin{tikzpicture}[remember picture,overlay]
  \draw[->,red] (a1) -- (a2);
  \draw[->,blue] (b1) -- (b2); \draw[->,blue] (b2) -- (b3);
  \draw[->,green] (aa1) -- (aa2); \draw[->,green] (aa2) -- (aa3);
  \draw[->,purple] (c1) -- (c2); \draw[->,purple] (c2) -- (c3);
\end{tikzpicture}
\end{mathpar}

We now move on to the admissibility of cut/substitution.
For this we may again choose between the one-place cut and the multi-cut.
We choose the former, because the notation is less heavy, and because it matches the more common path taken in type theory.
(The advantage of multi-cut that we saw in \cref{sec:seqcalc-monpos} is not relevant for natural deduction, since there are no left rules.
We will see something analogous in \cref{sec:logic}, however.)
But we encourage the interested reader to write down a multi--substitution too (\cref{ex:moncat-multisubadm}).

\begin{thm}\label{thm:multicat-subadm}
  Substitution is admissible in the cut-free type theory for multicategories under \cG: given derivations of $\Gamma\types M:A$ and of $\Delta,x:A,\Psi\types N:B$, we can construct a derivation of $\Delta,\Gamma,\Psi\types M[N/x]:B$.
\end{thm}
\begin{proof}
  This is essentially just \cref{thm:multiposet-cutadm}, with terms carried along.
  There is one thing to be said: since the variables used in any context must be distinct, including the given context $\Delta,x:A,\Psi$, it must be that the variables in $\Delta$ and $\Psi$ are pairwise distinct, and all of them are distinct from $x$.
  But the variables in $\Delta,\Psi$ may not be pairwise distinct from those in $\Gamma$, so the context of the desired conclusion $\Delta,\Gamma,\Psi\types M[N/x]:B$ may involve an $\alpha$-equivalence.
  For instance, if we have $y:C\types f(y):A$ and $y:C,x:A,z:D\types g(y,x,z):B$, we cannot naively conclude $y:C,y:C,z:D\types g(y,f(y),z):B$; we have to rename one of the $y$'s first and get $y:C,w:C,z:D\types g(y,f(w),z):B$.
  We emphasize, however, that this is only a point about the term notation.
  The proof of \cref{thm:multiposet-cutadm}, which doesn't mention variables or terms at all, \emph{is already} an operation on derivations, and the renaming of variables only arises when we notate those derivations in a particular way.
\end{proof}

As before, note that we can regard this as defining substitution; its inductive clauses are
\begin{align*}
  x[M/x] &= M\\
  f(N_1,\dots,N_n)[M/x] &= f(N_1,\dots,N_{i-1},N_i[M/x],N_{i+1},\dots,N_n)
\end{align*}
where $i$ is the unique index such that $x$ occurs in $N_i$ (which exists by \cref{thm:multicat-linear}).

The one-place substitution operation defined in \cref{thm:multicat-subadm} will, of course, give the $\circ_i$ operations in our free multicategory.
The index $i$ is specified implicitly by the position of the variable $x$ in the context of $N$.
A similar thing happens with the associativity and interchange axioms.

\begin{thm}\label{thm:multicat-subassoc}
  Substitution in the cut-free type theory for multicategories satisfies the associativity/interchange rules:
  \begin{enumerate}
  \item If $\Gamma\types M:A$ and $\Delta,x:A,\Delta' \types N:B$ and $\Psi,y:B,\Psi'\types P:C$, then\label{item:multicat-subassoc-1}
    \[ P[N/y][M/x] = P[N[M/x]/y] \]
  \item If $\Gamma\types M:A$ and $\Delta \types N:B$ and $\Psi,x:A,\Psi',y:B,\Psi''\types P:C$, then\label{item:multicat-subassoc-2}
    \[ P[N/y][M/x] = P[M/x][N/y] \]
  \end{enumerate}
\end{thm}
\begin{proof}
  In both cases we induct on the derivation of $P$.
  For~\ref{item:multicat-subassoc-1}, if $P=y$ then both sides are $N[M/x]$.
  If $P=f(P_1,\dots,P_n)$, suppose $y$ occurs in $P_i$.
  Then $P[N/y] = f(P_1,\dots,P_i[N/y],\dots,P_n)$ and $x$ occurs in $P_i[N/y]$, so
  $P[N/y][M/x] = f(P_1,\dots,P_i[N/y][M/x],\dots,P_n)$ and the inductive hypothesis applies.

  For~\ref{item:multicat-subassoc-2}, we can't have $P$ being a single variable since there are two distinct variables in its context.
  Thus it must be $f(P_1,\dots,P_n)$, with $x$ and $y$ appearing in $P_i$ and $P_j$ respectively.
  If $i=j$, then we simply apply the inductive hypothesis to $P_i$; while if $i\neq j$ then
  \begin{equation*}
    P[N/y][M/x] = f(P_1,\dots,P_i[M/x],\dots,P_j[N/y],\dots,P_n) = P[M/x][N/y]\qedhere
  \end{equation*}
\end{proof}

If we used de Bruijn levels instead of arbitrary named variables, then the statement of \cref{thm:multicat-subassoc} would involve the same arithmetic on variable numbers that appears in the $\circ_i$ operations.
It is pleasing how the use of abstract variables eliminates this tedious bookkeeping.
(It is also possible to eliminate the bookkeeping at the multicategorical level by using an alternative definition of multicategories such as that of~\cite[Appendix A]{leinster:higher-opds}.)

\begin{thm}\label{thm:multicat-initial}
  For any multigraph \cG, the free multicategory generated by \cG can be described by the cut-free type theory for multicategories under \cG: its objects are those of \cG, and its morphisms $\Gamma\to B$ are the derivations of $\Gamma\types B$ (or equivalently, the derivable term judgments $\Gamma\types M:B$ modulo $\alpha$-equivalence).
\end{thm}
\begin{proof}
  \cref{thm:multicat-subadm} gives us the one-place composition operations, and \cref{thm:multicat-subassoc} verifies the associativity/interchange axiom for these.
  The two identity axioms are $x[M/x]=M$ (one of the defining clauses of substitution) and ``$M[y/x] = M$'', which looks false or nonsensical but is actually just an instance of $\alpha$-equivalence.

  Thus, we have a multicategory $\F\bMCat\cG$.
  Let \cM be any multicategory and $P:\cG\to\cM$ a map of multigraphs; as usual we extend $P$ to the morphisms of $\F\bMCat\cG$ by induction on derivations, and such an extension is forced since the rules are all instances of functoriality.
  Finally we verify by induction on derivations that this extension is in fact functorial on all composites.
\end{proof}


\subsection{Monoidal categories}
\label{sec:moncat}

We now extend the term notation of \cref{sec:multicats} to the natural deduction for monoidal posets from \cref{sec:natded-monpos}, obtaining a \textbf{simple type theory for monoidal categories under \cG} shown in \cref{fig:moncat}.

The rule $\tensorI$, like the rule $\timesI$ from \cref{sec:catprod}, ``pairs up'' two derivations of $\Gamma\types A$ and $\Delta\types B$, and thus must include terms notating both.
In this case, however, rather than pulling out the same variable from each, we require that the variables used are disjoint, so that we can concatenate the contexts in the conclusion.
Thus once again we are pairing up only the term parts (associated to the consequents), but the variables in the two contexts remain distinct; to emphasize this difference we use a different notation $\tpair M N$ instead of $\pair M N$.

The rule $\tensorE$, on the other hand, is more like the rule $\plusE$ from \cref{sec:catcoprod}: it has to include terms for both premises, one of which involves variables not appearing in the conclusion.
But unlike in \cref{sec:catcoprod}, the term $N$ can contain other variables that remain in the context of the conclusion (and must be disjoint from those in $M$, by $\alpha$-equivalence if necessary).
We only need to ``bind'' the other two variables $x$ and $y$.
Thus, for instance, the following application of $\tensorI$:
\begin{mathpar}
  \inferrule*{u:C, v:D \types \tpair{f(u)}{g(v)}:A\tensor B\\
  z:G,x:A,y:B,w:H \types h(z,x,y,w):K
  }{z:G,u:C,v:D,w:H\types \match_\tensor(\tpair{f(u)}{g(v)}, xy.h(z,x,y,w)):K}
\end{mathpar}
produces a term % $\match_\tensor(\tpair{f(u)}{g(v)}, xy.h(z,x,y,w))$
in which the variables $z,w$ in $h(z,x,y,w)$ are free, in addition to the free variables $u,v$ in $\tpair{f(u)}{g(v)}$.
% (This term is also a counterexample to a natural conjecture that the variables in a term must appear in the same order that they do in the context.)

Intuitively, because the tensor product has a ``mapping out'' universal property like a coproduct (that is, it is a ``positive type''), its elimination rule is a sort of ``case analysis'' that decomposes an element of $A\tensor B$ into an element of $A$ and an element of $B$, rather than a pair of projections.
Just as the rule $\plusE$ says that ``to do something with an element of $A+B$, it suffices to assume that it is either $\inl(u)$ or $\inr(v)$'', the rule $\tensorE$ says that ``to do something with an element of $A\tensor B$, it suffices to assume that it is $\tpair{x}{y}$.''
And just as the variables $u$ and $v$ are ``bound'' in the term syntax $\acase AB(M,u.P,v.Q)$ for coproducts, the variables $x$ and $y$ are bound in the term syntax $\match_{A\tensor B}^{\Gamma|\Delta}(M,xy.N)$.
The annotations $A\tensor B$ and $\Gamma|\Delta$ are to make type-checking possible (see \cref{thm:moncat-tad}); but generally we will omit them and write simply $\match_\tensor(M,xy.N)$.

The case of $\one$ is similar but simpler: to do something with an element of $\one$, it suffices to assume that it is $\ott$.
This gives no extra information, so no new variables are bound.
That is, the term syntax $\match_\one(M,N)$ binds nothing in $N$; it simply allows us to ignore $M$ (while keeping the free variables of $M$ in the context).

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\types A\type}{x:A\types x:A}\and
    \inferrule{f\in \cG(A_1,\dots,A_n;B) \\ \Gamma_1\types M_1:A_1 \\ \dots \\ \Gamma_n \types M_n:A_n}{\Gamma_1,\dots,\Gamma_n\types f(M_1,\dots,M_n):B}\and
    \inferrule{\Gamma\types M:A \\ \Delta\types N:B}{\Gamma,\Delta\types \tpair{M}{N}:A\tensor B}\;\tensorI\and
    \inferrule{
      \Psi \types M:A\tensor B \\
      \Gamma,x:A,y:B,\Delta\types N:C
    }{
      \Gamma,\Psi,\Delta \types \match_{A\tensor B}^{\Gamma|\Delta}(M,xy.N):C
    }\;\tensorE\\
    \inferrule{ }{()\types \ott:\one}\;\one I\and
    \inferrule{
      \Psi\types M:\one \\
      \Gamma,\Delta\types N:A
    }{
      \Gamma,\Psi,\Delta\types \match_\one(M,N):C
    }\;\one E
  \end{mathpar}
  \caption{Simple type theory for monoidal categories}
  \label{fig:moncat}
\end{figure}

Like the theory of \cref{sec:multicats}, this theory is linear:

\begin{lem}\label{thm:moncat-linear}
  If $\Gamma\types M:B$ is derivable in the simple type theory for monoidal categories under \cG, then every variable in $\Gamma$ appears exactly once free in $M$.
\end{lem}
\begin{proof}
  By induction on the derivation.
  The cases of variables and generators are as in \cref{thm:multicat-linear}.
  For a pair $\tpair M N$ coming from $\tensorI$, each variable in $\Gamma,\Delta$ appears in exactly one of $\Gamma$ and $\Delta$, hence in exactly one of $M$ and $N$; we then apply the inductive hypotheses to $M$ or $N$ respectively.
  Similarly, for $\match_\tensor(M,xy.N)$ coming from $\tensorE$, each variable in $\Gamma,\Psi,\Delta$ must appear in exactly one of $\Gamma$, $\Psi$, or $\Delta$; by induction then in the first and third cases it must appear exactly once in $N$, and in the second case it must appear exactly once in $M$.
  The case of $\one E$ is similar, while there are no variables at all in $\ott$.
\end{proof}

\begin{lem}\label{thm:moncat-tad}
  If $\Gamma\types N:B$ is derivable in the simple type theory for monoidal categories under \cG, then it has a unique derivation.
\end{lem}
\begin{proof}
  The cases of $\idfunc$ and $f$ are as in \cref{thm:multicat-tad}, and the case of $\tensorI$ is similar, while $\one I$ is trivial.
  For $\match_\one(M,N)$, by linearity each variable occurs in exactly one of $M$ or $N$.
  If such a term is derivable, then the variables occurring in $M$ must be contiguous in the context, thereby splitting it as $\Gamma,\Psi,\Delta$ and determining the premises.
  If it should happen that \emph{no} variables occur in $M$ (such as if $M=\star$), then of course $\Psi=()$, but the splitting $\Gamma,\Delta$ is not uniquely determined; however since the premise has a re-joined context $\Gamma,\Delta$ anyway this doesn't matter.

  In the case of $\tensorE$, however, this latter point makes a difference, because the premise \emph{does} depend on which variables end up in $\Gamma$ and which in $\Delta$.
  This is why we have included the $\Gamma|\Delta$ annotation on $\match_\tensor^{\Gamma|\Delta}(M,xy.N)$, so that the context splitting is determined even if $M$ contains no variables.
  (See \cref{ex:moncat-context-splitting}.)
\end{proof}

\begin{lem}\label{thm:moncat-subadm}
  Substitution is admissible in the simple type theory for monoidal categories under \cG, in the same sense as \cref{thm:multicat-subadm}.
  Moreover, it is associative and interchanging in the same sense as \cref{thm:multicat-subassoc}.
\end{lem}
\begin{proof}
  The method is the same as that of \cref{thm:multiposet-cutadm}.
  Given judgments $\Gamma\types M:A$ and $\Delta,x:A,\Psi \types N:B$ (involving disjoint variables), we induct on the derivation of $N$.
  If the derivation is $\idfunc$, then $\Delta$ and $\Psi$ are empty and $N=x$, in which case we can just use $M$ itself.
  In all other cases, by \cref{thm:moncat-linear} the variable $x$ must appear in exactly one of the premises of the last rule applied to derive $N$ (which is to say, in exactly one of the subterms appearing in $N$ itself), and we inductively perform the substitution there.

  Explicitly, the defining clauses of the substitution operation are shown in \cref{fig:moncat-sub}.
%  (The case of $\ott$ actually does fit the general pattern discussed above, remembering that the rule $\one I$ has \emph{no} premises.)
  (Technically we also ought to indicate how the $\Gamma|\Delta$ superscripts on $\match_\tensor$ are frobnicated, but we leave that to the fastidious reader.)
  The proof of associativity and interchange is essentially the same as before: all the other rules behave just like the generator rules, except for $\ott$ where the claim is trivial.
\end{proof}

\begin{figure}
  \centering
  \begin{alignat*}{2}
    x[M/x] &= M\\
    f(N_1,\dots,N_n)[M/x] &= f(N_1,\dots,N_i[M/x],\dots,N_n) &&\quad\text{if $x$ occurs in $N_i$}\\
    \tpair P Q[M/x] &= \tpair{P[M/x]}{Q} &&\quad\text{if $x$ occurs in $P$}\\
    \tpair P Q[M/x] &= \tpair{P}{Q[M/x]} &&\quad\text{if $x$ occurs in $Q$}\\
    \match_\tensor(N,uv.P)[M/x] &= \match_\tensor(N[M/x],uv.P) &&\quad\text{if $x$ occurs in $N$}\\
    \match_\tensor(N,uv.P)[M/x] &= \match_\tensor(N,uv.P[M/x]) &&\quad\text{if $x$ occurs in $P$}\\
    % \match_\tensor^{\Gamma|\Delta}(N,uv.P)[M/x] &= \match_\tensor^{\Gamma|\Delta}(N[M/x],uv.P) &&\quad\text{if $x$ occurs in $N$}\\
    % \match_\tensor^{\Gamma_1,x,\Gamma_2|\Delta}(N,uv.P)[M/x] &= \match_\tensor^{\Gamma_1,\Psi,\Gamma_2|\Delta}(N,uv.P[M/x]) &&\quad\text{$\Psi$ the context of $M$}\\
    % \match_\tensor^{\Gamma|\Delta_1,x,\Delta_2}(N,uv.P)[M/x] &= \match_\tensor^{\Gamma|\Delta_1,\Psi,\Delta_2}(N,uv.P[M/x]) &&\quad\text{$\Psi$ the context of $M$}\\
    \ott[M/x] &&&\quad\text{cannot happen}\\
    \match_\one(N,P)[M/x] &= \match_\one(N[M/x],P) &&\quad\text{if $x$ occurs in $N$}\\
    \match_\one(N,P)[M/x] &= \match_\one(N,P[M/x]) &&\quad\text{if $x$ occurs in $P$}
  \end{alignat*}
  \caption{Substitution in the simple type theory for monoidal categories}
  \label{fig:moncat-sub}
\end{figure}

There is one final point to be made here about $\alpha$-equivalence: in the rule $\match_\tensor(N,uv.P)[M/x] = \match_\tensor(N,uv.P[M/x])$, we must rename variables to ensure that $u$ and $v$ do not appear free in $M$.
Otherwise, such a $u$ or $v$ in $M$ would after substitution be ``in the scope'' of the binding of $u$ or $v$, whereas all the free variables of $M$ ought to remain free in the substituted term.
(This issue didn't arise in \cref{sec:catcoprod} because there it was not possible to substitute into the subterms $u.P$ and $v.Q$ of a $\case$ term containing bound variables, since they could not contain any \emph{other} variables to be substituted for.)
When we regard substitution as an operation on derivations, the point is that to eliminate a cut after $\tensorE$ of the following sort:
\begin{mathpar}
  \inferrule*[Right=cut]{
    \Gamma\types M:A \\
    \inferrule*[Right=$\tensorE$]{
      \Xi \types N:C\tensor D\\
      \Delta_1,x:A,\Delta_2,u:C,v:D,\Psi \types P:B
    }{\Delta_1,x:A,\Delta_2,\Xi,\Psi \types \match_\tensor(N,uv.P):B
    }}{\Delta_1,\Gamma,\Delta_2,\Xi,\Psi \types \match_\tensor (N,uv.P[M/x]):B}
\end{mathpar}
we have to inductively cut
\begin{mathpar}
  \inferrule*[Right=cut]{
    \Gamma\types M:A \\
    \Delta_1,x:A,\Delta_2,u:C,v:D,\Psi \types P:B
  }{\Delta_1,\Gamma,\Delta_2,u:C,v:D,\Psi \types P[M/x]:B}
\end{mathpar}
and in order for \emph{this} cut to satisfy the variable condition explained in \cref{thm:multicat-subadm}, it must be that $u$ and $v$ do not occur in $\Gamma$.

When one takes terms with named variables as primary, this sort of ``capture-avoiding substitution'' is both necessary and tedious.
The de Bruijn methods avoid it, though at a fairly severe cost to readability.
But with substitution treated as an operation on derivations, there are no variables to ``capture'' and nothing to worry about.

With substitution in hand, we can state the $\beta$- and $\eta$-conversion rules that implement the universal properties.
\begin{mathpar}
  \match_\tensor(\tpair M N,xy.P) \equiv P[M/x,N/y]\and
  \match_\tensor(M,xy.N[\tpair xy/u]) \equiv N[M/u]\\
  \match_\one(\ott,N) \equiv N\and
  \match_\one(M,N[\ott/u]) \equiv N[M/u]
\end{mathpar}
As before, the $\beta$-conversion rule says that the map out of $A\tensor B$ defined by its universal property has the correct composite with the universal morphism $(A,B)\to A\tensor B$, while the $\eta$-conversion rule says that any map out of $A\tensor B$ is determined by the universal property from its composite with the universal morphism.
The rules for $\one$ are similar.

\begin{thm}\label{thm:moncat-initial}
  The free monoidal category generated by a multigraph \cG (or, more precisely, its underlying multicategory) can be described by the simple type theory for monoidal categories under \cG: its objects are the $A$ such that $\types A\type$, and its morphisms are the derivations of $\Gamma\types A$ (or the derivable judgments $\Gamma\types M:A$) modulo the congruence $\equiv$.
\end{thm}
\begin{proof}
  \cref{thm:moncat-subadm} shows that we obtain a multicategory $\F\bMonCat\cG$ this way, just as in \cref{thm:multicat-initial}.
  The rules for $\tensor$ and $\one$, together with the $\beta$- and $\eta$-rules for $\equiv$, tell us that it is representable, and hence a monoidal category.
  Now if \cM is a monoidal category and $P:\cG\to\cM$ a map of multigraphs, we extend it to $\F\bMonCat\cG$ by induction on derivations (of objects and morphisms and equalities) using the fact that \cM is a representable multicategory, observe that this definition is forced by functoriality and (strict) preservation of the monoidal structure, and then prove by induction that it is indeed a functor.
\end{proof}


\subsection*{Exercises}

\begin{ex}\label{ex:repmulticat-moncat}
  Our proof of \cref{thm:moncat-initial} relied on the fact that monoidal categories are equivalent to representable multicategories, which we sketched but did not prove carefully.
  If we don't assume this fact, then our proof of \cref{thm:moncat-initial} is actually just about free representable multicategories.
  Using this version of the theorem, prove \emph{using type theory} that any representable multicategory is monoidal: that is, its tensor product is coherently associative and unital.
\end{ex}

\begin{ex}\label{ex:moncat-multisubadm}
  Formulate and prove the admissibility of a ``multi-substitution'' rule like \cref{thm:multiposet-multicutadm} for the type theories considered in this section.
\end{ex}

\begin{ex}\label{ex:moncat-context-splitting}
  The annotation $\Gamma|\Delta$ on $\match_{A\tensor B}^{\Gamma|\Delta}$ is something that appears only in the non-symmetric case, so we encourage the reader not to worry overmuch about it.
  However, for the reader who nevertheless insists on worrying, here is some extra reassurance.
  \begin{enumerate}
  \item We noted in \cref{thm:moncat-tad} that this annotation on $\match_{A\tensor B}^{\Gamma|\Delta}(M,xy.N)$ is only necessary if $M$ contains no variables.
    To see that it can actually matter in that case, find an example of two distinct derivations whose corresponding terms differ \emph{only} in their annotations $\Gamma|\Delta$.\label{item:moncat-context-splitting-1}
  % \begin{mathpar}
  %   \inferrule*{()\types \tpair\star\star:\one\tensor \one\\
  %     \inferrule*{
  %       x:\one \types x:\one\\
  %       \inferrule*{
  %         y:\one \types y:\one\\
  %         u:C \types u:C
  %       }{u:C,y:\one \types \match_\one(y,u):C}
  %     }{u:C,x:\one,y:\one \types \match_\one(x,\match_\one(y,u)):C}
  %   }{u:C \types \match_\tensor(\tpair\star\star, xy.\match_\one(x,\match_\one(y,u))) : C}
  %   \and
  %   \inferrule*{()\types \tpair\star\star:\one\tensor \one\\
  %     \inferrule*{
  %       x:\one \types x:\one\\
  %       \inferrule*{
  %         y:\one \types y:\one\\
  %         u:C \types u:C
  %       }{y:\one,u:C \types \match_\one(y,u):C}
  %     }{x:\one,y:\one,u:C \types \match_\one(x,\match_\one(y,u)):C}
  %   }{u:C \types \match_\tensor(\tpair\star\star, xy.\match_\one(x,\match_\one(y,u))) : C}
  % \end{mathpar}
  \item Prove that any two terms as in \ref{item:moncat-context-splitting-1} are related by $\equiv$.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:debruijn-context-concat}
  Describe precisely what has to happen to de-Bruijn-style variables when concatenating contexts, and formulate the rules for the type theories of this section using de Bruijn variables.
% Let us write $A[+n]$, $M[+n]$, $\Gamma[+n]$, and so on for the operation of adding $n$ to all the variable indices\footnote{Technically, all the \emph{free} variable indices; we will return to this in \cref{sec:moncat}.} in any syntactic object; thus for instance $f(x_1,x_2)[+3] = f(x_4,x_5)$.
% Also, if $\Gamma$ is a context, let us write $|\Gamma|$ for its length.
% Now if we use ``de Bruijn levels'', the concatenation of $\Gamma$ and $\Delta$ must be $\Gamma,\Delta[+|\Gamma|]$, while if we use ``de Bruijn indices'' it must be $\Gamma[+|\Delta|],\Delta$.
% For instance, we concatenate $x_1:A_1, x_2:A_2$ with $x_1:B_1$ to get $x_1:A_1, x_2:A_2,x_3:B_1$.
%
% \begin{rmk}
%   If we use de Bruijn style for variables, then this issue doesn't arise, but other confusing phenomena appear instead.
%   In general, when combining terms defined in different contexts using de Bruijn variables, we increment their variable numbers to match those in the new context.
%   For instance, given $x_1:A,x_2:B \types f(x_1,x_2):C$ and $x_1:D\types g(x_1):E$, we increment the latter when forming $x_1:A,x_2:B,x_3:D\types \tpair{f(x_1,x_2)}{g(x_3)}:C\tensor E$.
%   However, in a $\match_\tensor$ term, we can do this with the discriminee $M$ but not with the case branch $N$.
%   Inside $N$ in $\match_\tensor(M,x_i x_{i+1}.N)$, we need the variables $x_i$ and $x_{i+1}$ to refer to the ones being bound.
%   But if $\Psi$ contains fewer than two variables, then in the concatenated context $\Gamma,\Psi,\Delta$ some of the variables in $\Delta$ will also be named $x_i$ or $x_{i+1}$; so inside $N$ in $\match_\tensor(M,x_i x_{i+1}.N)$ we need to refer to variables in $\Delta$ by the numbers that they have in the original judgment $\Gamma,x_i:A,x_{i+1}:B,\Delta\types N:C$.
%   Thus, for instance, if $\Psi=()$ and $N$ is $x_1:A,x_2:B,x_3:D\types f(x_1,x_2,x_3):C$, then the result of $\tensorE$ must be $x_1:D \types \match(M,x_1 x_2.f(x_1,x_2,x_3)):C$, in which the variable denoted ``$x_3$'' in the term actually refers to the variable denoted ``$x_1$'' in the context.
%   (This example is for de Bruijn levels; de Bruijn indices, with the numbers going the other way, have a similar problem with $\Gamma$ in place of $\Delta$.)
%   There is no technical ambiguity and a computer can parse this term just fine, but it is fairly unreadable for a human.
%   (Indeed, according to Conor McBride, Bob Atkey once described the ability to read de Bruijn variables as a reverse Turing test.)
% \end{rmk}
\end{ex}


\section{Adding products and coproducts}
\label{sec:multicat-prod-coprod}

Now that we understand the simple type theories of multicategories and monoidal categories, let's add products and coproducts as well.
This is where we start to see the value of principle~\eqref{princ:independence} from \cref{sec:why-multicats}: for the most part we can just ``put together'' the rules from \cref{sec:multicat-moncat,sec:catprod,sec:catcoprod}, although there is a little extra work to generalize the rules for products and coproducts to the non-unary case.

In \cref{ex:monpos-mslat,ex:monpos-jslat} you studied sequent calculi for monoidal posets with meets and distributive monoidal posets.
Now we formulate similar rules in natural deduction style, annotated with terms; the entire \textbf{simple type theory for distributive monoidal categories with products} (except for the obvious rules governing the judgment $\types A\type$) is shown in \cref{fig:moncat-prod-coprod}.
(To obtain theories for monoidal categories with products only, or distributive monoidal categories, or multicategories with products and coproducts, and so on, we can simply omit some of these rules and their corresponding clauses in the following proofs.)

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\types A\type}{x:A\types x:A}\;\idfunc
    \and
    \inferrule{f\in \cG(A_1,\dots,A_n;B) \\ \Gamma_1\types M_1:A_1 \\ \dots \\ \Gamma_n \types M_n:A_n}{\Gamma_1,\dots,\Gamma_n\types f(M_1,\dots,M_n):B}\;fI
    \and
    \inferrule{\Gamma\types M:A \\ \Delta\types N:B}{\Gamma,\Delta\types \tpair{M}{N}:A\tensor B}\;\tensorI
    \and
    \inferrule{
      \Psi \types M:A\tensor B \\
      \Gamma,x:A,y:B,\Delta\types N:C
    }{
      \Gamma,\Psi,\Delta \types \match_{A\tensor B}^{\Gamma|\Delta}(M,xy.N):C
    }\;\tensorE
    \\
    \inferrule{ }{()\types \ott:\one}\;\one I
    \and
    \inferrule{
      \Psi\types M:\one \\
      \Gamma,\Delta\types N:C
    }{
      \Gamma,\Psi,\Delta\types \match_\one(M,N):C
    }\;\one E
    \\
    \inferrule{\Gamma\types M:A \\ \Gamma\types N:B}{\Gamma\types \pair{M}{N} :A\times B}\;\timesI
    \and
    \inferrule{\Gamma\types M:A\times B}{\Gamma\types \pr1AB(M):A}\;\timesE1
    \and
    \inferrule{\Gamma\types M:A\times B}{\Gamma\types \pr2AB(M):B}\;\timesE2
    \\
    \inferrule{ }{x_1:A_1,\dots,x_n:A_n\types \ttt(x_1,\dots,x_n):\unit}\;\unit I
    \and
    \inferrule{\Psi\types M:\zero}{\Gamma,\Psi,\Delta\types \abort^{\Gamma,\Delta}(M):C}\;\zero E
    \\
    \inferrule{\Gamma\types M:A}{\Gamma\types \inl(M):A+B}\;\plusI1
    \and
    \inferrule{\Gamma\types N:B}{\Gamma\types \inr(N):A+B}\;\plusI2
    \and
    \inferrule{
      \Psi\types M:A+B \\ \Gamma,u:A,\Delta \types P:C \\ \Gamma,v:B,\Delta\types Q:C
    }{\Gamma,\Psi,\Delta \types\acase AB^{\Gamma|\Delta}(M,u.P,v.Q):C}\;\plusE
  \end{mathpar}
  \caption{Distributive monoidal categories with products}
  \label{fig:moncat-prod-coprod}
\end{figure}

A few things are worth remarking on.
Firstly, the types $\tensor,\one,+,\zero$ are ``positive'' (have ``mapping out'' universal properties), while the types $\times,\unit$ are ``negative'' (have ``mapping in'' universal properties).
All the positive types have elimination rules involving a $\match$ that binds variables (perhaps zero of them), while the negative types do not.
This is a general feature of the behavior of positive and negative types with respect to abstract variables.

Secondly, as in \cref{ex:monpos-jslat}, the elimination rules for $\zero$ and $A+B$ act on a single type in the context, leaving the others untouched.
This corresponds to the definition of coproducts in a multicategory from \cref{thm:multicat-coprod}.

Thirdly, notice the difference between $\one I$ and $\unit I$: both have no premises, but in $\one I$ the context of the conclusion must be empty, whereas in $\unit I$ it can be arbitrary.
Similarly, the difference between $\tensorI$ and $\timesI$ is that in $\tensorI$ the contexts are concatenated in the conclusion, while in $\timesI$ both premises must have the same context, which is repeated in the conclusion.

Finally, there are some curious annotations.
As in \cref{sec:moncat}, the superscripts $\Gamma|\Delta$ on $\match_\tensor$ and $\match_+$ are to ensure type-checking, and can usually be omitted; and similarly for the superscript $AB$ on $\pi_i$ as in \cref{sec:catprod}.
The superscript $\Gamma,\Delta$ on $\abort$, however, is there for a different purpose, which is the same purpose as the passing of all the variables in the context as arguments to $\ttt$; it has to do with linearity.

Unlike the theory of \cref{sec:multicat-moncat}, this type theory is not globally ``linear'': for instance in $x:A \types \pair x x : A\times A$ the variable $x$ appears twice.
But by including the unused variables in $\unit I$ and $\zero E$ we can ensure the following weaker property.

\begin{lem}\label{thm:moncat-prod-coprod-superlin}
  In any derivable sequent $\Gamma\types M:A$, every variable in $\Gamma$ appears at least once (free) in the term $M$.
\end{lem}
\begin{proof}
  An easy induction over derivations.
\end{proof}

This ``superlinearity'' property guarantees that terms are derivations.

\begin{lem}\label{thm:moncat-prod-coprod-tad}
  A derivable sequent $\Gamma\types M:A$ uniquely determines a derivation.
\end{lem}
\begin{proof}
  By induction as usual.
  The cases involving $f$, $\tensor$, and $\one$ are essentially just like in \cref{thm:moncat-tad}; \cref{thm:moncat-prod-coprod-superlin} ensures that each variable appears at least once in the term, and if the term is derivable then each variable must appear in only one subterm, determining the context splitting.
  The cases involving $\times,\unit,+,\zero$ are straightforward.
\end{proof}

A concrete example where we need the extra arguments to $\ttt$ is:
\begin{equation}\label{eq:ttt-arg}
  \inferrule*{\inferrule*{ }{x:A \types \ttt(x):\unit} \\
    \inferrule*{ }{\cdot \types \ttt():\unit}}
  {x:A \types \tpair{\ttt(x)}{\ttt()} : \unit\tensor\unit}
  \qquad
  \inferrule*{\inferrule*{ }{()\types \ttt():\unit} \\
    \inferrule*{ }{x:A \types \ttt(x):\unit}}
  {x:A \types \tpair{\ttt()}{\ttt(x)} : \unit\tensor\unit}
\end{equation}
Unlike with the annotations $\Gamma|\Delta$ on $\match$es (see \cref{ex:moncat-context-splitting}), these terms really can represent distinct morphisms (see \cref{ex:moncat-prod-coprod-context}).

\begin{thm}\label{thm:moncat-prod-coprod-subadm}
  Substitution is admissible in the {simple type theory for distributive monoidal categories with products}: given derivations of
  $\Gamma\types M:A$ and $\Delta,x:A,\Psi\types N:B$, we can construct a derivation of $\Delta,\Gamma,\Psi\types M[N/x]:B$.
  Moreover, it is associative and interchanging.
\end{thm}
\begin{proof}
  The defining equations are shown in \cref{fig:moncat-prod-coprod-sub}.
  They basically augment the rules from \cref{fig:moncat-sub} with versions of the rules from \cref{thm:catprod-subadm,thm:catcoprod-subadm}.
  Note the difference between the cases for $\tpair P Q$ and $\pair P Q$: in the first we recurse into only one of the subterms, while in the second we recurse into both.
  Also there are a couple of new rules for $\abort$ and $\case$ to deal with the fact that a free variable might occur in one of the case branches rather than the discriminee.
\end{proof}

\begin{figure}
  \centering
  \begin{alignat*}{2}
    x[M/x] &= M\\
    f(N_1,\dots,N_n)[M/x] &= f(N_1,\dots,N_i[M/x],\dots,N_n) &&\quad\text{if $x$ occurs in $N_i$}\\
    \tpair P Q[M/x] &= \tpair{P[M/x]}{Q} &&\quad\text{if $x$ occurs in $P$}\\
    \tpair P Q[M/x] &= \tpair{P}{Q[M/x]} &&\quad\text{if $x$ occurs in $Q$}\\
    \match_\tensor(N,uv.P)[M/x] &= \match_\tensor(N[M/x],uv.P) &&\quad\text{if $x$ occurs in $N$}\\
    \match_\tensor(N,uv.P)[M/x] &= \match_\tensor(N,uv.P[M/x]) &&\quad\text{if $x$ occurs in $P$}\\
    % \match_\tensor^{\Gamma|\Delta}(N,uv.P)[M/x] &= \match_\tensor^{\Gamma|\Delta}(N[M/x],uv.P) &&\quad\text{if $x$ occurs in $N$}\\
    % \match_\tensor^{\Gamma_1,x,\Gamma_2|\Delta}(N,uv.P)[M/x] &= \match_\tensor^{\Gamma_1,\Psi,\Gamma_2|\Delta}(N,uv.P[M/x]) &&\quad\text{$\Psi$ the context of $M$}\\
    % \match_\tensor^{\Gamma|\Delta_1,x,\Delta_2}(N,uv.P)[M/x] &= \match_\tensor^{\Gamma|\Delta_1,\Psi,\Delta_2}(N,uv.P[M/x]) &&\quad\text{$\Psi$ the context of $M$}\\
    \ott[M/x] &&&\quad\text{cannot happen}\\
    \match_\one(N,P)[M/x] &= \match_\one(N[M/x],P) &&\quad\text{if $x$ occurs in $N$}\\
    \match_\one(N,P)[M/x] &= \match_\one(N,P[M/x]) &&\quad\text{if $x$ occurs in $P$}\\
    \ttt(\vec y,x,\vec z)[M/x] &= \ttt(\vec y,\vec w, \vec z) &&\quad\text{$\vec w$ the free variables of $M$}\\
    (\pi_1(N))[M/x] &= \pi_1(N[M/x])\\
    (\pi_2(N))[M/x] &= \pi_2(N[M/x])\\
    \pair{P}{Q}[M/x] &= \pair{P[M/x]}{Q[M/x]}\\
    \abort(N)[M/x] &= \abort(N[M/x]) &&\quad\text{if $x$ occurs in $N$}\\
    \abort(N)[M/x] &= \abort(N) &&\quad\text{if $x$ not in $N$}\\
    \inl(N)[M/x] &= \inl(N[M/x])\\
    \inr(N)[M/x] &= \inr(N[M/x])\\
    \case(N,u.P,v.Q)[M/x] &= \case(N[M/x],u.P,v.Q) &&\quad\text{if $x$ occurs in $N$}\\
    \case(N,u.P,v.Q)[M/x] &= \case(N,u.P[M/x],v.Q[M/x]) &&\quad\text{if $x$ occurs in $P,Q$}
  \end{alignat*}
  \caption{Substitution for distributive monoidal categories with products}
  \label{fig:moncat-prod-coprod-sub}
\end{figure}

The $\beta$- and $\eta$-conversion rules are likewise obtained by combining those of \cref{sec:multicat-moncat,sec:catprod,sec:catcoprod}; they are shown in \cref{fig:moncat-prod-coprod-equiv}.

\begin{figure}
  \centering
  \begin{mathpar}
    \match_\tensor(\tpair M N,xy.P) \equiv P[M/x,N/y]\and
    \match_\tensor(M,xy.N[\tpair xy/u]) \equiv N[M/u]\\
    \match_\one(\ott,N) \equiv N\and
    \match_\one(M,N[\ott/u]) \equiv N[M/u]\\
    \pi_1(\pair M N) \equiv M\and
    \pi_2(\pair M N) \equiv N\\
    \pair{\pi_1(M)}{\pi_2(M)} \equiv M\and
    \ttt(x_1,\dots,x_n) \equiv M \\
    \case(\inl(M),u.P,v.Q) \equiv P[M/u]\and
    \case(\inr(M),u.P,v.Q) \equiv P[M/v]\and
    \case(M,u.P[\inl(u)/y],v.P[\inr(v)/y]) \equiv P[M/y]\and
    \abort(M) \equiv P[M/y]\and
  \end{mathpar}
  \caption{Equality rules for distributive monoidal categories with products}
  \label{fig:moncat-prod-coprod-equiv}
\end{figure}

\begin{thm}\label{thm:moncat-prod-coprod-initial}
  The free distributive monoidal category with products generated by a multigraph \cG is presented by this theory in the usual way: its morphisms are the derivations of $\Gamma\types M$ (or the derivable terms $\Gamma\types M:A$) modulo $\equiv$.
\end{thm}
\begin{proof}
  As usual, \cref{thm:moncat-prod-coprod-subadm} gives us a multicategory, and the rules for the operations $\tensor,\one,\times,\ttt,+,\zero$ make it representable and give it products and coproducts.
  Initiality then follows by the usual induction over derivations.
\end{proof}

There are two important things to note here.
Firstly, while there are a lot of rules in this type theory, each of them is essentially something we already understood from a previous section, and we were able to put them together essentially independently without worrying about how they interact.
This is a good example of the ``modularity'' of type theory, and the value of principle~\eqref{princ:independence} from \cref{sec:why-multicats}.

Secondly, even though the rules for $\tensor$ and $+$ are completely independent, we nevertheless obtained a nontrivial interaction between them (distributivity), \emph{because of the structure of the context} and how it mirrors the categorical notion of multicategory.
This suggests that we could obtain further properties and relationships between type operations by modifying the judgmental/context structure.
The categorical side of this involves moving to \emph{generalized multicategories}.

\subsection*{Exercises}

\begin{ex}\label{ex:moncat-prod-coprod-context}
  Find an example of a distributive monoidal category with products in which the two terms in~\eqref{eq:ttt-arg} represent distinct morphisms.
\end{ex}


\section{Some generalized multicategories}
\label{sec:cartmulti}

% \subsection{Cartesian clubs}
% \label{sec:cart-clubs}

We want to consider monoidal categories with ``something extra'', such as symmetric monoidal categories or cartesian monoidal categories.
To describe a type theory for monoidal categories of this sort, principle~\eqref{princ:structural} from \cref{sec:why-multicats} suggests that we should ask what additional structure this ``something extra'' induces on their underlying multicategories.
Because the morphisms $(A_1,\dots,A_n) \to B$ in the underlying multicategory of a monoidal category \cC are, by definition, the morphisms $A_1\tensor\cdots\tensor A_n \to B$ in \cC, the answer to this question depends on what morphisms between tensor products exist ``generically'' in monoidal categories of our desired sort.
Here are some examples.
\begin{enumerate}
\item If \cC is a symmetric monoidal category, we have symmetry isomorphisms $A_1\tensor\cdots\tensor A_n\toiso A_{\sigma 1}\tensor\cdots\tensor A_{\sigma n}$ for any permutation $\sigma\in S_n$.
  Thus, by precomposing with these isomorphisms, we obtain functions between multicategorical hom-sets
  \begin{equation}
    \sigma^*: \cC(A_{\sigma 1},\dots,A_{\sigma n}; B) \to \cC(A_1,\dots,A_n;B)\label{eq:symm-multicat-action}
  \end{equation}
  that satisfy appropriate axioms.
\item If \cC is a cartesian monoidal category, we have symmetries but also diagonals such as $A\to A\times A$ and projections such as $A\times B \to B$.
  In general, for any function $\sigma : \{1,\dots,m\} \to \{1,\dots,n\}$ we have a morphism
  \[ A_1\times \cdots\times A_n \too A_{\sigma 1} \times\dots\times A_{\sigma m} \]
  whose component $A_1\times \cdots\times A_n \to A_{\sigma k}$ is the projection onto the $(\sigma k)^{\mathrm{th}}$ factor.
  Precomposition with these morphisms yields analogous functions
  \begin{equation}
    \sigma^*: \cC(A_{\sigma 1},\dots,A_{\sigma m}; B) \to \cC(A_1,\dots,A_n;B).\label{eq:cartmulticat-action}
  \end{equation}
\item Less well-known than symmetric and cartesian monoidal categories are \emph{semicartesian} monoidal categories, whose unit object is the terminal object, but whose tensor product is not necessarily the cartesian product.
  (An example familiar to higher category theorists is the category $\mathbf{2Cat}$ with its Gray tensor product.)
  We will always assume that semicartesian monoidal categories are additionally symmetric.
  The semicartesianness gives us projections but not diagonals, leading to functions~\eqref{eq:cartmulticat-action} whenever $\sigma$ is \emph{injective}.
\item Even less well-known are \emph{relevance} monoidal categories, which are symmetric and equipped with a coherent system of diagonals $A \to A\tensor A$ but whose unit object is not in general terminal.
  A familiar example is the category of pointed sets with its smash product~\cite{dp:relevant-cats}.
  In this case we have functions~\eqref{eq:cartmulticat-action} only when $\sigma$ is \emph{surjective}.
\end{enumerate}

All of these cases can be encompassed by the following definitions.

\begin{defn}
  Let \fN be the full subcategory of \bSet whose objects are the sets $\{1,\dots, n\}$ for all integers $n\ge 0$.
  We regard it as a \emph{cocartesian} strict monoidal category, under the disjoint union operation $\{1,\dots,n\} \sqcup \{1,\dots,m\} = \{1,\dots,n+m\}$.
  Moreover, for any $\sigma : \{1,\dots,m\} \to \{1,\dots,n\}$ and $k_1,\dots,k_n$, let $\sigma \wr (k_1,\dots,k_n)$ denote the composite function
  \begin{equation*}
    \{1,\dots,\textstyle\sum_{i=1}^m k_{\sigma i} \}
     \toiso \bigsqcup_{i=1}^m \{1,\dots,k_{\sigma i}\}
     \xto{\widehat{\sigma}} \bigsqcup_{j=1}^n \{1,\dots,k_{j}\}
     \toiso \{1,\dots,\textstyle\sum_{j=1}^n k_j \}
  \end{equation*}
  where $\widehat{\sigma}$ acts as the identity from the $i^{\mathrm{th}}$ summand to the $(\sigma i)^{\mathrm{th}}$ summand.
  A \textbf{faithful cartesian club} is a subcategory $\fS\subseteq \fN$ such that
  \begin{enumerate}
  \item \fS contains all the objects of \fN.
  \item \fS is closed under the cocartesian monoidal structure, i.e.\ if $\sigma$ and $\tau$ are morphisms of $\fS$ then so is $\sigma\sqcup \tau$.
  \item \fS is closed under $\wr$, i.e.\ whenever it contains $\sigma$ it also contains $\sigma \wr (k_1,\dots,k_n)$.
  \end{enumerate}
\end{defn}

The above examples are the cases when \fS consists of the bijections, all the functions, the injections, or the surjections respectively.
There is also the trivial case when \fS contains only the identities.

\begin{defn}\label{defn:fS-multicategory}
  Let \fS be a faithful cartesian club.
  An \textbf{\fS-multicategory} is a multicategory \cM together with operations
  \begin{align*}
    \cM(A_{\sigma 1},\dots,A_{\sigma m}; B) &\to \cM(A_1,\dots,A_n;B)\\
    f &\mapsto f\sigma^*
  \end{align*}
  for all functions $\sigma : \{1,\dots,m\} \to \{1,\dots,n\}$ in \fS, satisfying the following axioms:
  \begin{enumerate}
  \item $f \sigma^* \tau^* = f(\tau\sigma)^*$\label{item:cartmulti-1}
  \item $f (\idfunc_n)^* = f$\label{item:cartmulti-2}
  \item $g\circ (f_1 \sigma_1^* ,\dots, f_n \sigma_n^*) = (g \circ (f_1,\dots,f_n))(\sigma_1\sqcup \cdots \sqcup \sigma_n)^*$\label{item:cartmulti-3}
  \item $g\sigma^* \circ (f_1,\dots,f_n) = (g\circ (f_{\sigma 1},\dots, f_{\sigma m}))(\sigma \wr (k_1,\dots,k_n))^*$ where $k_i$ is the arity of $f_i$.\label{item:cartmulti-4}
  \end{enumerate}
  If each hom-set $\cM(A_1,\dots,A_n;B)$ has at most one element, we call \cM an \textbf{\fS-multiposet}.
  An \textbf{\fS-multigraph} is a multigraph equipped with similar operations satisfying~\ref{item:cartmulti-1} and~\ref{item:cartmulti-2}.
\end{defn}

As special cases we have, by definition:
\begin{center}
\begin{tabular}{c|c}
  When $\fS=$ & $\fS$-multicategories are called\\\hline
  bijections & \textbf{symmetric multicategories}\\
  all functions & \textbf{cartesian multicategories}\\
  injections & \textbf{semicartesian (symmetric) multicategories}\\
  surjections & \textbf{relevance multicategories}\\
  only identities & (ordinary) multicategories
\end{tabular}
\end{center}

% \subsection{Products in \fS-multicategories}
% \label{sec:cartmulti-prod}

Now, recall the definition of tensor products in a multicategory from \cref{defn:multicat-tensor}, and the result of \cref{thm:multicat-repr} that having all tensor products (being ``representable'') is equivalent to being a monoidal category.
For a general faithful cartesian club \fS, we might as well \emph{define} an \textbf{\fS-monoidal category} to be an \fS-multicategory that is representable.
However, in many cases this is equivalent to a more familiar notion.

\begin{thm}\label{thm:symm-multicat-repr}
  If \fS includes all bijections, then the monoidal category obtained from any representable \fS-multicategory is symmetric.
  Moreover, the equivalence of \cref{thm:multicat-repr} induces an equivalence between representable symmetric multicategories and symmetric monoidal categories.
\end{thm}
\begin{proof}
  If $\chi : (A,B) \to A\tensor B$ is a tensor product, then by acting on it with the transposition $\sigma : \{1,2\} \toiso \{1,2\}$ we obtain a morphism $\chi\sigma^* : (B,A) \to A\tensor B$.
  Applying the universal property of the tensor product $(B,A) \to B\tensor A$, we get a map $B\tensor A \to A\tensor B$.
  We can similarly use the universal property to check the symmetry axioms.

  Conversely, the coherence theorem for symmetric monoidal categories yields isomorphisms~\eqref{eq:symm-multicat-action}, composing with which gives its underlying multicategory a symmetric structure.
  It is straightforward to verify that these constructions are inverses up to isomorphism.
\end{proof}

Recall from \cref{sec:multicats-catth} the definition of products in a multicategory.

\begin{thm}\label{thm:semicart-multicat-repr}
  If \fS includes all injections, then an object $\one$ is terminal if and only if it is a unit object (i.e.\ there is a universal tensor product morphism $()\to \one$).
  Moreover, the equivalence of \cref{thm:multicat-repr} induces an equivalence between representable semicartesian multicategories and semicartesian monoidal categories.
\end{thm}
\begin{proof}
  If \fS includes injections, then for any $A_1,\dots,A_n$ the injection $\emptyset \to \{1,\dots,n\}$ induces a map
  \[ \cM(;B) \to \cM(A_1,\dots,A_n;B). \]
  Thus, if $\one$ is a unit object with universal morphism $\chi:()\to \one$, then this gives us induced maps $e_{A_1,\dots,A_n}:(A_1,\dots,A_n) \to \one$.
  Moreover, the fourth ``equivariance'' axiom of an \fS-multicategory implies that these maps are natural, in the sense that
  $e_{A_1,\dots,A_n} \circ (f_1,\dots,f_n) = e_{B_1,\dots,B_m}$ for any $f_1,\dots,f_n$.
  In particular, $e_{\one} \circ \chi = e_{()} = \chi$; so by the universal property of $\chi$, we have $e_{\one} = \idfunc_{\one}$.
  A standard argument (generalized from categories to multicategories) now implies that $\one$ is terminal.

  Conversely, suppose $\unit$ is terminal.
  Then in particular, we have a unique morphism $\chi : ()\to \unit$, and acting on $\chi$ by the injection $\emptyset \to \{1,\dots,n\}$ can only yield the unique morphism $(A_1,\dots,A_n)\to \unit$.
  Now we have to show that
  \[ (-\circ_{n+1} \chi) : \cM(A_1,\dots,A_n,\unit,B_1,\dots,B_m; C) \to \cM(A_1,\dots,A_n,B_1,\dots,B_m; C) \]
  is a bijection.
  But we have a map in the other direction given by acting with an appropriate injection, and the equivariance properties imply that this is an inverse.

  Lastly, if we have a semicartesian monoidal category, then for any injection $\sigma$ we have a map
  \[ A_1\tensor \cdots\tensor A_n \too A_{\sigma 1} \tensor\dots\tensor A_{\sigma m} \]
  defined by mapping each $A_j$ not in the image of $\sigma$ to the terminal object $1$, then removing those copies of $1$ from the tensor product since they are also the tensor unit (and finally permuting if necessary).
  It is straightforward to verify that these actions give a semicartesian multicategory, and that that these constructions are inverses up to isomorphism.
\end{proof}

\begin{thm}\label{thm:cart-multicat-repr}
  If \fS consists of all functions (i.e.\ we are in a cartesian multicategory), then products $A\times B$ are in bijective correspondence with tensor products $A\tensor B$.
  Moreover, the equivalence of \cref{thm:multicat-repr} induces an equivalence between representable cartesian multicategories and cartesian monoidal categories (i.e.\ categories with finite products).
\end{thm}
\begin{proof}
  By acting with injections, for any $A,B$ we obtain morphisms $(A,B)\to A$ and $(A,B)\to B$.
  Thus, if $A\times B$ is a product, we have an induced map $\chi:(A,B)\to A\times B$.
  Now if we have any morphism $(C_1,\dots,C_n,A,B,D_1,\dots,D_m)\to E$, we can compose with the two projections of the product to get a morphism $(C_1,\dots,C_n,A\times B,A\times B,D_1,\dots,D_m)\to E$, and then act by a surjection to get $(C_1,\dots,C_n,A\times B,D_1,\dots,D_m)\to E$.
  The equivariance properties of a cartesian multicategory, and the universal property of the product, imply that this operation is inverse to composing with $\chi$, so that the latter is a tensor product.

  Conversely, if $\chi:(A,B)\to A\tensor B$ is a tensor product, by applying its universal property to the above morphisms $(A,B)\to A$ and $(A,B)\to B$ we obtain projections $A\tensor B \to A$ and $A\tensor B\to B$.
  Now given $f:(C_1,\dots,C_n)\to A$ and $g:(C_1,\dots,C_n)\to B$, we have $\chi\circ (f,g) : (C_1,\dots,C_n,C_1,\dots,C_n) \to A\tensor B$, and by acting with a suitable surjection we get $(C_1,\dots,C_n)\to A\tensor B$.
  Again, the equivariance properties and the universal property of the tensor product imply that this is a unique factorization of $f$ and $g$ through the projections.
\end{proof}

Note although the first conclusion of \cref{thm:cart-multicat-repr} refers only to binary products, it still requires the presence of \emph{injections} in \fS in addition to surjections.
Indeed, the monoidal category of pointed sets with its smash product has an underlying multicategory that is relevance (i.e.\ admits an action by all surjections), but the smash product is different from the cartesian product.
It is also possible to characterize the \fS-monoidal categories when \fS is the injections, but we leave this to the interested reader; see \cref{ex:relevance-moncat}.

\begin{rmk}\label{rmk:absolute}
  \cref{thm:semicart-multicat-repr,thm:cart-multicat-repr} identify an object having a ``mapping out'' universal property (a tensor product or unit object in a multicategory) with an object having a ``mapping in'' universal property (a cartesian product or terminal object), in the strong sense that if either exists then it is also the other.
  This sort of ``ambidextrous'' universal property appears elsewhere in category theory as well.
  For instance, the splitting of an idempotent can be regarded as either a limit or a colimit; in a category enriched over abelian monoids, finite products and coproducts coincide; and more generally for any kind of enrichment there is a notion of ``absolute (co)limit''~\cite{street:absolute}.
  Thus, although multicategories are not ``enriched categories'' in the usual sense, we could say informally that in a cartesian multicategory products are absolute limits, while in a semicartesian multicategory terminal objects are.
  See also \cref{ex:absolute}.
\end{rmk}

Finally, we observe that closedness can be naturally characterized multicategorically.
Suppose for simplicity that \fS contains at least all bijections.
Then we say an \fS-multicategory is \textbf{closed} if for each pair of objects $A$ and $B$ there is a specified object $A\hom B$ and a morphism $\chi : (A\hom B,A) \to B$ postcomposition with which defines bijections
\[ (\chi\circ_1 -): \cM(C_1,\dots,C_n;A\hom B) \toiso \cM(C_1,\dots,C_n,A;B) \]
for all $C_1,\dots,C_n$.
(If \fS does not contain the bijections, we would just have to consider ``left and right closedness'' separately.)
The following is then straightforward.

\begin{thm}\label{thm:moncat-repr-closed}
  A symmetric monoidal category is closed if and only if its underlying multicategory is.
  Moreover, for all the above values of \fS that contain the bijections, this defines an equivalence of categories.\qed
\end{thm}

Of course, cartesian closed categories are just closed cartesian monoidal categories, so they are equivalent to closed cartesian multicategories.

\subsection*{Exercises}

\begin{ex}\label{ex:gen-multicat-repr}
  Fill in the details in the proof of \cref{thm:symm-multicat-repr,thm:semicart-multicat-repr,thm:cart-multicat-repr}.
\end{ex}

\begin{ex}\label{ex:club-generators}
  Let \fS be a faithful cartesian club.
  \begin{enumerate}
  \item Prove that if \fS contains the transposition $\{1,2\}\toiso \{1,2\}$, then it contains all bijections.
  \item Prove that if \fS contains the transposition $\{1,2\}\toiso \{1,2\}$ and also the injection $\emptyset \to \{1\}$, then it contains all injections.
  \item Prove that if \fS contains the transposition $\{1,2\}\toiso \{1,2\}$ and also the surjection $\{1,2\} \to \{1\}$, then it contains all surjections.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:cartmulti-oneplace}
  Define one-place versions of \fS-multicategories and show that they are equivalent to the multi-composition version defined in the text.
\end{ex}

\begin{ex}\label{ex:distrib}
  Show that representable cartesian multicategories with coproducts are equivalent to distributive categories.
\end{ex}

\begin{ex}\label{ex:absolute}
  Of course, for any \fS a \textbf{functor} between \fS-multicategories is required to preserve the $\sigma$-actions.
  Prove that:
  \begin{enumerate}
  \item Any functor between semicartesian multicategories must preserve unit objects / terminal objects.
  \item Any functor between cartesian multicategories must preserve tensor products / cartesian products.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:relevance-moncat}
  Define a notion of \textbf{relevance monoidal category}, by adding ``natural diagonals'' to a symmetric monoidal category, and show that such monoidal categories are equivalent to representable relevance multicategories.
  (See~\cite{dp:relevant-cats}.)
\end{ex}

\begin{ex}\label{ex:cocartesian-clubs}
  Define a notion of \textbf{faithful cocartesian club} and a corresponding notion of generalized multicategory that includes \emph{cocartesian} monoidal categories as the maximal case.
\end{ex}


\section{Intuitionistic logic}
\label{sec:logic}

We are now aiming at type theories for the generalized multicategories considered in \cref{sec:cartmulti}, along with the extra structures that they may have (tensor products, cartesian products, coproducts, and closedness).
In this section we start with the posetal case, which is also where our type theory at last begins to look rather like \emph{logic}.

\subsection{\fS-monoidal lattices}
\label{sec:monoidal-lattices}

According to principle~\eqref{princ:structural} from \cref{sec:why-multicats}, the additional action by $\sigma$'s in an \fS-multicategory should be represented by \emph{structural rules} in a type theory.
These rules are generally formulated and named as follows.
\begin{mathpar}
  \inferrule*[right=exchange]{\Gamma,A,B,\Delta\types C}{\Gamma,B,A,\Delta\types C}\and
  \inferrule*[right=weakening]{\Gamma,\Delta\types C}{\Gamma,A,\Delta\types C}\and
  \inferrule*[right=contraction]{\Gamma,A,A,\Delta\types C}{\Gamma,A,\Delta\types C}\and
\end{mathpar}
The correspondence between kinds of multicategory and structural rules\footnote{One can consider weakening and/or contraction without exchange, just as one might consider non-symmetric semicartesian or relevance multicategories.
  But this takes us rather far afield from categorical structures of general interest, so we leave it to the reader.} should not be surprising:
\begin{center}
\begin{tabular}{c|c}
  $\fS$-multicategories & structural rules\\\hline
  {symmetric multicategories} & exchange\\
  {cartesian multicategories} & exchange, weakening, contraction\\
  {semicartesian (symmetric) multicategories} & exchange, weakening\\
  {relevance multicategories} & exchange, contraction
\end{tabular}
\end{center}
Note that these structural rules refer only to single transpositions, projections, and duplications, rather than arbitrary functions in \fS.
This is similar to how, as we have noted, cut is usually stated in type theory using one-place composites rather than a multi-composition.
As in that case, the smaller operations suffice to generate the more general ones (c.f.\ \cref{ex:club-generators}).

What is somewhat less clear is how these rules can be made \emph{admissible} in line with principle~\eqref{princ:adm}.
For now let us ignore this question and take these rules (when we want them) as \emph{primitive} (recall \cref{rmk:admissible-derivable-1}).
This makes the treatment more parametric in \fS, and makes little difference for presenting free posets, since in that case we are only interested in the existence or nonexistence of derivations.
We will address the question of admissibility in \cref{sec:heyting-algebras,sec:cartmoncat,sec:symmoncat}.

For the rest of this subsection, let \fS be one of the four possibilities above (so in particular, it will always contain the bijections).
All our type theories will then include the appropriate primitive structural rules, according to the above table.
Our type operations will be the posetal versions of all the ones we saw in \cref{sec:multicat-prod-coprod} --- $\tensor,\one,\meet,\top,\join,\bot$ --- and also an internal-hom for $\tensor$, which we denote by $A\hom B$.
(We postponed introducing the internal-hom until now only to avoid worrying about left- versus right-closedness in non-symmetric multicategories.)
Thus, the categorical structure in question is \emph{closed \fS-monoidal lattices}.
Of course, as in \cref{sec:multicat-prod-coprod} we can remove any of these operations without affecting the others, obtaining a type theory for weaker categorical structures.

The primitive rules of the \textbf{natural deduction for closed \fS-monoidal lattices} are shown in \cref{fig:natded-logic}.
Except for the structural rules (discussed above) and $\hom$, they are all obtained by removing the term annotations from the theory of \cref{sec:multicat-prod-coprod}.
The only other change is that since we always include the exchange rule as primitive, in the rules $\tensorE,\one E,\joinE,\zero E$ we don't need to put $\Psi$ in the middle of the context but are free to put it on one side.
As usual, we have also omitted the rules for the judgment $\types A\type$, which just say that all the objects of $\cG$ are types, as are $\one,\top,\bot$ and $A\tensor B, A\meet B, A\join B, A\hom B$ if $A$ and $B$ are.

\begin{figure}
  \centering
  \begin{mathpar}
  \inferrule*[right=exchange]{\Gamma,A,B,\Delta\types C}{\Gamma,B,A,\Delta\types C}\\
  \inferrule*[right=weakening]{\Gamma,\Delta\types C}{\Gamma,A,\Delta\types C}\;\text{if injections}\subseteq\fS\\
  \inferrule*[right=contraction]{\Gamma,A,A,\Delta\types C}{\Gamma,A,\Delta\types C}\;\text{if surjections}\subseteq\fS\\
  \inferrule{\types A\type}{A\types A}\and
  \inferrule{(A_1,\dots,A_n \le B)\in\cG \\ \Gamma_1\types A_1 \\ \dots \\ \Gamma_n \types A_n}{\Gamma_1,\dots,\Gamma_n\types B}\and
  \inferrule{\Gamma\types A \\ \Delta\types B}{\Gamma,\Delta\types A\tensor B}\;\tensorI\and
  \inferrule{
    \Psi \types A\tensor B \\
    \Gamma,A,B\types C
  }{
    \Gamma,\Psi \types C
  }\;\tensorE\\
  \inferrule{ }{()\types \one}\;\one I\and
  \inferrule{
    \Psi\types \one \\
    \Gamma\types A
  }{
    \Gamma,\Psi\types C
  }\;\one E\\
    \inferrule{\Gamma\types A \\ \Gamma\types B}{\Gamma\types A\meet B}\;\meetI
    \and
    \inferrule{\Gamma\types A\meet B}{\Gamma\types A}\;\meetE1
    \and
    \inferrule{\Gamma\types A\meet B}{\Gamma\types B}\;\meetE2
    \\
    \inferrule{ }{\Gamma\types \top}\;\top I
    \and
    \inferrule{\Psi\types \bot}{\Gamma,\Psi\types C}\;\bot E
    \\
    \inferrule{\Gamma\types A}{\Gamma\types A\join B}\;\joinI1
    \and
    \inferrule{\Gamma\types B}{\Gamma\types A\join B}\;\joinI2
    \and
    \inferrule{
      \Psi\types A\join B \\ \Gamma,A \types C \\ \Gamma,B\types C
    }{\Gamma,\Psi \types C}\;\joinE\\
    \inferrule{\Gamma,A\types B}{\Gamma\types A\hom B}\;\homI\and
    \inferrule{\Psi\types A\\\Gamma\types A\hom B}{\Gamma,\Psi\types B}\;\homE
  \end{mathpar}
  \caption{Natural deduction for closed \fS-monoidal lattices}
  \label{fig:natded-logic}
\end{figure}

The introduction rule for $\hom$ is simply one direction of its universal property from \cref{sec:multicats-catth}.
The elimination rule is the inverse direction, but with a cut built in to make the context of the conclusion general (modulo a splitting).
That is, $\homE$ can be derived from the opposite of $\homI$ and cut:
\begin{mathpar}
  \inferrule*{\Psi\types A\\ \inferrule*{\Gamma\types A\hom B}{\Gamma,A\types B}}{\Gamma,\Psi\types B}
\end{mathpar}
Note that, as promised in \cref{sec:why-multicats}, by using sequents with multiple types in the context, we can formulate the rules for $\hom$ without reference to $\meet/\times$.

The contraction rule gives the cut-admissibility theorem a new wrinkle.
Let us first consider the cases without contraction, which are more straightforward.

\begin{lem}\label{thm:natded-logic-cutadm}
  If \fS consists of the bijections or the injections, then cut is admissible in the natural deduction for closed \fS-monoidal lattices: if we have derivations of $\Psi\types A$ and $\Gamma,A,\Delta\types B$ then we also have $\Gamma,\Psi,\Delta\types B$.
\end{lem}
\begin{proof}
  As always, we induct on the derivation of $\Gamma,A,\Delta\types B$.
  The cases for most of the connectives are just like those in \cref{thm:moncat-prod-coprod-subadm}, and those for $\hom$ are nothing new.
  However, now we have a new possibility: the derivation might end with a primitive structural rule (exchange or weakening --- our hypothesis on \fS rules out contraction).

  Firstly, if the structural rule does not affect the type $A$, then we can simply commute it past the cut.
  For instance, if we have $\Psi\types A$ and $\Gamma,A,\Delta_1,C,\Delta_2\types B$ arising by weakening from $\Gamma,A,\Delta_1,\Delta_2\types B$, we can inductively obtain $\Gamma,\Psi,\Delta_1,\Delta_2\types B$ and then apply weakening again to get $\Gamma,\Psi,\Delta_1,C,\Delta_2\types B$.

  Secondly, essentially the same is true if it is an exchange that does affect $A$.
  For instance, if we have $\Psi\types A$ and $\Gamma,A,C,\Delta\types B$ arising by exchange from $\Gamma,C,A,\Delta\types B$, we can inductively obtain $\Gamma,C,\Psi,\Delta\types B$, and then re-apply exchange once for each type in $\Psi$ to get $\Gamma,\Psi,C,\Delta\types B$.
  (It does matter here that we have formulated the admissible cut rule {with} $A$ in the middle of the context rather than on one side, even though we have the exchange rule; otherwise the induction would fail to go through here.)

  Finally, suppose it is a weakening that affects $A$, so we have $\Psi\types A$ and $\Gamma,A,\Delta\types B$ arising by weakening from $\Gamma,\Delta\types B$.
  In this case we can forget about the derivation of $\Psi\types A$ and just weaken $\Gamma,\Delta\types B$ once for each type in $\Psi$ to get $\Gamma,\Psi,\Delta\types B$.
\end{proof}

If we try to extend this to theories with contraction, however, we have a problem.
Suppose the derivation of $\Gamma,A,\Delta\types B$ ends with a contraction that affects $A$, so that we have $\Psi\types A$ and $\Gamma,A,\Delta\types B$ arising by contraction from $\Gamma,A,A,\Delta\types B$.
Then we would like to inductively cut the latter with $\Psi\types A$ twice to obtain $\Gamma,\Psi,\Psi,\Delta\types B$, transforming
\begin{equation*}
  \inferrule*[right=cut]{\Psi\types A \\
  \inferrule*[Right=contraction]{\Gamma,A,A,\Delta\types B}{\Gamma,A,\Delta\types B}}{\Gamma,\Psi,\Delta\types B}
\end{equation*}
into
\begin{equation*}
  \inferrule*[Right=cut]{\Psi\types A\\
    \inferrule*[Right=cut]{\Psi\types A\\\Gamma,A,A,\Delta\types B}{\Gamma,\Psi,A,\Delta,\types B}}
  {\Gamma,\Psi,\Psi,\Delta\types B}
\end{equation*}
After this we could apply exchanges to pair up the two copies of each type in $\Psi$, and finally a contraction on each of them to eliminate the duplicates.
However, now we have the sort of problem that we did in the proof of \cref{thm:monpos-cutadm}: the derivation of $\Gamma,\Psi,A,\Delta,\types B$ that we obtain from our first application of the inductive hypothesis may not be ``smaller'' than our given derivation, so we cannot apply the inductive hypothesis to it again.
Moreover, the solution sketched there (inducting first on types and then on derivations) does not work here, since the types are not changing.

The standard solution used in type theory is to generalize the cut rule to a rule called ``mix'' that enables the induction to go through.
In our case, the mix rule says that if we have derivations of $\Psi\types A$ and $\Gamma\types B$, where $\Gamma$ contains one or more copies of $A$, then we can construct a derivation of $\Psi,\Gamma^A\types B$, where $\Gamma^A$ is $\Gamma$ with one or more copies of $A$ removed.
In other words, we build a certain amount of contraction into the induction hypothesis.
This works, but a more categorically principled solution is to use the multi-cut as in \cref{thm:monpos-multicutadm}.
This amounts to approximately the same thing, but feels less \textit{ad hoc} to a category theorist (at least, it does to the author).

\begin{lem}\label{thm:natded-logic-multicutadm}
  For any of our four \fS's, multi-cut is admissible in the natural deduction for closed \fS-monoidal lattices: if we have derivations of $\Psi_i\types A_i$ for $1\le i\le n$, and also $A_1,\dots,A_n \types B$, then we can construct a derivation of $\Psi_1,\dots,\Psi_n\types B$.
\end{lem}
\begin{proof}
  The non-structural rules are easy, just as before.
  (Recall that in general, cut is very straightforward for natural deductions because all the rules act only on the right.
  With this in mind it is unsurprising that primitive structural rules are problematic, since they act on the left.)

  Now, however, the structural rules are almost just as easy.
  If our derivation of $A_1,\dots,A_n \types B$ ends with an exchange, we can simply switch two of the derivations $\Psi_i\types A_i$ and induct.
  Similarly, if it ends with a weakening, we can just forget about one of the $\Psi_i\types A_i$ and induct.
  Finally, if it ends with a contraction, we can again induct on the premise, using one of the derivations $\Psi_i\types A_i$ twice.
\end{proof}

Now we can prove the initiality theorem just as usual.

\begin{thm}\label{thm:natded-logic-initial}
  For any relational multigraph \cG and any of our four \fS's, the free closed \fS-monoidal lattice on \cG can be presented by this natural deduction, with $(A_1,\dots,A_n)\le B$ holding just when $A_1,\dots,A_n\types B$ is derivable.
\end{thm}
\begin{proof}
  \cref{thm:natded-logic-multicutadm} (together with the identity rule) gives us a multiposet, the rules for the type operations make it representable, closed, and a lattice, and the structural rules make it an \fS-multiposet.
  Thus it lives in the correct category; and its freeness follows by induction as usual.
\end{proof}


\subsection{Heyting algebras}
\label{sec:heyting-algebras}

Let us now specialize to the cartesian case, where we have all three structural rules.
Thus the categorical structure in question is \emph{cartesian closed lattices}, which are also known as \textbf{Heyting algebras}.
This theory is simpler because $\tensor$ and $\one$ coincide with $\meet$ and $\top$ (see \cref{ex:cart-typetheory}), so we can omit the former ones.
A second reason it is simpler is because it is easy to make the structural rules admissible.
The key observation is the following.

\begin{lem}\label{thm:cart-constctx}
  In the presence of exchange, contraction, and weakening, the following rules are inter-derivable with the rules $\bot E$, $\joinE$, $\homE$ from \cref{fig:natded-logic}.
  \begin{mathpar}
    \inferrule{\types A\type \\ A\in \Gamma}{\Gamma\types A}\;\idfunc'
    \and
    \inferrule{(A_1,\dots,A_n \le B)\in\cG \\ \Gamma\types A_1 \\ \dots \\ \Gamma \types A_n}{\Gamma\types B}\;f'
    \and
    \inferrule{\Gamma\types \bot}{\Gamma\types C}\;\bot E'
    \and
    \inferrule{
      \Gamma\types A\join B \\ \Gamma,A \types C \\ \Gamma,B\types C
    }{\Gamma \types C}\;\joinE'
    \and
    \inferrule{\Gamma\types A\hom B \\ \Gamma\types A}{\Gamma\types B}\;\homE'
  \end{mathpar}
\end{lem}
\begin{proof}
  Here are the referenced rules from \cref{fig:natded-logic}:
  \begin{mathpar}
    \inferrule{\types A\type}{A\types A}\;\idfunc
    \and
    \inferrule{(A_1,\dots,A_n \le B)\in\cG \\ \Gamma_1\types A_1 \\ \dots \\ \Gamma_n \types A_n}{\Gamma_1,\dots,\Gamma_n\types B}\;f
    \and
    \inferrule{\Psi\types \bot}{\Gamma,\Psi\types C}\;\bot E
    \and
    \inferrule{
      \Psi\types A\join B \\ \Gamma,A \types C \\ \Gamma,B\types C
    }{\Gamma,\Psi \types C}\;\joinE
    \and
    \inferrule{\Gamma\types A\hom B \\ \Psi\types A}{\Gamma,\Psi\types B}\;\homE
  \end{mathpar}
  Clearly $\idfunc$ is a special case of $\idfunc'$, while conversely $\idfunc'$ can be derived from $\idfunc$ followed by weakening.
  And $\bot E'$ is a special case of $\bot E$, while $f'$ and $\joinE'$ and $\homE'$ can be derived from $f$ and $\joinE$ and $\homE$ followed by exchange and contraction to turn contexts like $\Gamma,\Gamma$ into $\Gamma$.
  Conversely, given the premises of any of these ``unprimed'' rules, we can weaken each $\Gamma$ and $\Psi$ to $\Gamma,\Psi$ (or $\Gamma_i$ to $\Gamma_1,\dots,\Gamma_n$ in the case of $f$), then apply the primed version of that rule to deduce the conclusion of the unprimed rule.
\end{proof}

If we replace the rules in question by their modified versions, then all the rules will have the property that the context of the conclusion is arbitrary, while the context of the premises differ from the context of the conclusion at most by addition of a new type.
In other words, as we proceed \emph{down} a derivation tree, we only ever \emph{remove} types from the context; and dually as we proceed \emph{up} a tree we only ever \emph{add} to the context.
This will enable us to ``push the structural rules up'' past all primitive rules until we get to $\idfunc$, thereby making them admissible.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\types A\type \\ A\in \Gamma}{\Gamma\types A}\;\idfunc
    \and
    \inferrule{(A_1,\dots,A_n \le B)\in\cG \\ \Gamma\types A_1 \\ \dots \\ \Gamma \types A_n}{\Gamma\types B}\;f
    \and
    \inferrule{\Gamma\types A \\ \Gamma\types B}{\Gamma\types A\meet B}\;\meetI
    \and
    \inferrule{\Gamma\types A\meet B}{\Gamma\types A}\;\meetE1
    \and
    \inferrule{\Gamma\types A\meet B}{\Gamma\types B}\;\meetE2
    \\
    \inferrule{ }{\Gamma\types \top}\;\top I
    \and
    \inferrule{\Gamma\types \bot}{\Gamma\types C}\;\bot E
    \\
    \inferrule{\Gamma\types A}{\Gamma\types A\join B}\;\joinI1
    \and
    \inferrule{\Gamma\types B}{\Gamma\types A\join B}\;\joinI2
    \and
    \inferrule{
      \Gamma\types A\join B \\ \Gamma,A \types C \\ \Gamma,B\types C
    }{\Gamma \types C}\;\joinE\\
    \inferrule{\Gamma,A\types B}{\Gamma\types A\To B}\;\ToI\and
    \inferrule{\Gamma\types A\To B \\ \Gamma\types A}{\Gamma\types B}\;\ToE
  \end{mathpar}
  \caption{Natural deduction for Heyting algebras}
  \label{fig:natded-heyting}
\end{figure}

For convenience, we collect all the rules of this modified \textbf{natural deduction for Heyting algebras} in \cref{fig:natded-heyting}.
Note that we change our notation and write $A\hom B$ as $A\To B$.

\begin{lem}\label{thm:heyting-strucadm}
  All the structural rules of exchange, weakening, and contraction are admissible in the natural deduction for Heyting algebras.
\end{lem}
\begin{proof}
  It will suffice to prove admissibility of the following rule, for any function $\sigma : \{1,\dots,m\} \to \{1,\dots,n\}$:
  \begin{mathpar}
    \inferrule*{A_{\sigma 1},\dots,A_{\sigma m} \types B}{A_1,\dots,A_n \types B}
  \end{mathpar}
  This is almost immediate from the fact that the premises of all rules have the same context as the conclusion, perhaps with a type added: regardless of how a derivation of ${A_{\sigma 1},\dots,A_{\sigma m} \types B}$ ends, we can apply the inductive hypothesis to its premises (perhaps passing to $\sigma\sqcup\idfunc : \{1,\dots,m+1\} \to \{1,\dots,n+1\}$) and then re-apply the final rule.

  The only exception is the rule $\idfunc$, for which we observe that if $A$ appears in the context $A_{\sigma 1},\dots,A_{\sigma m}$, then $A=A_{\sigma j}$ for some $1\le j\le m$, and hence $A=A_i$ for some $1\le i\le n$ (namely, $i=\sigma j$).
  Thus, we can also apply the same rule to obtain ${A_1,\dots,A_n \types A_i}$.
\end{proof}

\begin{lem}\label{thm:heyting-initial}
  The free Heyting algebra on a relational multigraph \cG can be described by the natural deduction for Heyting algebras.
\end{lem}
\begin{proof}
  Left to the reader.
  This will also follow as a special case of \cref{thm:stlc-initial}.
\end{proof}


\subsection{Natural deduction}
\label{sec:natded-logic}

Let us now say a few words about what the natural deduction for Heyting algebras has to do with logic.
For a reader who thinks of logical connectives in terms of their action on truth values (e.g. ``if $A$ then $B$'' is true unless $A$ is true and $B$ is false), one way to make the connection to logic is to note that the poset of truth values
\[ \tv = \{ \mathrm{false} < \mathrm{true} \}\]
is a Heyting algebra, where the operations $\meet,\top,\join,\bot,\To$ correspond to ``and'', ``true'', ``or'', ``false'', and ``implies''.

Now suppose \cG is a relational multigraph, whose objects we call \textbf{propositional variables}, and suppose furthermore that we have a map of relational multigraphs $\nu:\cG\to\tv$.
In other words, we assign a truth value to each propositional variable, in such a way that if $(A_1,A_2,\dots,A_n)\le B$ in \cG, and if $\nu(A_i)$ is true for all $i$, then also $\nu(B)$ is true.
Then by \cref{thm:natded-logic-initial} we have an induced map $\F\bHeyting\cG \to \tv$ of Heyting algebras.

The objects of $\F\bHeyting\cG$ are \emph{propositional formulas}, built out of the propositional variables by the operations $\meet,\top,\join,\bot,\To$ which we now regard as denoting the logical connectives ``and'', ``true'', ``or'', ``false'', and ``implies''.
Since $\F\bHeyting\cG \to \tv$ is a map of Heyting algebras, it extends the truth assignment $\nu$ to all such formulas by using the ``truth tables'' for all the connectives, e.g.\ $\nu(A\meet B)$ is true just when $\nu(A)$ and $\nu(B)$ are both true, etc.
Finally, the fact that $\F\bHeyting\cG \to \tv$ preserves inequalities means that if $A_1,\dots,A_n \types B$ is derivable in the natural deduction for Heyting algebras, and if $\nu(A_i)$ is true for all $i$, then also $\nu(B)$ is true (where now the $A_i$ and $B$ are arbitrary formulas, not just propositional variables).

As a special case, if \cG has no nontrivial relations, then any derivable judgment $()\types B$ exhibits the propositional formula $B$ as a \textbf{tautology}: a statement that becomes true \emph{whatever} truth values are substituted for its propositional variables.
For instance, here is a derivation exhibiting $(A\meet (B\join C)) \To ((A\meet B) \join (A\meet C))$ as a tautology:
\begin{mathpar}
  \tiny
  \let\mymeet\meet
  \def\meet{\mathord{\mymeet}}
  \let\myjoin\join
  \def\join{\mathord{\myjoin}}
  \inferrule*{
      \inferrule*{
        \inferrule*{\inferrule*{ }{A\meet (B\join C) \types A\meet (B\join C)
          }}{A\meet (B\join C) \types B\join C}\\
        \inferrule*{\inferrule*{
            \inferrule*{\inferrule{ }{
                  A\meet (B\join C),B\types A\meet (B\join C)
              }}{A\meet (B\join C),B \types A} \\
            \inferrule*{ }{
              A\meet (B\join C),B \types B}
          }{A\meet (B\join C),B \types A\meet B
        }}{A\meet (B\join C),B \types (A\meet B) \join (A\meet C)}\\
        \text{(and dually)}
        }{A\meet (B\join C)\types (A\meet B) \join (A\meet C)}
  }{()\types (A\meet (B\join C)) \To ((A\meet B) \join (A\meet C))}
\end{mathpar}
% VERSION WITH PRIMITIVE STRUCTURAL RULES:
% \begin{mathpar}
%   \tiny
%   \let\mymeet\meet
%   \def\meet{\mathord{\mymeet}}
%   \let\myjoin\join
%   \def\join{\mathord{\myjoin}}
%   \inferrule*{
%     \inferrule*{
%       \inferrule*{
%         \inferrule*{\inferrule*{ }{A\meet (B\join C) \types A\meet (B\join C)
%           }}{A\meet (B\join C) \types B\join C}\\
%         \inferrule*{\inferrule*{
%             \inferrule*{\inferrule*{\inferrule{ }{
%                   A\meet (B\join C)\types A\meet (B\join C)
%                 }}{A\meet (B\join C) \types A
%               }}{A\meet (B\join C),B \types A} \\
%             \inferrule*{\inferrule*{ }{
%                 B\types B
%               }}{A\meet (B\join C),B \types B}
%           }{A\meet (B\join C),B \types A\meet B
%         }}{A\meet (B\join C),B \types (A\meet B) \join (A\meet C)}\\
%         \text{(and dually)}
%         }{A\meet (B\join C),A\meet (B\join C)\types (A\meet B) \join (A\meet C)}
%     }{A\meet (B\join C)\types (A\meet B) \join (A\meet C)}
%   }{()\types (A\meet (B\join C)) \To ((A\meet B) \join (A\meet C))}
% \end{mathpar}
Thus, the natural deduction for Heyting algebras can be used as a means to derive tautologies in propositional logic.

However, there is more to the relationship between type theory and logic than this.
There are many ways to derive tautologies, including methods such as simply plugging in all possible truth assignments for the propositional variables and checking that the formula is always true.
But the natural deduction for Heyting algebras has the important property that it (at least roughly) \emph{mirrors the process of ordinary informal mathematical reasoning}.

It is easiest to see this if we reformulate the theory a little.
Let us omit the contexts ``$\Gamma\types$'' from all judgments in a derivation tree, instead writing simply the consequent $A$.
In place of the $\idfunc$ rule deriving $\Gamma\types A$, we write simply ``$A$'' without any justification, and call it a \emph{hypothesis}.
Finally, when a type $A$ is removed from the context on our way down the tree, we cross off that hypothesis everywhere that it appears above, and say that the hypothesis has been \emph{discharged}.
At the end, the set of remaining hypothesis is the antecedent of the conclusion; if no hypotheses remain undischarged, we have derived a tautology.

For instance, the above derivation of the distributive law would be written in this style as
\begin{mathpar}
  \inferrule*[Right=$\ToI$]{
    \inferrule*[Right=$\joinE$]{
      \inferrule*{\cancel{A\meet (B\join C)}}{B\join C}\\
      \inferrule*{\inferrule*{
        \inferrule*{\cancel{A\meet (B\join C)}}{A}\\
        \cancel{B}
        }{A\meet B}}{(A\meet B)\join (A\meet C)}\\
      \inferrule*{\inferrule*{
        \inferrule*{\cancel{A\meet (B\join C)}}{A}\\
        \cancel{C}
        }{A\meet C}}{(A\meet B)\join (A\meet C)}\\
    }{(A\meet B)\join (A\meet C)}
  }{(A\meet (B\join C))\To ((A\meet B)\join (A\meet C))}
\end{mathpar}
Note that there is some ambiguity; it is not obvious from looking at the derivation which rule caused which hypothesis to be discharged.
In the above example, the hypotheses $B$ and $C$ are discharged by the $\joinE$ rule, while the hypothesis ${A\meet (B\join C)}$ (everywhere it appears) is discharged by the $\ToI$ rule.
Sometimes people annotate the discharges in some way to indicate this.

However, the real point of a representation like this is that the \emph{process of writing it}, from the top down, is supposed to mirror the process of informal reasoning.
First we assume $A\meet (B\join C)$, and deduce from it $B\join C$.
Then we use $B\join C$ by additionally assuming $B$ and $C$ in two separate cases (sub-derivations), and in each of those cases we separately deduce ${(A\meet B)\join (A\meet C)}$ (by way of $A\meet B$ and $A\meet C$ respectively).
Thus, completing those cases (and ending our assumptions of $B$ and $C$) we have $(A\meet B)\join (A\meet C)$.
Finally, ending our assumption of $A\meet (B\join C)$, we have $(A\meet (B\join C))\To ((A\meet B)\join (A\meet C))$.

From this perspective, the rules in \cref{fig:natded-heyting} can also be glossed in the language of ``proof strategies''.
For instance, $\meetI$ says that ``to prove $A\meet B$, it suffices to prove $A$ and $B$ separately'', while $\ToE$ says that ``if we know $A\To B$, and we also know $A$, then we can conclude $B$'' (the rule of \textit{modus ponens}).
We encourage the reader to similarly gloss the other rules.

While it is arguable whether this \emph{exactly} mirrors the process of informal reasoning, it certainly has a close kinship with it --- much closer than the production of tautologies by checking all possible truth assignments.
In particular, it includes one essential aspect of informal reasoning: the ability to \emph{reason under a temporary assumption} and then ``discharge'' that assumption in reaching some other conclusion.
This sort of \emph{hypothetical reasoning} is central to everyday mathematics, so the fact that it also appears in natural deduction logic is a strong argument in favor of the ``naturalness'' of the latter.

This is the real origin of the name ``natural deduction''.
In fact, historically, this representation with discharged hypotheses came first, and only later was it rewritten to carry along the context, and then generalized to theories without contraction and weakening.
Other systems of formal logic, such as ``Hilbert-style calculi'' (see \cref{ex:hilbert}), though they can derive the same class of tautologies, do not really include hypothetical reasoning as such, and hence do not model informal reasoning as well.

Now, it may seem that the logical expressivity of the natural deduction for Heyting algebras is lacking because there is no operation corresponding to \emph{negation}.
However, we can do pretty well by defining $\neg A$ to mean $A\To\bot$, so that its rules are
\begin{mathpar}
  \inferrule{\Gamma,A\types\bot}{\Gamma\types \neg A}\and
  \inferrule{\Gamma\types \neg A \\ \Gamma\types A}{\Gamma\types \bot}
\end{mathpar}
In other words, to prove $\neg A$, it suffices to show that assuming $A$ leads to a contradiction, while if we have both $\neg A$ and $A$ we obtain a contradiction.
Using these rules, here is a derivation of one of ``de Morgan's laws'' as a tautology:
\begin{mathpar}
  \small
  \inferrule*{
    \inferrule*{
      \inferrule*{\inferrule*{
        \inferrule*{ }{\neg(A\join B),A \types \neg(A\join B)}\\
        \inferrule*{\inferrule*{ }{\neg(A\join B),A \types A}}{\neg(A\join B),A \types A\join B}
        }{\neg(A\join B),A \types \bot
        }}{\neg(A\join B) \types \neg A}\\
      \text{(and dually)}
      }{\neg(A\join B) \types \neg A\meet \neg B}
  }{()\types \neg(A\join B)\To(\neg A\meet \neg B)}
\end{mathpar}
% VERSION WITH PRIMITIVE STRUCTURAL RULES
% \begin{mathpar}
%   \tiny
%   \let\mymeet\meet
%   \def\meet{\mathord{\mymeet}}
%   \let\myjoin\join
%   \def\join{\mathord{\myjoin}}
%   \let\myTo\To
%   \def\To{\mathord{\myTo}}
%   \inferrule*{
%     \inferrule*{
%       \inferrule*{\inferrule*{
%           \inferrule*{ }{(A\join B)\To\bot \types (A\join B)\To\bot}\\
%           \inferrule*{\inferrule*{ }{A\types A}}{A\types A\join B
%           }}{(A\join B)\To\bot,A \types \bot
%         }}{(A\join B)\To\bot \types A\To\bot}\\
%       \inferrule*{\inferrule*{
%           \inferrule*{ }{(A\join B)\To\bot \types (A\join B)\To\bot}\\
%           \inferrule*{\inferrule*{ }{B\types B}}{B\types A\join B
%           }}{(A\join B)\To\bot,B \types \bot
%         }}{(A\join B)\To\bot \types B\To\bot}
%       }{(A\join B)\To\bot \types (A\To\bot)\meet (B\To\bot)}
%   }{()\types ((A\join B)\To\bot)\To((A\To\bot)\meet (B\To\bot))}
% \end{mathpar}
However, not \emph{every} tautology can be derived this way.
In particular, $\neg\neg A \To A$ (the ``law of double negation'') and $A\join \neg A$ (the ``law of excluded middle'') are not derivable, because although they hold in $\tv$, their analogues fail to hold in other Heyting algebras.
(In fact, they hold in a Heyting algebra exactly when that Heyting algebra is a \emph{Boolean} algebra.)
Thus, although we have something that ``looks like logic'', it is not exactly classical logic.

One way to resolve this is to simply add another rule, such as the following for ``proof by contradiction'':
\begin{mathpar}
  \inferrule*{\Gamma,\neg A\types \bot}{\Gamma\types A}
\end{mathpar}
(The rule for $\neg A$ derived from $\To$ is the form of ``proof by contradiction'' where we prove a statement is \emph{false} by assuming it is true and deriving a contradiction; here we are considering the opposite form where we prove a statement to be \emph{true} by assuming it to be false and deriving a contradiction.)
This mirrors the process of informal reasoning in classical mathematics fairly closely, though it is a bit problematic from a type-theoretic perspective (e.g.\ it fails the principles enunciated in \cref{sec:why-multicats}).
As we will see in \cref{chap:polycats}, one can also forumlate a well-behaved type theory that it \emph{can} prove all classical tautologies, by restoring the left/right and $\meet/\join$ symmetries.

However, it is also valuable to observe that conversely, if we are willing to generalize our notion of ``logic'', we obtain something much more generally applicable.
Indeed, this is really the whole point of categorical logic, as put forward in \cref{sec:intro}: we can apply ``set-like'' reasoning to objects of arbitrary categories as long as we are careful about what sort of reasoning we use.

So far, we have applied this principle mainly to equational reasoning about different kinds of terms.
However, we now have a type theory that is powerful enough to codify significant amounts of mathematical reasoning (though not yet anything involving quantifiers such as ``for all'' and ``there exists''; that will come in \cref{chap:fol}).
Thus, we can lift our notion of ``generalized logic'' back to informal mathematical reasoning.
It takes a bit of practice to learn to write informal mathematical proofs that could (at least in principle) be codified in such a generalized logic, but it is eminently possible.
(It is much \emph{more} possible because, as discussed above, our ``generalized logic'' is expressed in a style that already closely mirrors ordinary mathematical reasoning; we simply have to learn which familiar styles of argument are valid in what situations.)

The payoff is that the result is much more general than it appears, since it is true ``internally to any Heyting algebra'' (whereas ordinary mathematical reasoning is only valid in Boolean algebras).
Lest the reader think that Heyting algebras seem esoteric, we point out that the lattice of open subsets of any topological space is a Heyting algebra (\cref{ex:frames}).

The ``generalized logic'' corresponding to Heyting algebras is called \textbf{intuitionistic} or \textbf{constructive logic}, because of its similarity to the mathematics advocated by certain mathematicians calling themselves ``intuitionist'' or ``constructive'' in the early 20th century.
While we are stuck with these labels, it is probably best (for a classically trained category theorist first encountering the notion) not to read too much into them.
The point is simply that we make our mathematics more general by generalizing our logic, and this is the logic that corresponds naturally to cartesian closed lattices, which are certainly a categorically natural notion.

The observation that the logical operations of ``and'', ``or'', ``if-then'', and so on in the poset $\tv$ have the same universal properties (and hence can be represented by the same type operations) as the operations $A\times B$, $A+B$, $B^A$ in the category \bSet has a distinguished pedigree and many names: \emph{propositions as types}, \emph{proofs as terms}, or the \emph{Curry--Howard correspondence} (see~\cite{wadler:pat} for some history).
As we will see, this correspondence is also central to the use of dependent type theory as a foundation for mathematics.
Some ``constructivist'' mathematicians have argued that this correspondence should determine the \emph{meanings} of the logical operations in terms of proofs --- that is, a proof of ``$P$ and $Q$'' should be a pair $(p,q)$ where $p$ is a proof of $P$ and $q$ is a proof of $Q$; a proof of ``if $P$ then $Q$'' should be a function transforming any proof of $P$ into a proof of $Q$; and so on.
This is sometimes called the \emph{Brouwer--Heyting--Kolmogorov (BHK) interpretation}.
However, we will have little to say about the philosophical side of constructive logic.

In any case, having made these observations in the case of \emph{cartesian} closed lattices, it is natural to entertain similar ideas for other values of \fS.
Roughly speaking, the names of the corresponding ``generalized logics'' are:
\begin{center}
\begin{tabular}{c|c}
  $\fS$ & generalized logic\\\hline
  cartesian & intuitionistic logic\\
  symmetric & linear logic\\
  semicartesian & affine logic\\
  relevance & relevance logic
\end{tabular}
\end{center}
To be precise, we are currently talking about variants of all these logics that should be qualified as ``intuitionistic''; there are also ``classical'' versions of linear, affine, and relevance logics in which the laws of double negation and excluded middle hold.
Moreover, at least in the linear case one should also add a phrase like ``multiplicative-additive''\footnote{In the lingo of linear logic, $\tensor$ is a ``multiplicative'' connective, while $\meet$ and $\join$ are ``additive''.
  Classical linear logic also includes another multiplicative connective called $\parr$ that is dual to $\tensor$ in the same way that $\join$ is dual to $\meet$; see \cref{sec:cllin}.} to describe our current theory, because the name ``linear logic'' usually refers to a system with some additional modalities (see [TODO]).
Furthermore, at this point all of them should have the prefix ``propositional'', since we are not yet considering quantifiers of any sort (``there exists'' and ``for all'').

The name ``linear logic'' comes from the same intuition as our use of ``linearity'' to describe \cref{thm:multicat-linear}.
The name ``affine logic'' is similarly inspired by the fact that while a linear transformation $T(\vec v) = A\vec v$ must use its argument exactly once in each term, an affine transformation $T(\vec v) = A\vec v + \vec b$ also has terms that do not use its argument at all.
Both of these logics are primarily studied by computer scientists; the distinction between $\tensor$ and $\meet$ can be interpreted in terms of ``resource usage'' (but that is far beyond our scope here).

Finally, ``relevance logic'' was invented by some philosophers seeking to avoid certain facts about implication that they regarded as ``paradoxical'' because their ``if'' parts are not ``relevant'' to their ``then'' parts, such as $A\To (B\To A)$.
The straightforward derivation of this tautology in our type theory requires weakening:
\begin{mathpar}
  \inferrule*{\inferrule*{\inferrule*[Right=weakening]{A\types A}{A,B\types A
      }}{A\types (B\To A)
    }}{()\types A\To (B\To A)}
\end{mathpar}
and in fact the type theory for closed relevance monoidal lattices cannot derive $()\types A\hom (B\hom A)$ (although this is not obvious; see \cref{ex:relevance-eg,ex:seqcalc-logic}).

The most commonly used relevance logics satisfy other principles that our type theory does not, notably the distributive law $A\meet (B\join C) \cong (A\meet B)\join (A\meet C)$ (note that our derivation of this above also used weakening).
Of course, any closed monoidal lattice satisfies the distributive law $A\tensor (B\join C) \cong (A\tensor B)\join (A\tensor C)$, but as we have observed, both weakening and contraction are necessary to force $\tensor$ to coincide with $\meet$.
It is possible to formulate type theories that ensure the $\meet/\join$ distributive law as well, but this requires a fancier notion of generalized multicategory, so we postpone it to [TODO].


\subsection*{Exercises}

\begin{ex}\label{ex:cart-typetheory}
  Prove \cref{thm:semicart-multicat-repr,thm:cart-multicat-repr} using our posetal type theories.
  Specifically:
  \begin{enumerate}
  \item If we have exchange and weakening, prove that $\one \cong \top$.
  \item If we have exchange, weakening, and contraction, prove that $A\tensor B \cong A\times B$.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:nnlem}
  Prove that $\neg\neg(P\join\neg P)$ is an intuitionistic tautology, i.e.\ construct a derivation of $()\types\neg\neg(P\join\neg P)$ in the natural deduction for Heyting algebras.
\end{ex}

\begin{ex}\label{ex:demorgan}
  Of the four ``de Morgan's laws'', three are intuitionistic tautologies and one is not.
  Construct derivations of three of the following sequents in the natural deduction for Heyting algebras:
  \begin{align*}
    \neg(P\join Q) &\types \neg P \meet \neg Q\\
    \neg(P\meet Q) &\types \neg P \join \neg Q\\
    \neg P \meet \neg Q &\types \neg(P\join Q)\\
    \neg P \join \neg Q &\types \neg(P\meet Q)
  \end{align*}
\end{ex}

\begin{ex}\label{ex:frames}
  A \textbf{frame} is a lattice with infinitary joins satisfying the infinite distributive law $A \meet \left(\bigjoin_i B_i\right) \cong \bigjoin_i (A\meet B_i)$.
  \begin{enumerate}
  \item Prove that any (small) frame is a Heyting algebra.
  \item Prove that the lattice of open sets of any topological space is a frame.
  \item Describe a type theory for frames.  This is called (propositional) \textbf{geometric logic}.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:not-lem}
  Give concrete examples of Heyting algebras satisfying the following:
  \begin{enumerate}
  \item There is an element $P$ for which $P\join \neg P$ is not the top element.
  \item There are elements $P$ and $Q$ for which the fourth de Morgan's law (see \cref{ex:demorgan}) does not hold.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:relevance-eg}
  Describe a concrete example of a closed relevance monoidal lattice containing two objects $A$ and $B$ such that there is no morphism from $\one$ (the unit object) to $A\hom (B\hom A)$.
  Deduce that $()\types A\hom (B\hom A)$ is not derivable in the type theory for closed relevance monoidal lattices.
\end{ex}

\begin{ex}\label{ex:seqcalc-logic}
  One of the advantages of sequent calculus over natural deduction is that because all of its rules \emph{introduce} operations on the left or the right, it is easier to conclude underivability theorems.
  \begin{enumerate}
  \item Define a sequent calculus for closed \fS-monoidal lattices, and prove the cut admissibility and initiality theorems.
  \item Prove that $()\types A\hom (B\hom A)$ is not derivable in the sequent calculus for closed relevance monoidal lattices, by ruling out all possible ways that such a derivation could end.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:hilbert}
  Another way of deriving tautologies is called a \textbf{Hilbert system}.
  A Hilbert system can be formulated as a sort of type theory where the judgments all have empty context, i.e.\ are of the form $\types A$ where $A$ is a propositional formula.
  Instead of the ``modular'' left/right rules of sequent calculus or the introduction/elimination rules of natural deduction, where the rules for each connective do not refer to any other connective, a Hilbert system gives a special place to implication $\To$.
  The \emph{only} rule with premises\footnote{Hilbert systems for more complicated logics have one or two more rules with premises, but in general there are very few.} is the empty-context form of $\ToE$, \emph{modus ponens}:
  \[ \inferrule{\types A\To B \\ \types A}{\types B} \]
  The behavior of all other connectives is specified by \emph{axioms} (rules with no premises, other than the well-formedness of the formulas appearing in them).
  For instance, we complete the description of $\To$ with the following axioms:
  \begin{mathpar}
    \inferrule{\types A\type}{\types A\To A}\and
    \inferrule{\types A\type \\ \types B\type}{\types A\To (B\to A)}\and
    \inferrule{\types A\type \\ \types B\type \\ \types C\type}{\types (A\To (B\To C)) \To ((A\To B)\To (A\To C))}
  \end{mathpar}
  The axioms for the remaining connectives are (omitting the obvious premises and the $\types$):
  \begin{mathpar}
    A\To (B\To (A\meet B))\and
    (A\meet B)\To A\and
    (A\meet B)\To B\\
    A\To (A\join B)\and
    B\To (A\join B)\and
    (A\To C)\To ((B\To C) \To ((A\join B)\To C))\\
    A\To \top\and
    \bot \To A
  \end{mathpar}
  Prove that this Hilbert system derives exactly the same tautologies as the natural deduction for Heyting algebras.

  (The main reasons for using a Hilbert system seem to be that it \emph{never} changes the context and has very few rules.
  This sometimes makes metatheoretic arguments easier, but at the cost of greater distance from informal mathematics, since as we have remarked the latter gives a central place to hypothetical reasoning.
  It should also be noted that the symbol $\types$ is often used differently in the context of Hilbert systems; rather than $\types$ being part of each judgment, the notation ``$\Gamma\types A$'' means that we can derive $A$ (that is, $\types A$ in our notation) in the Hilbert system augmented by all the formulas in $\Gamma$ as additional axioms.)
\end{ex}

\begin{ex}\label{ex:cocartesian-typetheory}
  Is there a well-behaved type theory (i.e.\ having admissible cut and an initiality theorem) corresponding to the (posetal version of the) ``cocartesian multicategories'' of \cref{ex:cocartesian-clubs}?
  \textit{(As of this writing, the answer is not known to the author.)}
\end{ex}



\section{Simply typed $\lambda$-calculus}
\label{sec:stlc}
\label{sec:cartmoncat}

We now move back up the ladder from posets to categories.
In this case it becomes more important to adhere to principle~\eqref{princ:adm} and make our structural rules admissible.
Otherwise our derivations would become polluted with applications of these rules, and our terms (which, as ever, we want to be simply syntax for derivations) would be likewise quite messy-looking.
We have already seen in \cref{thm:heyting-strucadm} that the structural rules can be made admissible in the cartesian case where we want all of them, so we consider that case first.
In addition to being the easiest, this is probably also the most commonly used case.

We begin by introducing terms for the rules from \cref{sec:heyting-algebras}, as shown in \cref{fig:stlc}.
As in \cref{sec:heyting-algebras}, we omit $\tensor$ and $\unit$ since they coincide with $\meet$ and $\top$.
We also switch back to categorical notations $\times,\unit,+,\zero$ instead of $\meet,\top,\join,\bot$.
We also write $A\to B$ instead of $A\hom B$; this has the pleasing consequence that the term syntax $M:A\to B$ looks the same as the common mathematical notation for functions.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\types A\type \\ (x:A)\in \Gamma}{\Gamma\types x:A}\;\idfunc
    \and
    \inferrule{f\in \cG(A_1,\dots,A_n;B) \\ \Gamma\types M_1:A_1 \\ \dots \\ \Gamma \types M_n:A_n}{\Gamma\types f(M_1,\dots,M_n):B}\;f
    \and
    \inferrule{\Gamma\types M:A \\ \Gamma\types N:B}{\Gamma\types \pair{M}{N} :A\times B}\;\timesI
    \and
    \inferrule{\Gamma\types M:A\times B}{\Gamma\types \pr1AB(M):A}\;\timesE1
    \and
    \inferrule{\Gamma\types M:A\times B}{\Gamma\types \pr2AB(M):B}\;\timesE2
    \\
    \inferrule{ }{\Gamma\types \ttt:\unit}\;\unit I
    \and
    \inferrule{\Gamma\types M:\zero}{\Gamma\types \abort(M):C}\;\zero E
    \\
    \inferrule{\Gamma\types M:A}{\Gamma\types \inl(M):A+B}\;\plusI1
    \and
    \inferrule{\Gamma\types N:B}{\Gamma\types \inr(N):A+B}\;\plusI2
    \and
    \inferrule{
      \Gamma\types M:A+B \\ \Gamma,u:A \types P:C \\ \Gamma,v:B\types Q:C
    }{\Gamma \types\acase AB(M,u.P,v.Q):C}\;\plusE
    \and
    \inferrule{\Gamma,x:A\types M:B}{\Gamma\types \lambda x.M:A\to B}\;\toI\and
    \inferrule{\Gamma\types M:A\to B \\ \Gamma\types N:A}{\Gamma\types M N:B}\;\toE
  \end{mathpar}
  \caption{The simply typed $\lambda$-calculus with products and coproducts}
  \label{fig:stlc}
\end{figure}

Most of the term annotations should be familiar from \cref{sec:multicat-prod-coprod}; indeed they are even simpler, since the (expected) presence of the structural rules allows us to omit some of the more verbose annotations.
The rule $\toI$ introduces a new kind of term, a \textbf{$\lambda$-abstraction}.
Since the variable $x$ appears in the premise but not the conclusion, it must be bound in the resulting term; there is nothing else to say, so we simply prefix the letter $\lambda$ to indicate the rule.
Intuitively, we think of $\lambda x.M$ as meaning ``the function that takes one argument, called $x$, and returns the value of $M$ (which includes $x$)''.
For instance, $\lambda x.x^2$ denotes the function that squares its argument, $\lambda x.(x+3)$ denotes the function that adds three to its argument, and so on.
Because of the importance of this operation, the type theory of \cref{fig:stlc}, which we expect to correspond to cartesian closed categories with coproducts, is called the \textbf{simply typed $\lambda$-calculus (STLC) with products and coproducts}.
(The unqualified ``STLC'' would omit the rules for $\times,\unit,+,\zero$.)

The term annotation for the rule $\toE$ simply ``pairs up'' two terms, one of which has type $A\to B$ and one of which has type $A$.
Intuitively, we are ``applying'' a function $M:A\to B$ to an argument $N:A$.
Technically there ought also to be a label indicating the rule being applied to pair these terms up, such as $\fapp(M,N)$.
However, any system of notation has room for \emph{one} operation denoted by simple juxtaposition (e.g.\ in high-school algebra it is multiplication, while in group theory it is the group operation), and the importance of the type operation $\to$ leads us to choose $\toE$ for this honor in type theory.
Most mathematicians write $f(a)$ for the application of a function $f$ to an argument $a$; since parentheses are used as usual for grouping, this notation is also valid here, just as $(x)(y)=xy$ in high-school algebra.

\begin{lem}\label{thm:stlc-uniqderiv}
  If a term $\Gamma\types M:A$ is derivable in the simply typed $\lambda$-calculus, then it has a unique derivation.
\end{lem}
\begin{proof}
  This is almost immediate.
  Since the premises of all rules have the same context as the conclusion, perhaps with a type added, there is no ambiguity about how to split things up, and hence no need for the uglier annotations used in \cref{sec:multicat-prod-coprod}.
\end{proof}

\begin{lem}\label{thm:stlc-strucadm}
  The structural rules of exchange, contraction, and weakening are admissible in the simply typed $\lambda$-calculus.
  Moreover, they make the derivations into a cartesian multigraph (i.e.\ they are functorial as in \cref{defn:fS-multicategory}).
\end{lem}
\begin{proof}
  The proof is essentially the same as \cref{thm:heyting-strucadm}, carrying along terms and variables; we prove the following rule, for any function $\sigma : \{1,\dots,m\} \to \{1,\dots,n\}$:
  \begin{mathpar}
    \inferrule*{y_1:A_{\sigma 1},\dots,y_m:A_{\sigma m} \types M:B}{x_1:A_1,\dots,x_n:A_n \types \sigma^*M:B}
  \end{mathpar}
  by pushing up through all rules until we get to $\idfunc$.
  Regarded as an operation on terms, $\sigma^*$ is defined by the clause
  \begin{align*}
    \sigma^*y_j &= x_{\sigma j}\\
    \intertext{along with trivial ``descending into subterms'' clauses for all other terms, such as}
    \sigma^*\pair M N &= \pair{\sigma^*M}{\sigma^*N}\\
    \sigma^*\case(M,u.P,v.Q) &= \case(\sigma^*M, u.(\sigma\sqcup\idfunc)^*P,v.(\sigma\sqcup\idfunc)^*Q)
  \end{align*}
  and so on.
  Intuitively, we simply substitute the variable $x_{\sigma j}$ for $y_j$ wherever it appears, for all $1\le j\le m$.
  A similar induction proves that this operation is functorial, yielding a cartesian multigraph.
\end{proof}

In fact, if $\sigma$ is injective --- that is, it is composed of exchange and weakening only --- and if we choose variables $y_j = x_{\sigma j}$ (which is only possible if $\sigma$ is injective), then in fact $\sigma^*M = M$.
For instance, we have
\[ x:A, y:B \types \pair x y : A\times B\]
and by exchange and weakening we can obtain also
\[ y:B, z:C, x:A \types \pair x y : A\times B.\]
with the same term $\pair x y$.
This does not contradict ``terms are derivations'', because we only require a term to determine a unique derivation \emph{when paired with its context and consequent}.

\begin{rmk}
  As remarked briefly at the beginning of the section, the admissibility of the structural rules is central to having a clean theory of terms and a clean proof of the initiality theorem.
  If we took the structural rules as primitive, then to maintain ``terms as derivations'' we would have to include information about the structural rules in terms, for instance annotating the derivation
  \[ \inferrule*{\inferrule*{A,B\types A \\ A,B\types B}{A,B\types A\times B}}{B,A\types A\times B}\]
  with a term like $y:B,x:A\types \sigma^*\pair{x}{y}:A\times B$, distinguishing it from the \emph{different} derivation
  \[ \inferrule*{B,A\types B \\ B,A\types A}{B,A\types A\times B}\]
  that we could write as $y:B,x:A\types \pair{x}{y}:A\times B$.
  Clearly these two derivations ought to have the same term.
  However, if structural rules are primitive, then using the same term for both of them would break the ``terms as derivations'' principle.
  If we did this, then to prove the initiality theorem using terms, after inducting over derivations we would have to prove that the interpretation of a term is independent of its derivation.
  This sort of thing is difficult and tedious, and hence often left to the reader or left unmentioned altogether.
  Making the structural rules admissible avoids both horns of the dilemma.
\end{rmk}

Now we need substitution.
Since all our other rules maintain the same context, it is natural to do the same here.

\begin{lem}\label{thm:stlc-subadm}
  Substitution is admissible in the simply typed $\lambda$-calculus: given derivations of $\Gamma\types M:A$ and $\Gamma,x:A\types N:B$, we can construct a derivation of $\Gamma\types M[N/x]:B$.
\end{lem}
\begin{proof}
  By induction on the derivation of $\Gamma,x:A\types N:B$, as usual.
  There are two mildly new features.
  Firstly, since the contexts are maintained rather than split, we have to recurse into \emph{all} premises of each rule.
  Secondly, when we get down to $\idfunc$ we might find a variable other than $x$, in which case there is no substitution to do.
  Thus the clauses defining substitution are as shown in \cref{fig:stlc-sub}.
  As in \cref{sec:multicat-moncat,sec:multicat-prod-coprod}, to write substitution using terms, we need to ensure by $\alpha$-equivalence that the bound variables $u,v$ in $\case$ and $y$ in $\lambda$ do not appear free in $M$.
\end{proof}

\begin{figure}
  \centering
  \begin{align*}
    x[M/x] &= M \\
    y[M/x] &= y \qquad (y\text{ a variable } \neq x)\\
    f(N_1,\dots,N_n)[M/x] &= f(N_1[M/x],\dots,N_n[M/x])\\
    \pair{P}{Q}[M/x] &= \pair{P[M/x]}{Q[M/x]}\\
    \pi_1(N)[M/x] &= \pi_1(N[M/x])\\
    \pi_2(N)[M/x] &= \pi_2(N[M/x])\\
    \ttt[M/x] &= \ttt\\
    \abort(N)[M/x] &= \abort(N[M/x])\\
    \inl(N)[M/x] &= \inl(N[M/x])\\
    \inr(N)[M/x] &= \inr(N[M/x])\\
    \case(N,u.P,v.Q)[M/x] &= \case(N[M/x],u.P[M/x],v.Q[M/x])\\
    (\lambda y.N)[M/x] &= \lambda y.N[M/x]\\
    (PQ)[M/x] &= (P[M/x])(Q[M/x])
  \end{align*}
  \caption{Substitution in simply typed $\lambda$-calculus}
  \label{fig:stlc-sub}
\end{figure}

Note that the contraction rule is actually a special case of substitution, namely the substitution of one variable for another.
That is, given $\Gamma,x:A,y:A \types M:B$, we have $\Gamma,x:A \types M[x/y]:B$ which is (by induction, if you wish) equal to the contraction of $M$ obtained from \cref{thm:stlc-strucadm}.

The natural sort of ``associativity'' for this kind of substitution is also different: it combines the ``associativity and interchange'' properties in one, since if a variable $y$ is free in $N[M/x]$ then it might appear in \emph{both} $M$ and $N$.

\begin{lem}\label{thm:stlc-subassoc}
  Given derivations of $\Gamma\types M:A$ and $\Gamma,x:A\types N:B$ and $\Gamma,x:A,y:B\types P:C$, we have
  \[ P[N/y][M/x] = P[M/x][N[M/x]/y]. \]
\end{lem}
On the left-hand side, $P[N/y][M/x]$ means $(P[N/y])[M/x]$.
On the right-hand side, when writing $P[M/x]$ we have technically to apply weakening to $M$ (by \cref{thm:stlc-strucadm}) so that it has context $\Gamma,y:B$ first.
\begin{proof}
  A straightforward induction on derivations.
  For the ``base cases'' of variables, we have
  \begin{align*}
    x[N/y][M/x] &= x[M/x]\\
                &= M \\
                &= M[N[M/x]/y]\tag{*}\\
                &= x[M/x][N[M/x]/y]\displaybreak[0]\\
    y[N/y][M/x] &= N[M/x]\\
                &= y[N[M/x]/y]\\
                &= y[M/x][N[M/x]/y]\displaybreak[0]\\
    z[N/y][M/x] &= z[M/x]\\
                &= z\\
                &= z[M/x]\\
                &= z[M/x][N[M/x]/y]
  \end{align*}
  where $z\neq x,y$.
  The equality (*) is because $y$ does not appear in $M$, i.e.\ $M$ has been obtained by weakening from a context not including $y$ as remarked above.
  (Formally, we ought to prove by a further induction that substituting for a variable obtained by weakening never changes the term/derivation.)
\end{proof}

We also need to know that substitution commutes with the other structural rules.
For weakening and exchange this is immediate from the observation that these rules do not change the term.
For contraction, it follows from \cref{thm:stlc-subassoc} and the observation that contraction is a special case of substitution:
\begin{gather}
  N[x/y][M/x] = N[M/x][x[M/x]/y] = N[M/x][M/y].\label{eq:sub-contr-1}\\
  N[M/x][y/z] = N[y/z][M[y/z]/x]\label{eq:sub-contr-2}
\end{gather}

Now we can state the $\beta$- and $\eta$-conversion rules.
Those for products and coproducts are the familiar ones from \cref{fig:moncat-prod-coprod-equiv}.
The $\beta$-conversion rule for $\to$:
\[ (\lambda x.M)N \equiv M[N/x]\]
says that if we apply a function defined by $\lambda$-abstraction to an argument, the result is what we get by ``plugging in'' the argument to the expression defining the function.
That is, if $f(x)=x^2$ then $f(3) = 3^2$.
The $\eta$-conversion rule says that any function is a $\lambda$-abstraction:
\[  M \equiv \lambda x.Mx \qquad \text{if } M:A\to B\]
A straightforward induction shows that $\equiv$ is a congruence not only for substitution, but also for the new admissible structural rules from \cref{thm:stlc-strucadm}.

Now we are ready to prove the initiality theorem.
Note that we generate our free structure from a mere multigraph, not (as one might guess) a cartesian multigraph.
A cartesian multigraph contains operations and equations, so to use it as base data we would need to incorporate those operations into $\equiv$.

\begin{thm}\label{thm:stlc-initial}
  The free cartesian closed category with coproducts generated by a multigraph \cG can be presented by the simply typed $\lambda$-calculus under \cG: its underlying cartesian multigraph is that constructed in \cref{thm:stlc-strucadm} modulo $\equiv$, and its composition is given by substitution.
\end{thm}
\begin{proof}
  Although \cref{thm:stlc-subadm,thm:stlc-subassoc} are not stated in the usual form of multicategory composition operations, we can easily derive those operations from them.
  Given $\Gamma\types M:A$ and $\Delta,x:A,\Psi\types N:B$, we can apply weakening and exchange to obtain $\Delta,\Gamma,\Psi\types M:A$ and $\Delta,\Gamma,\Psi,x:A\types N:B$; then \cref{thm:stlc-subadm} yields $\Delta,\Gamma,\Psi\types N[M/x]:B$.
  Associativity and interchange are the special cases of \cref{thm:stlc-subassoc} where $x$ does not occur in $P$ and where $x$ does not occur in $N$, respectively, and the identity laws follow as usual.
  Thus to have a cartesian multicategory it remains to check \cref{defn:fS-multicategory}\ref{item:cartmulti-3} and \ref{item:cartmulti-4}, using in particular~\eqref{eq:sub-contr-1} and~\eqref{eq:sub-contr-2}.
  We leave this to the reader in \cref{ex:stlc-cartmulti}; it can be done directly or by way of \cref{ex:cartmulti-oneplace}.

  The rules for all the type operations give this cartesian multicategory products, coproducts, and closed structure; thus it underlies a cartesian closed category with coproducts.
  Initiality follows as usual: given a map of multigraphs $P:\cG\to \cM$, where \cM is a cartesian closed category with coproducts, we extend $P$ to all types by induction, then define it on all derivations by induction, then check that $\equiv$ is preserved by induction.
  As always, this works because the rules for type operations (including the new one $\to$) are defined to mirror those of categorical universal properties.
\end{proof}

To end this section, recall from \cref{sec:unary-theories} that for almost any type theory with an initiality theorem we can build a notion of a ``theory'' that allows generating morphisms with arbitrary types in their domains and codomains, as well as generating equalities between arbitrary terms.
We have not explicitly mentioned this in the present chapter, but it is true for all of the type theories we have considered.

In the present case, a $(\to,\times)$-theory (say) has a set $\cG_0$ of objects, sets $\cG_1(A_1,\dots,A_n;B)$ of generating multimorphisms where $A_i$ and $B$ can be built up out of $\times,\unit,\to$, and a set of generating equations between terms defined from these.
For instance, we could have a generating morphism with domain $(A\to B, B\times B\to A)$ and codomain $(A\to C)\times B$.
As in \cref{sec:unary-theories} we can show that the type theory of a $(\to,\times)$-theory freely generates an (in this case) cartesian closed category, and that every cartesian closed category is equivalent to that freely generated by its underlying $(\to,\times)$-theory.
Thus, if we define the morphisms between $(\to,\times)$-theories in a sufficiently tautological way, we obtain a 2-category biequivalent to that of cartesian closed categories.

We can of course ``mix and match'' the type operations assumed in our theories, by the modularity of type theory (principle~\eqref{princ:independence}).
One particularly important case is that of \textbf{$\times$-theories}, where we omit $\to$ as well as $+,\zero$.
These $\times$-theories solve the problem that we had with our unary $\times$-theories in \cref{sec:unary-theories} of having to ``pack and unpack'' terms into ordered pairs in order to apply generators (such as the multiplication of a monoid object) to them.
For instance, the $\times$-theory for a monoid has:
\begin{itemize}
\item One base type $A$;
\item Two generating morphisms $m\in\cG_1(A,A;A)$ and $e\in \cG_1(();A)$; and
\item The following axioms:
  \begin{mathpar}
    x:A,y:A,z:A \types m(x,m(y,z)) \equiv m(m(x,y),z) : A\and
    x:A \types m(x,e) \equiv x :A\and
    y:A \types m(e,y) \equiv y :A
  \end{mathpar}
\end{itemize}
(As is common, since $e$ is a 0-ary generator, we write just ``$e$'' instead of explicitly giving it 0 arguments like ``$e()$''.)
In this formulation, the equational proof of uniqueness of inverses from \cref{sec:intro} finally makes sense.
For this we assume additional generators $i,j\in \cG_1(A,A)$ and axioms
\begin{mathpar}
  x:A \types m(x,i(x))\equiv e :A \and
  x:A \types m(x,j(x))\equiv e :A \and
  x:A \types m(i(x),x)\equiv e :A \and
  x:A \types m(j(x),x)\equiv e :A.
\end{mathpar}
If we write $m(x,y)$ infix as $x\cdot y$, then we can perform the computation exactly as written:
\[ x:A \types i(x) \equiv i(x) \cdot e \equiv i(x) \cdot (x \cdot j(x)) \equiv (i(x)\cdot x)\cdot j(x) \equiv e\cdot j(x) \equiv j(x) : A.\]

Note that although the general approach of \cref{sec:unary-theories} would allow these $\times$-theories to have generating morphisms whose sources and targets involve products of base types, we did not need any of these for our example.
In fact, we can \emph{always} get away without such, essentially because products in a cartesian multicategory have an ``ambidextrous'' universal property as mentioned in \cref{sec:cartmulti}; see \cref{ex:catprod-thy-noprod}.
With this simplification, a $\times$-theory is also known as an \textbf{finitary algebraic theory}: it consists of a set of (base) types, a set of operations with specified finite arities and types, and a collection of axioms about the composites of those operations.
The category with products $\F\bPrCat\cG$ is then the \emph{category with products freely generated by a model of \cG}.

There is another thread in categorical logic that \emph{defines} ``theories'' to be the categories they freely generate.
A finitary algebraic theory, for instance, is defined to be a category with products whose objects are freely generated by some subset thereof under products, and a model of that theory is defined to be a product-preserving functor out of it.
In the special case when there is only one generating object, such a category with products is called a \textbf{Lawvere theory} after~\cite{lawvere:functsem}.
These freely generated categories are very useful; but of course in practice algebraic theories are generally specified by generators and relations, so it is also important to understand the process by which these generate a category.
Type theory, with its technology of cut-admissibility, gives us a concrete way to construct and understand such categories, rather than (for example) simply deducing their existence by an adjoint functor theorem.


\subsection*{Exercises}

\begin{ex}\label{ex:stlc-cartmulti}
  Complete the proof in \cref{thm:stlc-initial} that type theory yields a cartesian multicategory by checking \cref{defn:fS-multicategory}\ref{item:cartmulti-3} and \ref{item:cartmulti-4}, or perhaps the corresponding one-place axioms you found in \cref{ex:cartmulti-oneplace}.
\end{ex}

\begin{ex}\label{ex:catprod-ehnr-again}
  Re-do \cref{ex:catprod-eckmann-hilton,ex:catprod-nearring} using the present multicategorical $\times$-theories.
  Notice how much nicer they are.
\end{ex}

\begin{ex}\label{ex:catprod-thy-noprod}
  Show that for any $\times$-theory \cG there is another $\times$-theory \cH whose generating arrows contain only base types in their domains and codomains and such that $\F\bPrCat\cG\simeq \F\bPrCat\cH$.
\end{ex}


\section{Programming with $\lambda$-calculus}
\label{sec:primrec}

[NNOs, primitive recursion, functional programming, perhaps more general GADTs]


\section{$\lambda$-calculus and de Bruijn variables}
\label{sec:de-bruijn}

[Arise automatically from judgmental contexts]


\section{Symmetric monoidal categories}
\label{sec:symmoncat}

Now we consider a type theory for symmetric monoidal categories.
That is, we add the exchange rule (and the $\hom$ rules) to \cref{sec:multicat-prod-coprod}, or remove weakening and contraction from \cref{sec:stlc}.
As always we want exchange to be admissible, but we cannot use the same trick for this that we did in \cref{sec:stlc}, because in the absence of weakening the identity rule must be $x:A\types x:A$ rather than containing a whole context on the left.

Thus, we cannot expect to push all structural rules up to the top.
Instead we need to build them into the other rules.
For instance, in the theory of \cref{sec:logic} we can derive $B,A\types A\tensor B$ by using primitive exchange:
\[ \inferrule*{\inferrule*{A\types A \\ B\types B}{A,B\types A\tensor B}}{B,A\types A\tensor B} \]
To obtain this without primitive exchange, we must incorporate some exchange into the $\tensorI$ rule.
That is, it must say something like
\[ \inferrule{\Gamma\types A \\ \Delta\types B \\ \Gamma,\Delta \cong \Phi}{\Phi\types A\tensor B} \]
allowing us to permute the concatenated context $\Gamma,\Delta$ of the premises to obtain the context of the conclusion.

However, this is not quite right yet: it introduces too much redundancy.
For instance, with this rule we would have the following derivation of $A,B,C\types A\tensor (B\tensor C)$:
\begin{mathpar}
  \inferrule*{A\types B \\ \inferrule*{B\types B \\ C\types C \\ B,C\cong C,B}{C,B\types B\tensor C} \\ A,C,B\cong A,B,C}{A,B,C\types A\tensor (B\tensor C)}
\end{mathpar}
which would be distinct from the obvious one:
\begin{mathpar}
  \inferrule*{A\types B \\ \inferrule*{B\types B \\ C\types C \\ B,C\cong B,C}{B,C\types B\tensor C} \\ A,B,C\cong A,B,C}{A,B,C\types A\tensor (B\tensor C)}
\end{mathpar}
This is not what we want; both clearly represent the same morphism in a symmetric multicategory, and moreover both are naturally represented by the same term $x:A,y:B,z:C \types \tpair{x}{\tpair{y}{z}}:A\tensor (B\tensor C)$.
What we need to do is incorporate ``just enough'' exchange to obtain any desired ordering of the context of the conclusion, but without introducing redundancy.

The redundancy comes from the fact that the contexts of the premises must already be free to occur in any order.
Thus, we don't want to re-build-in permutations of those, only permutations that alter the relative order between the contexts of different premises.
Formally, what we need is a \emph{shuffle}.

\begin{defn}
  For $p_1,\dots,p_n \in \dN$, a \textbf{$(p_1,\dots,p_n)$-shuffle} is a permutation of $\bigsqcup_{i=1}^n\{1,\dots,p_i\}$ with the property that it leaves invariant the internal ordering of each summand.
\end{defn}

For instance, if we write $\{1,2\}\sqcup \{1,2,3\}$ as $\{1,2,1',2',3'\}$, then here are some $(2,3)$-shuffles:
\begin{mathpar}
  121'2'3'\and
  11'2'23'\and
  1'2'13'2\and
  1'12'3'2
\end{mathpar}
In all cases $1$ comes before $2$, and also $1'$ comes before $2'$ which comes before $3'$.
The name ``shuffle'', of course, comes from the fact that when $n=2$ this is exactly the sort of permutation that can be obtained by cutting a deck of $p+q$ cards into a $p$-stack and a $q$-stack and riffle-shuffling them together.

Now let $S_p$ denote the symmetric group on $p$ elements; thus the $(p_1,\dots,p_n)$-shuffles are elements of $S_{p_1+\cdots+p_n}$.
Note that they are not a subgroup.
However, we do have a (non-normal) inclusion $S_{p_1}\times \cdots\times S_{p_n} \into S_{p_1+\cdots+p_n}$ given by the \textbf{block sum of permutations}, acting on $\bigsqcup_{i=1}^n\{1,\dots,p_i\}$ by permuting each summand individually.
The following is straightforward to verify.

\begin{lem}
  Every coset of $S_{p_1}\times \cdots \times S_{p_n}$ in $S_{p_1+\cdots+p_n}$ contains exactly one $(p_1,\dots,p_n)$-shuffle.
  Thus, every permutation of $p_1+\cdots+p_n$ can be written uniquely as the product of a block sum from $S_{p_1}\times \cdots \times S_{p_n}$ and a $(p_1,\dots,p_n)$-shuffle.\qed
\end{lem}

If $\Gamma_i$ is a context of length $p_i$ for $i=1,\dots,n$, then we write $\nShuf(\Gamma_1,\dots,\Gamma_n;\Psi)$ for the set of $(p_1,\dots,p_n)$-shuffles that act on the concatenated context $\Gamma_1,\dots,\Gamma_n$ to produce the context $\Psi$.
When using named variables, we assume that the variable names are preserved by this action (which means that the contexts $\Gamma_1,\dots,\Gamma_n$ and $\Psi$ uniquely determine such a shuffle if it exists).
Now we can state a better version of $\tensorI$:
\[ \inferrule{\Gamma\types A \\ \Delta\types B \\ \sigma\in\nShuf(\Gamma,\Delta;\Phi)}{\Phi\types A\tensor B} \]
This allows deriving $B,A\types A\tensor B$, but rules out the undesired redundant derivation above, since the permutation $A,C,B\cong A,B,C$ is not a $(1,2)$-shuffle.
We can treat all the other rules from \cref{sec:multicat-prod-coprod} (and also the $\hom$ rules) similarly, moving the active types in the context to the far right as we did in \cref{sec:stlc}; the result is shown in \cref{fig:smc}.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\types A\type}{x:A\types x:A}\;\idfunc
    \and
    \inferrule{f\in \cG(A_1,\dots,A_n;B) \\ \Gamma_1\types M_1:A_1 \\ \dots \\ \Gamma_n \types M_n:A_n \\ \nShuf(\Gamma_1,\dots\Gamma_n;\Phi)}{\Phi\types f(M_1,\dots,M_n):B}\;fI
    \and
    \inferrule{\Gamma\types M:A \\ \Delta\types N:B \\ \nShuf(\Gamma,\Delta;\Phi)}{\Phi\types \tpair{M}{N}:A\tensor B}\;\tensorI
    \and
    \inferrule{
      \Psi \types M:A\tensor B \\
      \Gamma,x:A,y:B\types N:C\\
      \nShuf(\Gamma,\Psi;\Phi)
    }{
      \Phi \types \match_{A\tensor B}(M,xy.N):C
    }\;\tensorE
    \\
    \inferrule{ }{()\types \ott:\one}\;\one I
    \and
    \inferrule{
      \Psi\types M:\one \\
      \Gamma\types N:C \\
      \nShuf(\Gamma,\Psi;\Phi)
    }{
      \Phi\types \match_\one(M,N):C
    }\;\one E
    \\
    \inferrule{\Gamma\types M:A \\ \Gamma\types N:B}{\Gamma\types \pair{M}{N} :A\times B}\;\timesI
    \and
    \inferrule{\Gamma\types M:A\times B}{\Gamma\types \pr1AB(M):A}\;\timesE1
    \and
    \inferrule{\Gamma\types M:A\times B}{\Gamma\types \pr2AB(M):B}\;\timesE2
    \\
    \inferrule{ }{x_1:A_1,\dots,x_n:A_n\types \ttt(x_1,\dots,x_n):\unit}\;\unit I
    \and
    \inferrule{\Psi\types M:\zero \\ \nShuf(\Gamma,\Psi;\Phi)}{\Phi\types \abort^\Gamma(M):C}\;\zero E
    \\
    \inferrule{\Gamma\types M:A}{\Gamma\types \inl(M):A+B}\;\plusI1
    \and
    \inferrule{\Gamma\types N:B}{\Gamma\types \inr(N):A+B}\;\plusI2
    \and
    \inferrule{
      \Psi\types M:A+B \\ \Gamma,u:A \types P:C \\ \Gamma,v:B\types Q:C \\ \nShuf(\Gamma,\Psi;\Phi)
    }{\Phi \types\acase AB(M,u.P,v.Q):C}\;\plusE\and
    \inferrule{\Gamma,x:A\types M:B}{\Gamma\types \lambda x.M:A\hom B}\;\homI\and
    \inferrule{\Gamma\types M:A\to B \\ \Delta\types N:A \\ \nShuf(\Gamma,\Delta;\Phi)}{\Phi\types M N:B}\;\homE
  \end{mathpar}
  \caption{Type theory for symmetric monoidal categories}
  \label{fig:smc}
\end{figure}

\begin{lem}\label{thm:smc-exchadm}
  Exchange is admissible in this type theory: if we have a derivation of $\Gamma\types A$, and a permutation $\Gamma\cong \Delta$ rearranging $\Gamma$ to $\Delta$, then we can construct a derivation of $\Delta\types A$.
  Moreover, this operation is a group action.
\end{lem}
\begin{proof}
  We induct on the given derivation of $\Gamma\types A$, and most of the cases are essentially the same.
  Consider $\tensorI$: given its premises and a permutation $\Phi\cong\Phi'$, we decompose the composite permutation $\Gamma,\Delta\cong \Phi\cong\Phi'$ uniquely as the block sum of two permutations $\Gamma\cong \Gamma'$ and $\Delta\cong\Delta'$ with a shuffle $\Gamma',\Delta' \cong \Phi'$.
  Now by the inductive hypothesis we can derive $\Gamma'\types A$ and $\Delta'\types B$, whence applying $\tensorI$ again with the shuffle $(\Gamma',\Delta') \cong \Phi'$ we get $\Phi' \types A\tensor B$.
  All the other cases can be treated similarly.

  We likewise show that it is a group action by induction.
  In the case of $\tensorI$, if we have $\Phi\cong \Phi'\cong\Phi''$, we decompose $(\Gamma,\Delta)\cong \Phi\cong \Phi'$ as $(\Gamma,\Delta)\cong (\Gamma',\Delta')\cong\Phi'$; then we decompose $(\Gamma',\Delta')\cong\Phi' \cong \Phi''$ as $(\Gamma',\Delta')\cong (\Gamma'',\Delta'')\cong\Phi''$.
  But since composition of permutations is associative, this is the same as decomposing $(\Gamma,\Delta)\cong \Phi\cong \Phi''$ as $(\Gamma,\Delta)\cong (\Gamma'',\Delta'')\cong \Phi''$ directly.
  We conclude by applying the inductive hypothesis to the actions of $\Gamma\cong \Gamma'\cong \Gamma''$ and $\Delta\cong \Delta'\cong\Delta''$ on the premises.
\end{proof}

Note that the shuffles appearing in the premises are \emph{not} notated explicitly in the terms!
Nevertheless, terms still uniquely determine derivations, because we can inspect the order that the variables appear in a term.
As in \cref{sec:multicat-prod-coprod} we need ``superlinearity'' first.

\begin{lem}\label{thm:smc-superlin}
  If $\Gamma\types M:A$ is derivable, then every variable in $\Gamma$ appears at least once (free) in $M$.
\end{lem}
\begin{proof}
  An easy induction just like \cref{thm:moncat-prod-coprod-superlin}.
  Note that we again had to include the unused variables in $\unit I$ and $\zero E$ for this purpose.
\end{proof}

\begin{lem}\label{thm:smc-tad}
  If $\Gamma\types M:A$ is derivable, then it has a unique derivation.
\end{lem}
\begin{proof}
  By induction on derivations.
  Clearly the structure of a term determines the \emph{rule} that must have been applied to produce it, so the question is whether it determines the premises uniquely as well.
  The interesting cases are the rules that involve shuffles: $fI,\tensorI,\tensorE,\one E,\zero E,\plusE$.

  Consider $\tensorI$: looking at the conclusion $\Phi \types \tpair M N : A \tensor B$, the rule ensures that each variable in $\Phi$ can only occur in one of $M$ or $N$, and by \cref{thm:smc-superlin} it must appear in exactly one of them.
  Thus, it must be that $\Gamma$ consists of those variables occurring in $M$ while $\Delta$ consists of those variables occurring in $N$.
  Moreover, since the shuffle $\nShuf(\Gamma,\Delta;\Psi)$ cannot alter the relative order of variables in $\Gamma$ and $\Delta$, it must be that the variables in $\Gamma$ and $\Delta$ occur in the same order as they do in $\Psi$.
  Thus the premises $\Gamma\types M:A$ and $\Delta\types N:B$ are uniquly determined, and once $\Gamma$ and $\Delta$ are fixed the shuffle is also uniquely determined.
  All the other rules involving shuffles behave similarly.
\end{proof}

From this point onwards the theory looks almost exactly like that of \cref{sec:multicat-prod-coprod}: substitution is admissible, we define $\beta$- and $\eta$-conversion rules, and construct a free closed symmetric monoidal category with products and coproducts (or less, by omitting some of our type operations).
We leave the details to the reader (\cref{ex:smc}).

In particular, because we have ensured that terms uniquely represent derivations, we can prove the initiality theorem as usual by a simple induction over derivations.
To the author's knowledge the use of shuffles for this purpose is an improvement over the existing literature: it produces a free symmetric multicategory using nice-looking terms while still maintaining the ``terms are derivations'' principle, so that we can prove the initiality theorem without incurring the (rarely-satisfied) obligation to prove that definitions by induction over derivations depend only on the term.

One sometimes also finds remarks that the context in a linear type theory can be treated as a ``finite multiset'' (a ``set that can contain some elements more than once'').
Whether this is true depends on what exactly one means by a multiset.
On one hand, if a multiset just means a set with a (finite) positive ``count'' labeling each element, then this is true for posetal linear logic as in \cref{sec:logic}, but not when we want to present non-posetal symmetric monoidal categories, since it doesn't allow us to distinguish between $x:A,y:A \types \tpair x y:A\tensor A$ and $x:A,y:A \types \tpair y x:A\tensor A$.
On the other hand, if a multiset means a set with a (finite) nonempty \emph{set} of ``occurrences'' labeling each element, then the occurrences play essentially the same role as named variables.
This suggests a type theory corresponding somehow to the ``fat symmetric multicategories'' of~\cite[Appendix A]{leinster:higher-opds}; but we will not pursue this further.

Finally, we can enhance the present theory as in \cref{sec:unary-theories} to allow generating morphisms with arbitrary types in their domains and codomains as well as arbitrary generating equalities relating pairs of terms.
If we omit all type operations (so that we have a theory only of symmetric multicategories) but allow arbitrary generating equalities, then we obtain what might be called \textbf{linear finitary algebraic theories}: a set of types, a set of operations with finite arities and types, and a set of axioms about the the composites of those operations such that ``each variable appears exactly once on both sides of each axiom''.
For instance, the theory of monoids is linear, with the axioms:
\begin{align*}
  x:A,y:A,z:A &\types x\cdot (y\cdot z) \equiv (x\cdot y)\cdot z :A\\
  x:A &\types x\cdot e \equiv x:A\\
  x:A &\types e\cdot x \equiv x:A
\end{align*}
but the theory of groups, which adds a unary operation $i$ and the axioms
\begin{align*}
  x:A &\types x\cdot i(x) \equiv e:A \\
  x:A &\types i(x)\cdot x \equiv e:A
\end{align*}
is not.
Note that formally, we do not have to give a precise meaning to ``each variable appears exactly once on both sides of each axiom''; instead the terms that can appear in axioms are defined inductively by rules that happen to \emph{ensure} that this condition holds.

If \cG is a linear finitary algebraic theory, then of course it generates a free symmetric multicategory $\F\bSymMulti\cG$ whose objects are precisely the (base) types of \cG.
In particular, if \cG has one type, then $\F\bSymMulti\cG$ has one object.
A (symmetric) multicategory with one object is called an \textbf{operad} (enriched in \bSet --- though much of the interest of operads lies in operads enriched in other categories); they were originally defined by~\cite{may:goils}, and the terminology has since been much generalized~\cite{leinster:higher-opds}.
(Indeed, arbitrary multicategories are sometimes called ``colored operads''.)

On the other hand, there are cases where we want to allow tensor products in the codomain.
For instance, the theory of bimonoids mentioned in \cref{sec:intro} would have one type $A$, four generating morphisms
\begin{mathpar}
  m:(A,A)\to A\and
  e:() \to A\and
  \comult:A\to A\tensor A\and
  \counit : A\to \one
\end{mathpar}
and the axioms shown in \cref{fig:smc-bimonoid}.
An antipode would augment this by a generating morphism $i:A\to A$ and the axioms
\begin{align*}
  x:A &\types \match_\tensor(\comult(x),uv.m(u,i(v))) \equiv \match_\one(\counit(x),e) :A\\
  x:A &\types \match_\tensor(\comult(x),uv.m(i(u),v)) \equiv \match_\one(\counit(x),e) :A
\end{align*}

\begin{figure}
  \centering
  \begin{align*}
    x:A,y:A,z:A &\types x\cdot (y\cdot z) \equiv (x\cdot y)\cdot z :A\\
    x:A &\types x\cdot e \equiv x:A\\
    x:A &\types e\cdot x \equiv x:A\\
    x:A &\types
          \begin{multlined}[t]
            \match_\tensor(\comult(x),uv.\tpair{u}{\comult(v)})\\[\jot]
            \equiv
            \match_\tensor(\comult(x),uv.\match_\tensor(\comult(u),wz.\tpair{w}{\tpair{z}{v}})\\
            : A\tensor (A\tensor A)
          \end{multlined}\\
    x:A &\types \match_\tensor(\comult(x),uv.\match_\one(\counit(u),v)) \equiv x : A\\
    x:A &\types \match_\tensor(\comult(x),uv.\match_\one(\counit(v),u)) \equiv x : A\\
    x:A,y:A &\types
              \begin{multlined}[t]
                \match_\tensor(\comult(x),uv,\match_\tensor(\comult(y),wz.\tpair{m(u,w)}{m(v,z)}))\\[\jot]
                \equiv \comult(m(x,y)) : A\tensor A
              \end{multlined}\\
    x:A,y:A &\types \counit(m(x,y)) \equiv \match_\one(\counit(x),\match_\one(\counit(y),\ott)) :\one\\
    () &\types \comult(e) \equiv \tpair{e}{e} :A\tensor A\\
    () &\types \counit(e) \equiv \ott :\one
  \end{align*}
  \caption{Axioms for a bimonoid}
  \label{fig:smc-bimonoid}
\end{figure}

If we also include another antipode $j:A\to A$, we can then try to
reproduce the uniqueness argument from \cref{sec:intro}.

[TODO]


\subsection*{Exercises}

\begin{ex}\label{ex:smc}
  Finish the theory of this section: prove the admissibility of substitution, state the $\beta$- and $\eta$-conversion rules, and prove the initiality theorem.
\end{ex}

\begin{ex}\label{ex:smc-usage}
  Another approach to linear type theory is to annotate some types in the context with an ``unused'' marker such as $(-)^0$, and allow weakening of ``unused'' types.
  Thus we could have for instance $x:A^0,y:B,z:C^0 \types y:B$, since $x$ and $z$ are marked as unused.
  Two of these contexts $\Gamma$ and $\Delta$ can be \emph{merged} if they contain the same types in the same order, and each type is used (the opposite of unused) in at most one of them; in that case they merge to a context $\Gamma\merge\Delta$ containing the same types again, with those used that are used in either $\Gamma$ or $\Delta$.
  For instance, $(A^0,B,C^0)\merge(A,B^0,C^0) = (A,B,C^0)$.
  \begin{enumerate}
  \item Formulate a type theory containing $\otimes,\times,+,\hom$ using contexts with usage markers and merging rather than concatenation.
    For instance, the rule $\tensorI$ should be
    \[ \inferrule{\Gamma\types M:A \\ \Delta\types N:B}{\Gamma\merge\Delta \types \tpair M N : A\tensor B.}\]
    Prove that exchange, and weakening for unused types, are admissible and functorial, by pushing them up the entire derivation as we did in the cartesian case in \cref{sec:stlc}.
  \item Define a corresponding multicategory-like structure whose domains are lists with usage markers, and establish a correspondence of some sort with symmetric monoidal categories.
  \item Prove an initiality theorem.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:semicart-relevance-strucadm}
  Is it possible to formulate a theory for semicartesian or relevance monoidal categories in which the appropriate structural rules are admissible?
\end{ex}


\ChapterExercises


\chapter{Classical type theories}
\label{chap:polycats}


\section{Polycategories and linear logic}
\label{sec:cllin}

[Mention linearly distributive categories and $\ast$-autonomous categories.
But don't belabor them, and perhaps just cite references like~\cite{cs:wkdistrib} for their universal characterizations and initiality theorems.]


\section{Classical logic}
\label{sec:classical}

[Cartesian polycategories.  I expect that the polycomposition coming from their definition as generalized polycategories includes the ``mix rule'', and is sufficient for a direct structural proof of cut admissibility.]


\section{Posetal props and symmetric monoidal posets}
\label{sec:proppos-smpos}

Now let us consider sequents $\Gamma\types\Delta$ where the commas on both sides are intended to represent the \emph{same} tensor product $\tensor$.
For simplicity, we assume this tensor product is symmetric, so that we have an exchange rule on both sides.
This leads us to consider different identity and cut rules (in fact, two identity rules):
\begin{mathpar}
  \inferrule*{ }{()\types()}\and
  \inferrule*{\Gamma\types\Delta}{\Gamma,A\types \Delta,A}\and
  \inferrule*[Right=cut]{\Gamma\types\Xi,\Psi \\ \Psi,\Phi \types \Delta}{\Gamma,\Phi \types \Delta,\Xi}
\end{mathpar}
As always, of course, we intend for the cut rule to be admissible, but writing down at this point what it should be helps us build it into other rules appropriately.

To prove an initiality theorem, we need an appropriate categorical structure.
We define a \textbf{prop} to be a symmetric polygraph \cP together with the following data and axioms:
\begin{enumerate}
\item A morphism $\idfunc_{()}:()\to ()$.
\item For every $A\in\cP$, a morphism $\idfunc_A :(A)\to (A)$.
  % For every $f:\Gamma\to\Delta$ and object $A$, a morphism $(f,\idfunc_A):(\Gamma,A) \to (\Delta,A)$.
  % By induction from this and the previous, we have $\idfunc_{\Gamma} :\Gamma\to\Gamma$ for any $\Gamma$.
\item For every $f:\Gamma\to (\Xi,\Psi)$ and $g:(\Psi,\Phi)\to \Delta$, a composite $g\circ_\Psi f : (\Gamma,\Phi) \to (\Xi,\Delta)$.
% \item Identities are invariant under permutation: $\sigma\idfunc_{\Gamma}\sigma = \idfunc_{\sigma\Gamma}$.
\item Composition is invariant under permutation: $\tau(g\circ_\Psi f)\sigma = (\tau g)\circ_\Psi (f\sigma)$ and $g\sigma \circ_\Psi f = g\circ_{\sigma \Psi} \sigma f$.
\item Composition is unital:
  % $g\circ_\Psi \idfunc_\Psi = g$ and $\idfunc_\Psi\circ_\Psi f = f$.
  $g\circ_A \idfunc_A = g$ and $\idfunc_A\circ_A f = f$.
\item Composition is associative: given $f:\Gamma\to (\Xi,\Theta,\Psi)$ and $g:(\Psi,\Phi)\to (\Delta,\Upsilon)$ and $h:(\Upsilon,\Theta,\Lambda)\to \Omega$, we have
  \[h \circ_{\Upsilon,\Theta} (g\circ_\Psi f) = (h\circ_\Upsilon g) \circ_{\Theta,\Psi} f \qquad \text{(as morphisms} (\Gamma,\Phi,\Lambda) \to (\Xi,\Delta,\Omega)). \]
\item Composition is interchanging: $g\circ_{()}f = f\circ_{()}g$ (modulo a symmetry action).
\end{enumerate}
By composing along empty lists, we get a ``tensor product'' of morphisms: if $f:\Gamma\to\Xi$ and $g:\Phi\to\Delta$, then $g\circ_{()}f : (\Gamma,\Phi) \to (\Xi,\Delta)$.
(This sort of operation is what distinguishes a prop from a ``properad''.)
In particular, we can produce identity morphisms for lists: $\idfunc_{(A,B)} = \idfunc_A \circ_{()} \idfunc_B$ and so on.

Similarly we have $f\circ_{()}g : (\Phi,\Gamma) \to (\Delta,\Xi)$, and the interchange axiom means that these two morphisms are related by the appropriate symmetric group actions.
In the special case when $\Gamma=\Phi=\Xi=\Delta=()$, the Eckmann--Hilton argument implies that the morphisms $()\to ()$ form a commutative monoid; thus our props are the original ones of Adams--MacLane~\cite{maclane:natural-assoc,maclane:cat-alg} rather than the ``graphical'' ones of Batanin--Berger~\cite{bb:htapm}.

It may not be obvious that the above axioms do actually give the original notion of prop.
Eventually we will prove this; but in this section we restrict to the posetal case, in which case the axioms are irrelevant.
By a \textbf{posetal prop} we mean a prop in which there is at most one morphism $\Gamma\to\Delta$ for any $\Gamma,\Delta$.
Given a relational polygraph \cG, we define the \textbf{type theory for posetal props under \cG} to have the two identity rules and a Yoneda-ified generator rule, plus a primitive exchange rule on both sides (in \cref{sec:prop-smc} we will make exchange admissible):
\begin{mathpar}
  \inferrule{ }{()\types()}\and
  \inferrule{\Gamma\types\Delta}{\Gamma,A\types \Delta,A}\and
  \inferrule{\Gamma\types\Xi,\Psi \\ f\in\cG(\Psi,\Phi; \Delta)}{\Gamma,\Phi \types \Delta,\Xi}\\
  \inferrule{\Gamma,A,B,\Delta\types \Psi}{\Gamma,B,A,\Delta\types \Psi}\and
  \inferrule{\Gamma\types \Delta,A,B,\Psi}{\Gamma\types \Delta,B,A,\Psi}
\end{mathpar}

\begin{thm}\label{thm:prop-cutadm}
  For any polygraph \cG, the intended cut rule is admissible in the type theory for posetal props under \cG:
  \[ \inferrule*[Right=cut]{\Gamma\types\Xi,\Psi \\ \Psi,\Phi \types \Delta}{\Gamma,\Phi \types \Delta,\Xi}\]
  [TODO: To deal with primitive exchange, this rule may need to incorporate permutations.]
\end{thm}
\begin{proof}
  As always, we induct on the derivation of $\Psi,\Phi \types \Delta$.
  \begin{enumerate}
  \item If it is the empty identity rule $()\types()$, then $\Gamma\types\Xi,\Psi$ is just $\Gamma\types\Xi$ and is also the desired conclusion.
  \item If it ends with the other identity rule, then there are two cases.
    \begin{enumerate}
    \item If $A$ is in $\Phi$, then we have $\Phi=\Phi',A$ and $\Delta=\Delta',A$ and a derivation of $\Psi,\Phi'\types\Delta'$.
      Applying the inductive hypothesis to this we get $\Gamma,\Phi'\types \Xi,\Delta'$, and then applying the identity rule again gives the desired conclusion.
    \item If $A$ is in $\Psi$, then we have $\Psi=\Psi',A$ and $\Delta=\Delta',A$ and a derivation of $\Psi',\Phi\types\Delta'$.
      Now we can inductively cut this with the given $\Gamma\types \Xi,A,\Psi'$ along $\Psi'$ to get $\Gamma,\Phi\types \Xi,A,\Delta'$ which is the desired conclusion.
    \end{enumerate}
  \item Finally, suppose $\Psi,\Phi \types \Delta$ ends with the rule for an $f$.
    Thus, we have $\Delta=\Delta_1,\Delta_2$ and $\Psi=\Psi_1,\Psi_2$ and $\Phi=\Phi_1,\Phi_2$, where $f\in\cG(\Psi_2,\Phi_2,\Upsilon;\Delta_1)$ and a given derivation of $\Psi_1,\Phi_1\types \Delta_2,\Upsilon$.
    We first inductively cut the latter with our given $\Gamma\types \Xi,\Psi_1,\Psi_2$ along $\Psi_1$ to get $\Gamma,\Phi_1\types \Xi,\Psi_2,\Delta_2,\Upsilon$, then apply the $f$ rule on $\Psi_2,\Upsilon$ to get the desired $\Gamma,\Phi_1,\Phi_2\types \Xi,\Delta_1,\Delta_2$ as desired.\qedhere
  \end{enumerate}
\end{proof}

\begin{thm}\label{thm:posprop-initial}
  For any relational polygraph \cG, the free posetal prop generated by \cG is described by the type theory for posetal props under \cG: its objects are those of \cG, and we have $\Gamma\le\Delta$ just when $\Gamma\types\Delta$ is derivable.
\end{thm}
\begin{proof}
  \cref{thm:prop-cutadm} implies that these definitions give us a posetal prop $\F\bSMPos\cG$.
  Now if \cM is any other posetal prop and $P:\cG\to\cM$ is a map of relational polygraphs, to extend $P$ to $\F\bSMPos\cG$ it suffices to check that it preserves the relation.
  This follows by an easy induction on the rules.
\end{proof}

Now, there are actually \emph{three} ways we could imagine introducing tensor products in a prop.
Fortunately, they are all equivalent.

\begin{lem}\label{thm:prop-tensor}
  For any two objects $A$ and $B$ in a prop \cP, the following are equivalent.
  \begin{enumerate}
  \item A morphism $(A,B) \to A\tensor B$, precomposition with which defines bijections\label{item:prop-tensor-left}
    \[ \cP(\Gamma,A\tensor B;\Delta) \to \cP(\Gamma,A,B;\Delta) \]
  \item A morphism $A\tensor B \to (A,B)$, postcomposition with which defines bijections\label{item:prop-tensor-right}
    \[ \cP(\Gamma;\Delta,A\tensor B) \to \cP(\Gamma;\Delta,A,B) \]
  \item Morphisms $(A,B) \to A\tensor B$ and $A\tensor B \to (A,B)$ whose composites in both directions
    \begin{gather*}
      A\tensor B \too (A,B)  \too A\tensor B\\
      (A,B)\too A\tensor B \too (A,B)
    \end{gather*}
    are identities.\label{item:prop-tensor-iso}
  \end{enumerate}
\end{lem}
\begin{proof}
  If we have~\ref{item:prop-tensor-iso}, then pre- or post-composing with the other morphism yields the bijections in~\ref{item:prop-tensor-left} and~\ref{item:prop-tensor-right}.
  On the other hand, given~\ref{item:prop-tensor-left}, the identity $(A,B)\to (A,B)$ must be the composite $(A,B)\to A\tensor B \to (A,B)$ for some unique morphism $A\tensor B \to (A,B)$, and the other composite must be the identity since its precomposite with $(A,B)\to A\tensor B$ is $(A,B)\to A\tensor B$; so~\ref{item:prop-tensor-iso} holds.
  (This is basically the Yoneda lemma.)
  The case of~\ref{item:prop-tensor-right} is dual.
\end{proof}

The case with units is similar.

\begin{lem}\label{thm:prop-unit}
  In a prop \cP, the following are equivalent.
  \begin{enumerate}
  \item A morphism $()\to \one$, precomposition with which defines bijections
    \[\cP(\Gamma,\one;\Delta) \to \cP(\Gamma;\Delta).\]
  \item A morphism $\one\to()$, postcomposition with which defines bijections
    \[\cP(\Gamma;\Delta,\one) \to \cP(\Gamma;\Delta).\]
  \item Morphisms $()\to \one$ and $\one\to()$ whose composites in both directions are identities.\qed
  \end{enumerate}
\end{lem}

\begin{thm}\label{thm:prop-smc}
  There is an equivalence between (1) symmetric monoidal categories and (2) props with tensors and units in the sense of \cref{thm:prop-tensor,thm:prop-unit}.
\end{thm}
\begin{proof}
  Given a symmetric monoidal category \cC, we define a prop \cP by
  \[ \cP(A_1,\dots,A_n ; B_1,\dots,B_m) = \cC(A_1\tensor \cdots\tensor A_n, B_1\tensor\cdots\tensor B_m)\]
  or, more succinctly, $\cP(\Gamma;\Delta) = \cC(\bigtensor\Gamma, \bigtensor\Delta)$.
  Of course, $\bigtensor() = \one$.
  The identity rules come from $\idfunc_\one$ and $f\tensor \idfunc_A$, while the composite of $f:\Gamma\to (\Xi,\Psi)$ and $g:(\Psi,\Phi)\to \Delta$ is $(\bigtensor\Xi \tensor g) \circ (f\tensor \bigtensor \Phi)$.
  Of course, this prop has tensors and units quite trivially.

  Conversely, given a prop with tensors and units, we have an underlying category with an object $\one$ and an operation $\tensor$.
  The functoriality of $\tensor$ is defined on $f:A\to A'$ and $g:B\to B'$ as the composite
  \[ A\tensor B \too (A,B) \xto{(f,\idfunc)} (A',B) \xto{(\idfunc,g)} (A',B') \too A'\tensor B'. \]
  The axioms of a prop ensure that this is the same as if we put $f$ and $g$ in the other order (or we could write simply $(f,g)$ using the yet-to-be-defined polycomposition).
  Functoriality is similarly easy.
  For associativity, we have
  \[ ((A\tensor B)\tensor C) \too (A\tensor B,C) \too (A,B,C) \too (A,B\tensor C) \to (A\tensor (B\tensor C)) \]
  and for unitality we have
  \[ (A\tensor \unit) \too (A,\unit) \too (A) \]
  and dually.
  It is straightforward to check that these are natural isomorphisms and satisfy the necessary axioms.
  For symmetry, we use the fact that our prop is a \emph{symmetric} polygraph and act by symmetry on the morphisms $(A,B)\to A\tensor B$ and $A\tensor B\to (A,B)$, obtaining ``inverse isomorphisms'' $(B,A) \to A\tensor B$ and $A\tensor B \to (B,A)$.
  From these we can show $A\tensor B \cong B\tensor A$ and check naturality and the axioms.

  It is straightforward to check that these two constructions are inverses.
\end{proof}

\cref{thm:prop-tensor,thm:prop-unit} give several ways to introduce $\tensor$ and $\unit$ into the type theory of posetal props.
For a sequent calculus, the symmetrical approach is probably the most natural:
\begin{mathpar}
  \inferrule{\Gamma,A,B\types \Delta}{\Gamma,A\tensor B\types \Delta}\;\tensorL\and
  \inferrule{\Gamma\types \Delta,A,B}{\Gamma\types \Delta,A\tensor B}\;\tensorR\and
  \inferrule{\Gamma\types\Delta}{\Gamma,\one\types\Delta}\;\one L\and
  \inferrule{\Gamma\types\Delta}{\Gamma\types\Delta,\one}\;\one R\and
\end{mathpar}
Of course, we also include the identity and generator rules (the former only for base types coming from \cG), the exchange rules, and the type judgment:
\begin{mathpar}
  \inferrule{ }{()\types()}\and
  \inferrule{\Gamma\types\Delta\\ A\in\cG}{\Gamma,A\types \Delta,A}\and
  \inferrule{\Gamma\types\Xi,\Psi \\ f\in\cG(\Psi,\Phi; \Delta)}{\Gamma,\Phi \types \Delta,\Xi}\\
  \inferrule{\Gamma,A,B,\Delta\types \Psi}{\Gamma,B,A,\Delta\types \Psi}\and
  \inferrule{\Gamma\types \Delta,A,B,\Psi}{\Gamma\types \Delta,B,A,\Psi}\\
  \inferrule{A\in\cG}{\types A\type}\and
  \inferrule{ }{\types \one\type}\and
  \inferrule{\types A\type \\ \types B\type}{\types A\tensor B\type}
\end{mathpar}
We call this the \textbf{classical sequent calculus for symmetric monoidal posets under \cG}.
(``Classical'' because we have multiple formulas on both sides, ``posets'' because we are not yet distinguishing derivations or introducing terms.)

\begin{thm}\label{thm:seqcalc-smpos-invertible}
  All the rules for $\tensor$ and $\one$ in the classical sequent calculus for symmetric monoidal posets under \cG are invertible.
\end{thm}
\begin{proof}
  Suppose we have a derivation of $\Gamma,A\tensor B\types \Delta$; we want a derivation of $\Gamma,A,B\types \Delta$.
  We induct on the given derivation.
  \begin{enumerate}
  \item The easy case is when it ends with $\tensorL$ whose premise is what we want.
  \item If it ends with $\tensorL$ acting on another type, or any of $\tensorR$, $\one L$, or $\one R$, then we induct on the premise and then apply that rule to the result.
  \item If it ends with the identity rule for $C$, then since $C\in \cG$ it can't be $A\tensor B$, so we can again induct on the premise and apply the identity rule afterwards.
  \item Finally, if it ends with a generator rule for $f$, then since the domain of $f$ is a list of objects of $\cG$, none of them can be $A\tensor B$; so $A\tensor B$ must be in the sequent that the generator rule cuts with, and we can induct on that and then apply the generator rule.
  \end{enumerate}
  The other rules $\tensorR$, $\one L$, and $\one R$ are similar.
\end{proof}

\begin{thm}\label{thm:seqcalc-smpos-identity}
  The following general identity rule is admissible in the classical sequent calculus for symmetric monoidal posets under \cG:
  \begin{mathpar}
    \inferrule{\Gamma\types\Delta\\ \types A\type}{\Gamma,A\types \Delta,A}\and
  \end{mathpar}
  In particular, since $()\types()$, we have $A\types A$ whenever $\types A\type$.
\end{thm}
\begin{proof}
  We induct on the derivation of $\types A\type$.
  \begin{enumerate}
  \item If it is from \cG, we apply the postulated identity rule.
  \item If it is $\one$, we have the following derivation:
    \begin{mathpar}
      \inferrule*{\inferrule*{\inferrule*{ }{()\types()}}{() \types \one}}{\one\types\one}
    \end{mathpar}
  \item If it is $A_1\tensor A_2$, we have by induction a derivation $\sD_1$ of $\Gamma,A_1\types,\Delta,A_1$, and then (again by induction) a derivation $\sD_2$ of $\Gamma,A_1,A_2\types \Delta,A_1,A_2$.
    Now we augment this in a similar way:
    \begin{equation*}
      \inferrule*{
        \inferrule*{
          \inferrule*{\sD_2\\\\\vdots}{\Gamma,A_1,A_2\types \Delta,A_1,A_2}
        }{
          \Gamma,A_1\tensor A_2\types \Delta,A_1,A_2
        }}{
        \Gamma,A_1\tensor A_2\types \Delta,A_1\tensor A_2
      }\qedhere
    \end{equation*}
  \end{enumerate}
\end{proof}

\begin{thm}\label{thm:seqcalc-smpos-cutadm}
  The prop cut rule from \cref{thm:prop-cutadm} is admissible in the classical sequent calculus for symmetric monoidal posets under \cG.
\end{thm}
\begin{proof}
  Suppose given derivations of $\Gamma\types\Xi,\Psi$ and $\Psi,\Phi \types \Delta$; we want to derive $\Gamma,\Phi \types \Delta,\Xi$.
  We induct on the derivation of $\Psi,\Phi \types \Delta$.
  \begin{enumerate}
  \item If it ends with an identity or a generator, we do as in \cref{thm:prop-cutadm}.
  \item If it ends with $\tensorR$ or $\one R$, we can simply apply the inductive hypothesis to cut with the premise of that rule.
  \item If it ends with $\tensorL$, there are two cases.
    If the introduced $A\tensor B$ is in $\Phi$, then we can inductively cut the premise along $\Psi$ again.
  \item On the other hand, if the introduced $A\tensor B$ is in $\Psi$, then our other derivation has the form $\Gamma\types\Xi,\Psi',A\tensor B$.
    Since $\tensorR$ is invertible by \cref{thm:seqcalc-smpos-invertible}, we can also derive $\Gamma\types\Xi,\Psi',A,B$, and then cut with the premise $\Psi',A,B,\Phi \types \Delta$ of our $\tensorL$.
  \item The case of $\one L$ is similar.\qedhere
  \end{enumerate}
\end{proof}

\begin{thm}\label{thm:seqcalc-smpos-initial}
  For any relational polygraph \cG, the free symmetric monoidal poset generated by \cG is described by the classical sequent calculus for symmetric monoidal posets under \cG.
\end{thm}
\begin{proof}
  \cref{thm:seqcalc-smpos-identity,thm:seqcalc-smpos-cutadm} imply in particular that the sequent calculus gives us a posetal prop $\F\bSMPos\cG$.
  Moreover, the rules for $\tensor$ and $\one$ give $\F\bSMPos\cG$ the tensor and unit structure of \cref{thm:prop-tensor,thm:prop-unit}, so that it is a symmetric monoidal poset.
  Now if \cM is any other symmetric monoidal poset and $P:\cG\to\cM$ a map of symmetric relational polygraphs, there is a unique extension of $P$ to the objects of $\F\bSMPos\cG$.
  As usual, by induction on the rules of the sequent calculus and the fact that \cM is a posetal prop, this extension preserves the relations $\Gamma\le\Delta$.
  Finally, it also preserves the tensor and unit structure, so it is a strict functor of symmetric monoidal posets.
\end{proof}


\section{Props and symmetric monoidal categories}
\label{sec:prop-smc}

So much for symmetric monoidal posets.
What we really want, however, is a type theory for symmetric monoidal \emph{categories}, in which we can talk about equality of morphisms, and that can also deal with tensors in the codomain.

In our type theories for categories and multicategories in \cref{sec:category-cutadm,sec:multicat-moncat} (before introducing any operations such as products or tensors), we did not have to impose any equivalence relation on the derivations.
However, in the case of props, the interchange rule makes things more complicated.
For instance, if we have $f\in \cG(A;B)$ and $g\in\cG(C;D)$, then in the type theory of \cref{sec:proppos-smpos} are two distinct derivations of a sequent representing $f\circ_{()} g$:
\begin{mathpar}
  \inferrule*[Right=$g$]{
    \inferrule*{
      \inferrule*[Right=$f$]{
        \inferrule*{\inferrule*{ }{()\types()}}{A\types A}
      }{
        A\types B
      }}{
      A,C\types B,C
    }}{
    A,C\types B,D
  }\and
  \inferrule*[Right=$f$]{
    \inferrule*{
      \inferrule*[Right=$g$]{
        \inferrule*{\inferrule*{ }{()\types()}}{C\types C}
      }{
        C\types D
      }}{
      A,C\types A,D
    }}{
    A,C\types B,D
  }\and
\end{mathpar}
If we write down a term calculus whose terms correspond exactly to derivations in this theory, as we usually do, then the desired equality between these two derivations would look something like
\begin{equation}
  x:A, y:C \types (f,\idfunc)((\idfunc,g)(x,y)) \equiv (\idfunc,g)((f,\idfunc)(x,y)) : (B,D)\label{eq:prop-bad-terms}
\end{equation}
Note that unlike the $\beta$- and $\eta$-conversions, the equality~\eqref{eq:prop-bad-terms} is not directional: it makes no sense to regard one or the other side as ``simpler'' or ``more canonical'' than the other.
We would like to avoid having to assume such equalities in $\equiv$, and furthermore the terms appearing in~\eqref{eq:prop-bad-terms} are rather ugly.
One approach to deal with this would be to break the bijection between terms and deductions, in a way that enables us to represent both of the above two derivations by the same term.
However, a better approach is to design a different theory in which there is only \emph{one} derivation of $f\circ_{()} g$, allowing us to maintain the principle that terms correspond uniquely to derivations.

The non-directionality of~\eqref{eq:prop-bad-terms} also makes it unclear how to design a type theory in which one would be permitted but not the other.
Instead we will forbid \emph{both} of them, replacing the generator rule by a ``multi-generator'' rule allowing only a one-step derivation
\[ \inferrule{x:A,y:C \types (x,y):(A,C)}{x:A,y:C \types (f(x),g(y)):(B,D)}\;f,g \]
The intuition in this term notation is of course that $f(x):B$ and $g(y):D$.
We could write it as ``$f(x):B,g(y):D$'', but we choose to tuple the terms up as in $(f(x),g(y)):(B,D)$ for a couple of reasons.
The first reason is that when doing equational reasoning (such as for the antipode calculation), the equalities must relate entire tuples rather than single terms.
The second reason is that in general, we also need to include some ``terms without a type'' (e.g.\ coming from morphisms with empty codomain $()$, which is a judgmental representation of the unit object), and this looks a little nicer when all the terms are grouped together: we write for instance $(f(x),g(y)\mid h(z))$ to mean that $h(z):()$.

There are also, of course, function symbols with \emph{multiple} outputs.
To deal with this case we write $f_1(x)$, $f_2(x)$, and so on for the terms corresponding to all the types in the codomain.
For example, we write the composite of $f:(A,B) \to (C,D)$ with $g:(E,D)\to (F,G)$ as
\[ x:A, y:B, z:E \types (f_1(x,y),g_1(z,f_2(x,y)),g_2(z,f_2(x,y))):(C,F,G) \]
Note that the variables in a context are not literally treated ``linearly'', since they can occur multiple times in the multiple ``components'' of a map $f$.
Instead the ``usages'' of a variable are controlled by the codomain arity of the morphisms applied to them.
In general, we write $\vec f(\vec M)$ for the list of ``all the values of $f$'' applied to the list of arguments $\vec M$.

This does require one further technical device (that will be almost invisible in practice).
Suppose we have $f:()\to (B,C)$, written in our type theory as $()\types (f_1,f_2):(B,C)$, and we compose/tensor it with itself to get a morphism $() \to (B,B,C,C)$.
We would na\"ively write this as $() \types (f_1,f_1,f_2,f_2)$, but this is ambiguous since we can't tell which $f_1$ matches which $f_2$.
We disambiguate the possibilities by writing $() \types (f_1,f'_1,f_2,f'_2)$ or $() \types (f_1,f'_1,f'_2,f_2)$.
Although this issue seems to only arise for morphisms with empty domain and greater than unary codomain, for consistency we formulate the syntax with a label (like $'$) on \emph{every} term former, and simply omit them informally when there is no risk of ambiguity (which includes the vast majority of cases).
We assume given an infinite alphabet of symbols \fA for this purpose (such as $','',''',\dots$, or $1,2,3,\dots$), and we annotate our judgments with a finite subset $\fB\subseteq \fA$ indicating which labels might have been used already in the terms.

Thus, a first approximation to our generator rule is
\[ \inferrule*{
  \Gamma \types^\fB (\vec M^1,\dots,\vec M^n,\vec N \mid \vec Z)
  : (\vec A^1,\dots, \vec A^n,\vec C)\\
  f^1 \in \cG(\vec A^1,\vec D^1)\\\cdots\\
  f^n \in \cG(\vec A^n,\vec D^n)\\\\
  \fa_i\notin \fB\text{ and pairwise distinct}
}{\Gamma \types^{\fB\cup\{\fa_i\}} (\vec f^{1,\fa_1}(\vec M^1),\dots,\vec f^{n,\fa_n}(\vec M^n),\vec N \mid \vec Z)
  : (\vec D^1,\dots, \vec D^n,\vec C)}
\]
Since we have taken over subscripts to indicate the multiple outputs of a single operation, we are using superscripts to distinguish among the $n$ generators $f^1,\dots,f^n$ being applied at once.
For instance, if $f^1\in \cG((A^1_1,A^1_2),(D^1_1,D^1_2,D^1_3))$ then we have
\[ x^1_1:A^1_1, x^1_2:A^1_2 \types (f^1_1(x^1_1,x^1_2),f^1_2(x^1_1,x^1_2),f^1_3(x^1_1,x^1_2)):(D^1_1,D^1_2,D^1_3) \]

We also have to modify the above rule to allow generators with empty target, which get collected into the $\vec Z$'s.
Thus we have
\[ \inferrule*{
  \Gamma \types^\fB (\vec M^1,\dots,\vec M^n,\vec P^1,\dots,\vec P^m,\vec N \mid \vec Z)
  : (\vec A^1,\dots, \vec A^n,\vec B^1,\dots,\vec B^m,\vec C)\\
  f^1 \in \cG(\vec A^1,\vec D^1)\\\cdots\\
  f^n \in \cG(\vec A^n,\vec D^n)\\\\
  g^1 \in \cG(\vec B^1,())\\\cdots\\
  g^m \in \cG(\vec B^n,())\\\\
  \fa_i,\fb_j\notin \fB\text{ and pairwise distinct}
}{\Gamma \types^{\fB\cup\{\fa_i,\fb_j\}} \left(\vec f^{1,\fa_1}(\vec M^1),\dots,\vec f^{n,\fa_n}(\vec M^n),\vec N
    \;\middle|\; \vec g^{1,\fb_1}(\vec P^1),\dots,\vec g^{m,\fb_m}(\vec P^m), \vec Z\right)
  : (\vec D^1,\dots, \vec D^n,\vec C)}
\]
Eventually we will also have to incorporate shuffles into the rulea as in \cref{sec:symmoncat}, but we postpone that for now.
Let us consider instead how to prevent duplication of derivations.
In addition to our desired term
\begin{align}
  x:A, y:C &\types (f(x),g(y)):(B,D)\label{eq:prop-good-term}
  \intertext{we must also be able to write}
  x:A, y:C &\types (f(x),y):(B,C)\label{eq:prop-goodish-term-1}\\
  \intertext{and}
  x:A, y:C &\types (x,g(y)):(A,D)
\end{align}
so how do we prevent ourselves from being able to apply the generator rule again to the latter two, obtaining two more derivations of the same morphism as~\eqref{eq:prop-good-term}?
The idea is to force ourselves to ``apply all functions as soon as possible'': we cannot apply $g$ to $y$ in~\eqref{eq:prop-goodish-term-1} because we \emph{could have} already applied it to produce~\eqref{eq:prop-good-term}.
On the other hand, we could apply $h:(B,C) \to E$ in~\eqref{eq:prop-goodish-term-1} to get
\[ x:A, y:C \types (h(f(x),y)):E \]
because $h$ uses $f$ as one of its inputs and so could not have been applied at the same time as $f$.

In the general case, in a judgment $\Gamma\types^\fB (\vec{M}\mid\vec{W}):\Delta$ we will assign to each label $\fa\in\fB$ (that is, to each ``use'' of a generator morphism) a natural number called its \textbf{depth}.
The depth of $f^\fa$ is defined recursively to be one greater than the maximum depth of all the labels occurring ``outermost'' in the arguments of $f^\fa$, with variables and 0-ary generators considered to have depth $0$.
Thus for instance in $x:A, y:C \types (h(f(x),y)):E$, the depth of $f$ is $1$ since its argument is a variable $x$, hence of depth $0$; while the depth of $h$ is $2$ since its arguments $f(x)$ and $y$ have depth $1$ and $0$ respectively.
(This definition appears to require inspecting the term, but when we make it formal it will be part of the derivation.)

Now we require, as a condition on the application of the generator rule, that each of the new generators appearing in the conclusion (the $f_i$ and $g_j$) is applied to at least one argument that is of maximum depth in the principal premise.
This ensures that the new generators are precisely those of maximum depth in the conclusion.
It also means that we can never apply this rule to 0-ary generators, so we incorporate those into the $\idfunc$ rule







To be precise, we augment our judgments (\emph{not} their terms) by labeling some of the types in the consequent as \emph{active}, denoted by an underline.
The identity rule makes all types active, while the generator rule makes only the outputs of the generators active.
We then restrict the generator rule to require that at least one of the \emph{inputs} of each generator being applied must be active in the premise; this means that none of them could have been applied any sooner, since at least one of their arguments was just introduced by the previous rule.
Thus, our desired derivation
\begin{mathpar}
  \inferrule*[Right={$f,g$}]{\inferrule*{ }{A,C \types \actv{A},\actv{C}}}{A,C\types \actv{B},\actv{D}}
\end{mathpar}
is allowed, while the undesired ones such as
\begin{mathpar}
  \inferrule*[Right=$g$???]{\inferrule*[Right=$f$]{\inferrule*{ }{A,C \types \actv{A},\actv{C}}}{A,C\types \actv{B},C}}{A,C \types B,D}
\end{mathpar}
is not allowed, since in the attempted application of $g$ the input type $C$ is not active.

Of course, this restriction on its own would prevent generators with nullary domain from ever being applied.
Since these can always be applied at the very beginning, we incorporate them into the identity rule.

Finally, since we want to make the exchange rule admissible, we have to build permutations into the rules as well.
As in \cref{sec:symmoncat}, each rule adds only the part of a permutation that hasn't already been specified by the premises.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{
      f_1 \in \cG((),\vec{B_1})\\\cdots\\
      f_n \in \cG((),\vec{B_n})\\\\
      g_1 \in \cG((),())\\\cdots\\
      g_m \in \cG((),())\\\\
      \sigma : {\vec A},{\vec B}_1,\dots, {\vec B}_n \toiso \Delta\\
      \sigma\text{ preserves the relative order of } B_{11}, B_{21},B_{31},\dots, B_{n1}\\
      \text{All types in }\Delta\text{ are active}\\
      \fa_i,\fb_j\in \fB\text{ and pairwise distinct}
    }{\vec x:\vec A\types^\fB
      \left(\sigma\left(\vec x,{\vec f}_1^{\fa_1},\dots,{\vec f}_n^{\fa_n}\right)
      \,\middle|\, g_1^{\fb_1},\dots,g_m^{\fb_m}\right)
      :\Delta}
    \and
    \inferrule{
      \Gamma \types^\fB (\vec M_1,\dots,\vec M_n,\vec P_1,\dots,\vec P_m,\vec N \mid \vec Z)
      : (\vec A_1,\dots, \vec A_n,\vec B_1,\dots,\vec B_m,\vec C)\\
      \text{At least one type in each } \vec A_i, \vec B_j\text{ is active}\\
      f_1 \in \cG(\vec A_1,\vec D_1)\\\cdots\\
      f_n \in \cG(\vec A_n,\vec D_n)\\\\
      g_1 \in \cG(\vec B_1,())\\\cdots\\
      g_m \in \cG(\vec B_m,())\\\\
      \sigma : \vec D_1,\dots,\vec D_n, \vec C \toiso \Delta\\
      \sigma \text{ preserves the relative order of types in } \vec C\\
      \sigma \text{ preserves the relative order of } D_{11}, D_{21}, D_{31},\dots, D_{n1}\\
      \text{In }\Delta\text{ all types from } \vec D_i \text{ are active, all types from } \vec C\text{ are inactive}\\
      \tau \in \mathrm{Shuf}(m,|\vec Z|)\\
      \fa_i,\fb_j\notin \fB\text{ and pairwise distinct}
    }{\Gamma \types^{\fB\cup\{\fa_i,\fb_j\}} %\fa_1,\dots,\fa_n,\fb_1,\dots,\fb_m\}}
      \left(\sigma\left({\vec f}_1^{\fa_1}(\vec M_1),\dots,{\vec f}_n^{\fa_n}(\vec M_n),\vec N\right)
      \,\middle|\, \tau\left({ g}_1^{\fb_1}(\vec P_1),\dots,{ g}_m^{\fb_m}(\vec P_m),\vec Z\right)\right)
      :\Delta
    }
  \end{mathpar}
  \caption{Type theory for props}
  \label{fig:props}
\end{figure}

The resulting \textbf{type theory for props under \cG} is shown in \cref{fig:props}.
Note that nearly all the subscripts shown are for indexing distinct morphisms or contexts, not for distinguishing between the different outputs of the some morphism as above.
The latter is mostly hidden in the notation $\vec f$, though indicated in a couple of places by double subscripts $D_{11}, D_{21},\dots$

The notation is quite heavy, but the idea is not so bad.
For the identity rule, we suppose given a context of variables belonging to types, and a list of generating morphisms with nullary domain.
Then we collect together all the variables in the context together with all the outputs of those of our generators with non-nullary codomain and permute them arbitrarily, except that we maintain the relative order of the first outputs of each generator (this prevents introduction of an extra permutation in the choice of how to order the list of generators).
Combined with our generators with nullary codomain, this gives a list of terms belonging to the appropriately permuted list of types (from the context and the generator outputs), in which all types are active.

For the generator rule, we suppose given a list of generating morphisms with non-nullary domain --- or more precisely two lists, one of morphisms with non-nullary codomain and one of those with nullary codomain.
We also suppose given a term judgment producing the domains of these morphisms concatenated in order, each containing at least one active type, and followed by some other number of types.
Then we collect all the outputs of our generators applied to these inputs, together with the types that were not input into any of them, and permute them arbitrarily, except that we mantain the relative order of the first outputs and of the un-inputted types (again, this prevents introduction of spurious permutations).
We activate all the outputs of our current generators, and deactivate all other types.
Finally, we similarly shuffle together the generators and the given terms of nullary codomain.

[TODO: Examples, unique derivations of terms, admissibility and functoriality of exchange, admissibility of cut, initiality.  Wait until after writing \cref{sec:symmoncat}.]


\newpage

Similarly, we can construct ``presented'' props and symmetric monoidal categories by including arbitrary generators of $\equiv$.
The uniqueness of antipodes in a bimonoid presented in \cref{sec:intro} is an example of this: \cG has one object $M$ and four morphisms
\begin{alignat*}{2}
m&:(M,M)\to M &&\quad\text{(written infix as $m(x,y) = x\cdot y$)}\\
e&:()\to M\\
\comult&:M\to (M,M) &&\quad\text{(written a little abusively as $\comult_i(x) = x_i$)}\\
\counit &:M\to () &&\quad\text{(written $\counit(x) = \cancel{x}$)}
\end{alignat*}
while the generators of $\equiv$ are the bimonoid axioms.

Here is another example.
If $A$ is an object of a prop, a \textbf{dual} of $A$ is an object $A^*$ with morphisms $\eta:()\to (A,A^*)$ and $\counit:(A^*,A)\to ()$ such that $\counit \circ_{A^*} \eta = \idfunc_A$ and $\counit \circ_{A} \eta = \idfunc_A^*$.
(In a symmetric monoidal category this reduces to the usual notion of dual.)
If $A$ has a dual $A^*$, and $f:A\to A$, the \textbf{trace} of $f$ is the composite
\[ \tr(f) = \counit \circ_{A,A^*} (f \circ_A \eta) \]
or equivalently $(\counit \circ_A f)\circ_{A,A^*} \eta$.
In the type theory for props, the axioms of a dual say
\begin{mathpar}
  x:A \types (\eta_1 \mid \counit(\eta_2,x)) \equiv x :A\and
  y:A^* \types (\eta_2 \mid \counit(y,\eta_1)) \equiv y :A^*
\end{mathpar}
while the trace is $(\,\mid \counit(\eta_2,f(\eta_1)))$.
Recall that $\equiv$ is a congruence for substitution; thus the dual axioms mean that \emph{any term} $M$ (appearing even as a sub-term of some other term) can be replaced by $\eta_1^\fa$ if we simultaneously add $\counit^\fb(\eta_2^\fa,M)$ to the list of null terms (here \fa and \fb are fresh labels).
And dually, if $\counit^\fb(\eta_2^\fa,M)$ appears in the null terms, for any term $M$, then it can be removed by replacing $\eta_1^\fa$ (wherever it appears) with $M$.

Now a classical fact about the trace is that it is \emph{cyclic}: if $A$ and $A'$ both have duals $A^*$ and $(A')^*$, and $f:A\to A'$ and $g:A'\to A$, then $\tr(gf) = \tr(fg)$.
To prove this using traditional commutative-diagram reasoning is quite involved.
It does have a pretty and intuitive proof using string diagrams.
However, its proof in the type theory for props is one line:
\[ \tr(gf) = (\,\mid \counit(\eta_2,g(f(\eta_1))))
= (\,\mid \counit(\eta_2,g(\eta'_1)),\counit'(\eta'_2,f(\eta_1)))
= (\,\mid \counit(\eta'_2,f(g(\eta'_1))))
= \tr(fg).
\]
Here $\eta,\counit$ exhibit the dual of $A$, while $\eta',\counit'$ exhibit the dual of $A'$.
The first and last equality are by definition;
the second applies the first duality equation for $A'$ with $x=f(\eta_1)$; and the third applies the first duality equation for $A$ with $x=g(\eta'_1)$.

One thing to note about these examples is that they use only the type theory of props, not its extension to symmetric monoidal categories.
In general, because the underlying prop of a symmetric monoidal category remembers its morphisms both into and out of tensor products, it seems rarely necessary to invoke the actual tensor product objects.

\begin{rmk}
  A \textbf{compact closed} category is a symmetric monoidal category in which every object has a dual.
  By adding appropriate objects, generators, and relations, we can obtain a type theory describing the free compact closed category on a polygraph, or on a graph, or on a category.
  This does not ``solve the coherence problem'' for compact closed categories, however, since with the additional $\equiv$ relations for duals, the terms no longer have an obvious canonical form.
  In fact, it turns out that an explicit description of the free compact closed category on a category can be given, and involves traces in an essential way; see~\cite{kl:cpt}.
\end{rmk}


\chapter{First-order logic}
\label{chap:fol}

[A ``presheaf on a multicategory'' for dependent types: motivate as the ``contravariant representables''.]


\appendix

\chapter{Deductive systems}
\label{chap:dedsys}

The purpose of this appendix is to explain the formal apparatus underlying type theory from a mathematical perspective, giving precise meanings to words like ``judgment'', ``rule'', ``derivation'', and ``binder''.
This is rarely explained in detail, yet in my experience the unfamiliar terminology is a large part of what makes type theory difficult for mathematicians to understand.

Formally speaking, this appendix should come before \cref{chap:unary}.
However, its technicalities seem unlikely to be appreciated without some concrete exposure to the ideas that it is trying to explain, so I have placed it as an appendix instead.
I encourage the reader to skip back and forth between it and the main text as needed.

I should say that probably not all type theorists would agree with the meanings assigned herein to words like ``judgment''.
Constructive type theory also has a philosophical/foundational aspect that I will not attempt to explain or engage with.
The purpose of this appendix, like that of the entire book, is to explain type theory \emph{only} in its role as a language for reasoning about categorical structures, without meaning thereby to disparage its other roles or regard them as unimportant.


\section{Trees and free algebras}
\label{sec:trees}

As remarked in \cref{sec:generalities}, our perspective on type theory is that it presents \emph{free categorical structures} in a particularly convenient way.
Since categorical structures are particular kinds of \emph{algebraic} structures, it seems reasonable to think first about what free algebraic structures look like in general.
In this section we begin by considering ``algebras without axioms''.

A \textbf{signature} $\sig$ is a set $\sig_1$ of \textbf{operations} with a function $\ay:\sig_1 \to \dN$ assigning to each operation a natural number\footnote{Everything in this chapter works just as well if arities are arbitrary cardinal numbers (except that in \cref{sec:axioms} we would require the axiom of choice).
  However, for simplicity we restrict to the case of finite arities, since that is where our ultimate interest lies.
  On the other hand, there may certainly be infinitely many operations.} called its \textbf{arity}.
A $\sig$-\textbf{algebra} is a set $A$ together with, for each $m\in\sig_1$, a function $\act m :A^{\ay(m)} \to A$.
There is an obvious notion of $\sig$-algebra morphism, forming a category.

Algebras over a signature are a very ``primordial'' sort of algebraic structure, with arbitrary operations but no axioms allowed.
For instance, if $\sig_1=\{e,m\}$ with $\ay(e)=0$ and $\ay(m)=2$, then $\sig$-algebras are \emph{pointed magmas}: sets equipped with a basepoint and a binary operation.
We will see how to add axioms in \cref{sec:axioms}.

Free $\sig$-algebras are conveniently described in terms of {trees}.
A \textbf{tree} is a set whose elements are called \textbf{nodes}, together with a binary relation between them called \textbf{edge existence} (a ``relational graph'') that is connected and loop-free.
A tree is \textbf{rooted} if it is equipped with a chosen node called the \textbf{root}.
In a rooted tree every node $x$ has a unique (non-retracing) path to the root; if $x$ is not the root, this path goes through a unique edge connected to $x$ that we call \textbf{outgoing}, and the node at the other end of that edge is the \textbf{parent} of $x$.
The non-outgoing edges connected to $x$ are called \textbf{incoming}, and the nodes they connect it to are called its \textbf{children}.
A node is a \textbf{descendant} of $x$ if its path to the root passes through $x$, which is to say it is a child of a child of a\dots of $x$.
A node $x$ together with all its descendants forms another rooted tree with $x$ as the root.
% The \textbf{height} of a node is the length of its path to the root; this is always a natural number, and the height of a child of $x$ is one more than the height of $x$.
A \textbf{branch} is a non-retracing path starting at the root; a rooted tree is \textbf{well-founded} if there are no infinite branches.

If $\sig$ is a signature, then a $\sig$-\textbf{labeled tree} is a rooted tree equipped with a \textbf{labeling} function from nodes to $\sig_1$, along with for every node $x$ labeled by $m\in\cO$, a bijection from the incoming edges of $x$ to $\{1,\dots,\ay(m)\}$ (hence, in particular, that there are exactly $\ay(m)$ such edges).
There is an obvious notion of \emph{isomorphism} between labeled trees.
We write $W\sig$ for the set of all isomorphism classes of \emph{well-founded} $\sig$-labeled trees.
(Note that $W\sig$ is empty unless there is at least one nullary operation.)
Then $W\sig$ has a $\sig$-algebra structure defined as follows: given $m\in\sig_1$ and a list of trees $t_1,\dots,t_{\ay(m)}$, define a tree $\act m (t_1,\dots,t_{\ay(m)})$ with nodes $\{\star\} \sqcup \bigsqcup_i t_i$, where $\star$ is the root, with label $m$, and its children are the roots of the trees $t_i$.

The central fact is that $W\sig$ is the initial $\sig$-algebra.
We will give a classical set-theoretic proof of this for the comfort of a certain kind of reader, but readers of a different kind, or who already believe this fact, are welcome to skip the proof.
(From a constructive type-theoretic point of view, $W\sig$ and its initiality are sometimes a fundamental axiom not reducible to sets.)

\begin{thm}\label{thm:tree-ind}
  Suppose $P\subseteq W\sig$ has the property that for any $m$ and trees $t_1,\dots,t_{\ay(m)}$ such that each $t_i\in P$, then also $\act m(t_1,\dots,t_{\ay(m)})\in P$.
  Then $P= W\sig$.
\end{thm}
\begin{proof}
  Suppose not, so there is a well-founded $\sig$-labeled tree not in $P$.
  Let $m$ be the label of its root and $t_1,\dots,t_{\ay(m)}$ its children; then our given tree is (isomorphic to) $\act m(t_1,\dots,t_{\ay(m)})$.
  By the contrapositive of our assumption, therefore, there must be some $i$ such that $t_i\notin P$.
  Iterating, we obtain an infinite branch, contradicting well-foundedness.
\end{proof}

\begin{thm}\label{thm:tree-rec}
  For any $\sig$-algebra $A$, there is a unique $\sig$-algebra morphism $W\sig \to A$.
\end{thm}
\begin{proof}
  TODO: standard argument.
\end{proof}

Now that we have \emph{initial} $\sig$-algebras, note that \emph{free} $\sig$-algebras can be constructed by a simple modification.
Given $\sig$ and any set $X$, define a new signature $\sig[X]$ by $\sig[X]_1 = \sig_1 \sqcup X$, where $\ay(x)=0$ for all $x\in X$.
Then a $\sig[X]$-algebra is just a $\sig$-algebra together with a map from $X$ into its underlying set, so the initial such algebra is exactly the free $\sig$-algebra on $X$.
Thus, the forgetful functor from $\sig$-algebras to sets has a left adjoint.

A different way to express \cref{thm:tree-rec} is that \emph{given an arbitrary set $A$, to define a function $W\sig \to A$ it suffices to define a $\sig$-algebra structure on $A$}.
This may seem like a trivial reformulation, but it better reflects the way we use it to describe type theories.

In yet other words, we may define a function $f:W\sig \to A$ by specifying $f(\act m(t_1,\dots,t_{\ay(m)}))$ for each $m$, assuming recursively that $f(t_1),\dots,f(t_{\ay(m)})$ have already been defined.
Formally this is the same as specifying a $\sig$-algebra structure on $A$ --- the definition of ``$f(\act m(t_1,\dots,t_{\ay(m)}))$'' given the ``values of $f(t_1),\dots,f(t_{\ay(m)})$'' is precisely the action $\act m$ on $A$ --- but it often matches our thought processes best.

\subsection*{Exercises}

\begin{ex}\label{ex:wf-rigid}
  Prove that a well-founded $\sig$-labeled tree has no nonidentity automorphisms.
  Thus, the passage to isomorphism classes in the definition of $W\sig$ is ``categorically harmless''.
\end{ex}

\begin{ex}\label{ex:natw}
  Exhibit a signature $\sig$ such that $W\sig \cong \dN$ and \cref{thm:tree-ind} reduces to ordinary mathematical induction.
\end{ex}


\section{Indexed trees}
\label{sec:indexed-trees}

The signatures and algebras in \cref{sec:trees} have only one underlying set, or \emph{sort}, but sometimes algebraic structures have more than one sort.
As a simple example, we could consider a set together with a group acting on that set to be a single algebraic structure; then the group and the set are two sorts.

Categories could be regarded as having two sets, namely objects and arrows; but it is generally better to treat them differently.
Specifically, for a fixed set $\cO$, we regard categories with object set \cO as an algebraic structure whose set of sorts is $\cO\times \cO$.
Thus each hom-set is a separate sort, and each triple $A,B,C$ gives a different binary composition operation
\[ \circ_{A,B,C} : (\map(B,C),\map(A,B)) \to \map(A,C) \]
This may seem a little odd, but as we will see it makes sense.

To deal with multi-sorted algebraic structures in general, we augment our signatures with a set $\sig_0$ of \textbf{sorts} together with, for each operation $m\in\sig_1$, an \textbf{output sort} $c_m\in\sig_0$ and also a list of \textbf{input sorts} $d_{m,1},\dots,d_{m,\ay(m)}$.
For brevity we write such an operation as $m:(d_{m,1},\dots,d_{m,\ay(m)}) \to c_m$.
From now on we call these \emph{multi-sorted signatures} simply \textbf{signatures}; the simpler signatures of \cref{sec:trees} we re-christen \textbf{one-sorted signatures}.
(In fact, a multi-sorted signature is essentially the same as a ``multigraph'', \cref{defn:multigraph}.)

For a multi-sorted signature $\sig$, a $\sig$-\textbf{algebra} is a $\sig_0$-indexed family of sets $\{A_i\}_{i\in\sig_0}$ together with for each $m\in\sig_1$ a function $A_{d_{m,1}}\times \cdots \times A_{d_{m,\ay(m)}} \to A_{c_m}$.
For instance, if $\sig_0 = \{g,s\}$ and $\sig_1=\{m,t\}$ with
$m : (g,g) \to g$ and $t : (g,s) \to s$,
then an indexed algebra consists of two sets $A_g$ and $A_s$, a binary operation on $A_g$, and an action of $A_g$ on $A_s$.

Similarly, we define a $\sig$-\textbf{labeled tree} as before, with the additional requirement that if $x$ is the $k^{\mathrm{th}}$ child of $y$, and $x$ is labeled by $m\in\sig_1$ while $y$ is labeled by $p\in\sig_1$, then $c_m = d_{p,k}$.
For each $i\in\sig_0$, let $W\sig_i$ be the set of isomorphism classes of $\sig$-labeled trees for which the output sort of the root is $i$.
Then $\{W\sig_i\}_{i\in\sig_0}$ has a similar tautological $\sig$-algebra structure, and is the initial one.

\subsection*{Exercises}

\begin{ex}\label{ex:multi-sorted-W}
  Prove that $\{W\sig_i\}_{i\in\sig_0}$ is the initial $\sig$-algebra.
\end{ex}

\section{Free algebras with axioms}
\label{sec:axioms}

Of course, most algebraic structures of interest contain axioms as well as operations; for instance, multiplication in a group or monoid must be associative and unital.
The free monoid on a set $X$ is naturally regarded as a quotient of the free pointed magma on $X$ that forces associativity and unitality to hold.
It turns out that we can construct free algebras of this sort quite generally by defining an equivalence relation \emph{as another indexed free algebra}.

Making this completely precise in general is a bit technical, so we will begin with a concrete example.
Suppose we want to generate the free semigroup on a set $X$.
Let $\F\bMag X$ denote the free magma on $X$, constructed as in \cref{sec:trees}.
(A magma is a set with a single binary operation; a semigroup is a magma whose operation is associative.)

Now define a signature $\sig^{\equivsym}$ with $\sig^{\equivsym}_0 = \F\bMag X\times \F\bMag X$ and the following operations.
\begin{itemize}
\item For each $x\in \F\bMag X$, a nullary operation $() \to (x,x)$.
\item For each $x,y\in \F\bMag X$, a unary operation $((x,y)) \to (y,x)$.
\item For each $x,y,z\in \F\bMag X$, a binary operation $((x,y),(y,z)) \to (x,z)$.
\item For each $x,y,z,w\in \F\bMag X$, a binary operation \[((x,y),(z,w)) \to (x\cdot z,y\cdot w),\] where $\cdot$ denotes the binary magma operation on $\F\bMag X$.
\item For each $x,y,z\in \F\bMag X$, a nullary operation \[() \to (x\cdot (y\cdot z),(x\cdot y)\cdot z).\]
\end{itemize}
An algebra for this signature is an $(\F\bMag X\times \F\bMag X)$-indexed family of sets $R(x,y)$ equipped with elements and operations
\begin{align*}
  e_x &\in R(x,x)\\
  R(x,y) &\to R(y,x)\\
  R(x,y)\times R(y,z) &\to R(x,z)\\
  R(x,y) \times R(z,w) &\to R(x\cdot z,y\cdot w)\\
  a_{x,y,z} &\in R(x\cdot (y\cdot z),(x\cdot y)\cdot z)
\end{align*}
Now for any such $R$, ``$R(x,y)$ is nonempty'' is a binary \emph{relation} on $\F\bMag X$, which we abusively denote also by $R(x,y)$.
The above elements and operations imply that this is an equivalence relation that is a congruence for the magma operation and moreover relates $x\cdot (y\cdot z)$ to $(x\cdot y)\cdot z$ for all $x,y,z$.
And conversely, if we have any such binary relation $\sim$, we can construct an indexed algebra $R$ by setting $R(x,y) = \unit$ if $x\sim y$ and $R(x,y)=\emptyset$ otherwise.

Let $\equivsym$ denote the binary relation obtained as above from nonemptiness of the \emph{initial} algebra for this indexed signature.

\begin{thm}\label{thm:free-monoid}
  The quotient of $\F\bMag X$ by $\equivsym$ is the free semigroup generated by $X$.
\end{thm}
\begin{proof}
  First we show that it is a semigroup.
  Given $u,v\in\F\bMag X/\equivsym$, choose representatives $x,y\in\F\bMag X$ for them, and let $u\cdot v$ be the equivalence class of $x\cdot y$.
  Since $\equiv$ is a congruence for the magma operation, the result is independent of the choice of representatives; thus $\F\bMag X/\equivsym$ is a magma.
  Now given $u,v,w \in \F\bMag X/\equivsym$, choose representatives $x,y,z$; then since $x\cdot (y\cdot z)\equiv (x\cdot y)\cdot z$, we have $u\cdot (v\cdot w) =  (u\cdot v)\cdot w$.
  Thus $\F\bMag X/\equivsym$ is a semigroup

  Now let $M$ be any other semigroup and $\psi:X\to M$ a map.
  Since $M$ is in particular a magma, we have a unique induced magma morphism $\phi : \F\bMag X\to M$.
  Define a binary relation $R$ on $\F\bMag X$ by saying that $R(x,y)$ means $\phi(x)=\phi(y)$.
  Since $\phi$ is a magma morphism and $M$ is a semigroup, $R$ can be regarded as an algebra for the above indexed signature.
  Thus it admits a map from the initial such algebra.
  Hence, if $x\equiv y$, then $R(x,y)$, i.e.\ $\phi(x)=\phi(y)$; so $\phi$ factors through $\F\bMag X/\equivsym$.
  It is straightforward to check that this factorization is a semigroup morphism and is the unique such extending $\psi$.
\end{proof}

In the general case, we proceed as follows.
Suppose $\sig$ is a (multi-sorted) signature and we have additionally a set $\axes$ of \textbf{axioms}, each of which is a pair $(a,b)$ of elements of the free algebra $W\sig[V]_i$ for some $i\in\sig_0$ and some finite set $V$.
Then for any $\sig$-algebra $A$, any axiom $a,b\in W\sig[V]_i$, and any function $g:V\to A$ (picking out some finite set of elements of $A$), we have an induced $\sig$-algebra map $\gbar:W\sig[V]\to A$.
We define a $(\sig,\axes)$-\textbf{algebra} to be a $\sig$-algebra $A$ such that $\gbar(a)=\gbar(b)$ for any $(a,b)\in\axes$ and $g:V\to A$.

For instance, associativity in a magma is represented by the axiom
\[ \left(
  \vcenter{\xymatrix@-1.5pc{ & m \ar@{-}[dl] \ar@{-}[dr] \\ x && m \ar@{-}[dl] \ar@{-}[dr] \\ & y && z }},
  \vcenter{\xymatrix@-1.5pc{ && m \ar@{-}[dl] \ar@{-}[dr] \\ & m \ar@{-}[dl] \ar@{-}[dr] && z \\ x && y }}
\right)
\in W\sig[\{x,y,z\}]
\]
The $(\sig,\axes)$-algebras in this case are exactly semigroups.

Now, given a set $X$, we define a signature $\sig^{\equivsym}$ with
\[\sig^{\equivsym}_0 = \setof{(i,x,y) | i\in \sig_0; x,y\in W\sig[X]_i}\]
and the following operations:
\begin{itemize}
\item For each $x\in W\sig[X]_i$, a nullary operation $() \to (i,x,x)$.
\item For each $x,y\in W\sig[X]_i$, a unary operation $((i,x,y)) \to (i,y,x)$.
\item For each $x,y,z\in W\sig[X]_i$, a binary operation $((i,x,y),(i,y,z)) \to (i,x,z)$.
\item For each operation $m:(d_{m,1},\dots,d_{m,\ay(m)}) \to c_m$ in $\sig$, and each collection of pairs of elements $x_k,y_k \in W\sig[X]_{d_{m,k}}$ for $1\le k\le \ay(m)$, an operation
  \begin{multline*}
    ((d_{m,1},x_1,y_1),\dots,(d_{m,\ay(m)},x_{\ay(m)},y_{\ay(m)})) \\\too
    (c_m, \act m(x_1,\dots,x_{\ay(m)}), \act m(y_1,\dots,y_{\ay(m)})).
  \end{multline*}
\item For each axiom $a,b\in W\sig[V]_i$ in $\axes$ and each function $g:V\to W\sig[X]$ with unique extension $\gbar:W\sig[V]\to W\sig[X]$, a nullary operation
  \[ () \to (i,\gbar(a),\gbar(b)). \]
\end{itemize}
Let $\equivsym_i$ be the binary relation on $W\sig[X]_i$ defined by $a\equiv_i b$ if the sort $(i,a,b)$ is nonempty in the initial $\sig^{\equivsym}$-algebra.

\begin{thm}\label{thm:tree-quotient}
  Each $\equivsym_i$ is an equivalence relation and a congruence for the $\sig$-algebra structure, and the quotients $W\sig[X]_i/\equivsym_i$ form the free $(\sig,\axes)$-algebra.\qed
\end{thm}

As in \cref{sec:trees}, we will usually think of this theorem slightly differently: to define a family of maps $f_i:W\sig[X]_i/\equivsym_i\to A_i$, it suffices to define each $f_{c_m}(\act m(t_1,\dots,t_{\ay(m)}))$ assuming recursively that $f_{d_{m,1}}(t_1),\dots, f_{d_{m,\ay(m)}}(t_{\ay(m)})$ have been defined, and also to check that for any axiom $(a,b)\in W\sig[V]_i$ and $g:V\to W\sig[X]_i$ we have $f_i(\gbar(a))=f_i(\gbar(b))$.

\subsection*{Exercises}

\begin{ex}\label{ex:tree-quotient}
  Prove \cref{thm:tree-quotient}.
\end{ex}

\begin{ex}\label{ex:infop-ac}
  Why is the axiom of choice required to generalize \cref{thm:tree-quotient} to the case of infinitary operations?
\end{ex}


\section{Rules and deductive systems}
\label{sec:rules}

The basic machinery of type theory is an iteration and reformulation of the preceding sections in different language, simultaneously introducing convenient notations.

We consider a sequence of signatures $\sig^{(1)},\sig^{(2)},\dots,\sig^{(n)}$ for which the sorts of $\sig^{(k)}$ are defined in terms of the initial algebras $W\sig^{(j)}$ for the previous signatures $j<k$.
For instance, we might have $\sig^{(2)}_0 = W\sig^{(1)} \times W\sig^{(1)}$.
A particularly important special case is when $\sig^{(k)}$ is $(\sig^{(j)})^{\equivsym}$ for some $j<k$ and some set of axioms, as in \cref{sec:axioms}.

Each sort in one of the signatures $\sig^{(k)}$ is called a \textbf{judgment}.
We write $\cJ$ for a generic judgment, but we use more specific and congenial notation in particular cases, such as:
\begin{itemize}
\item When categories with object set \cO are regarded as an $(\cO\times\cO)$-sorted theory as mentioned in \cref{sec:indexed-trees}, the sort $(A,B)$ is usually written $A\types B$.
  This signature (with an $\equivsym$ on top of it) corresponds to the cut-ful type theory for categories from \cref{sec:category-cutful}.
  The cut-free type theory for categories has different operations but the same sorts, and uses the same notation.
\item If $\sig^{(1)}$ is a one-sorted signature regarded as describing the \emph{objects} of some categorical structure, then we denote its sort by ``$\mathsf{type}$''.
  We generally then have $\sig^{(2)}_0 = W\sig^{(1)} \times W\sig^{(1)}$ (for a unary type theory), with sorts again written as $A\types B$, where now $A$ and $B$ are elements of the initial $\sig^{(1)}$-algebra rather than elements of a fixed set \cO.
\item The multicategorical and polycategorical theories of \cref{chap:simple,chap:polycats} use a similar notation $\Gamma\types\Delta$ for sorts $(\Gamma,\Delta)$ where one or both of $\Gamma$ and $\Delta$ is a list rather than a single item.
\item If $\sig^{(k)}=(\sig^{(j)})^{\equivsym}$, then its sort $(\cJ,x,y)$ is usually written $x\equiv y : \cJ$.
\end{itemize}

In general, each operation $m:(\cJ_1,\dots,\cJ_n) \to \cJ'$ in one of the signatures $\sig^{(k)}$ is called a \textbf{rule}, and written
\[ \inferrule*[right=$m$]{\cJ_1 \\ \cdots \\ \cJ_n}{\cJ'}. \]
The input judgments $\cJ_1,\dots,\cJ_n$ of a rule are called its \textbf{premises}, and the output judgment $\cJ'$ is called its \textbf{conclusion}.

Finally, each element of $W\sig^{(k)}$ is called a \textbf{derivation} (sometimes a derivation \emph{of} its root judgment) and written by placing rules on top of each other to form its tree structure.
For instance, if \cJ denotes the single sort of the signature for semigroups, then the associativity axiom of a monoid is
\[
\inferrule*[Right=$m$]{\inferrule*[Right=$x$]{\qquad}{\cJ} \\
  \inferrule*[Right=$m$]{\inferrule*[Right=$y$]{\qquad}{\cJ}\\\inferrule*[Right=$z$]{\qquad}{\cJ}}{\cJ}}{\cJ}
\qquad \equiv \qquad
\inferrule*[Right=$m$]{\inferrule*[Right=$m$]{\inferrule*[Right=$x$]{\qquad}{\cJ}\\\inferrule*[Right=$y$]{\qquad}{\cJ}}{\cJ} \\
  \inferrule*[Right=$z$]{\qquad}{\cJ}}{\cJ}
\]
Note the rules with empty premises, corresponding to nullary operations.
Similarly, for the cut-ful type theory for categories, associativity is the collection of axioms (one for each $A,B,C\in\cO$)
\begin{multline*}
\inferrule*[right=$\circ_{A,B,D}$]{\inferrule*[Right=$x$]{\qquad}{A\types B} \\
  \inferrule*[Right=$\circ_{B,C,D}$]{\inferrule*[Right=$y$]{\qquad}{B\types C}\\\inferrule*[Right=$z$]{\qquad}{C\types D}}{B\types D}}{A\types D}
\\ \equiv \qquad
\inferrule*[right=$\circ_{A,C,D}$]{\inferrule*[right=$\circ_{A,B,C}$]{\inferrule*[Right=$x$]{\qquad}{A\types B}\\\inferrule*[Right=$y$]{\qquad}{B\types C}}{A\types C} \\
  \inferrule*[Right=$z$]{\qquad}{C\types D}}{A\types D}
\end{multline*}

The whole sequence of signatures $\sig^{(1)},\sig^{(2)},\dots,\sig^{(n)}$ is called a \textbf{deductive system}.
Thus, for instance, the signature $\sig[X]$ for semigroups under a fixed set $X$, together with the axiom-signature for monoids under $X$ on top of it, form a single deductive system.
Some deductive systems (probably not all) deserve to be called \emph{type theories}; but we will not attempt to give any definition of this class except by the examples we consider (throughout the entire book).

\begin{rmk}
  To be sure, not all type theories fit exactly into the picture presented here.
  In particular, \emph{dependent} type theories break the clean ``stratification'' of a deductive system $\sig^{(1)},\sig^{(2)},\dots,\sig^{(n)}$, since in the judgment $\types A\type$ the type $A$ can now contain terms from the ``higher level'' judgment $\Gamma\types M:B$.
  Thus the whole system must be defined by one big mutual induction (in type-theoretic lingo it is an ``inductive-inductive definition'').
  The general idea is similar, however.
\end{rmk}


\section{Terms}
\label{sec:terms}

Since the judgments in each signature $\sig^{(k)}$ in a deductive system are defined in terms of the \emph{elements} of $W\sig^{(j)}$ for $j<k$, and the latter are rooted trees, the notation would rapidly get unwieldy if each $\cJ$ in a rule contained within it some number of derivation trees.
Thus, we generally represent derivations by \textbf{terms}, which are a more concise syntax containing enough information to reconstruct the derivation.
For instance, the expressions $x\cdot (y\cdot z)$ and $(x\cdot y)\cdot z$ for the two sides of associativity are terms, in which we have represented the rule $m$ by an infix operation ``$\cdot$''.

If $M$ is a term representing a derivation of the judgment \cJ, we generally write $M:\cJ$.
(A notable exception is that if \cJ is the sort of a one-sorted $\sig^{(1)}$ presenting the objects of a category, as mentioned above, we usually write ``$A\type$'' or ``$\types A\type$'' rather than ``$A:\mathsf{type}$''.)
We describe a syntax for terms by annotating the rules of a deductive system with terms, so that for instance the multiplication of a semigroup would be
\[ \inferrule*[Right=$m$]{M:\cJ \\ N:\cJ}{M\cdot N :\cJ} \]
Here $M$ and $N$ are ``metavariables'' standing for terms, indicating that whatever terms we have representing two derivations of \cJ, we represent their combination by $m$ by juxtaposing them with an infix dot.
(We always assume that parentheses are added as necessary to ensure correct grouping.)

For purposes of this discussion, ``terms with variables from the context'' such as $x:A\types M:B$ can be regarded as merely a variant notation of something like $x.M : (A\types B)$.
Thus we still have a single thing (namely $x.M$) that represents the entire derivation, even though we generally apply the word ``term'' only to part of this thing (namely $M$).
Similarly, an equality judgment like $x:A \types M\equiv N:B$ is shorthand for $(x.M)\equiv (x.N) : (A\types B)$.
There is one actual difference here in that we generally consider terms of this form modulo ``$\alpha$-equivalence'', i.e.\ the consistent renaming of variables.
For now, let us assume that we know what that means; in \cref{sec:alpha} we will explain it precisely.

There is no unique way to assign terms to a deductive system; all that is necessary is to describe some kind of syntax from which a derivation tree can be algorithmically extracted.
When a human mathematician reads an expression such as $x\cdot (y\cdot z)$, they generally mentally organize it as a tree without really thinking about it: here the first $\cdot$, being the ``outer'' operation, is the root, with children $x$ and $y\cdot z$, and the latter decomposes further into another $\cdot$ node with children $y$ and $z$.
This ``internal syntax tree'' has exactly the same shape as the intended derivation tree.
An alternative reading where the second $\cdot$ is the root with children ``$x\cdot (y$'' and ``$z)$'' is ruled out by our intuitive understanding of the meaning of parentheses.
When a computer reads such an expression it likewise constructs an internal tree representation, but the programmer has to explicitly instruct it how to do so; this is called \textbf{parsing}.

If we are given a putative term claiming to represent a derivation of some judgment, then after parsing there is a further step of verifying that the ``parse tree'' indeed corresponds to a valid derivation tree.
This is called \textbf{type-checking}.
Technically it could be done at the same time as parsing, but both human and electronic mathematicians generally separate them.
Thus the parse tree is a sort of ``raw abstract syntax'' that knows how operations are grouped but not whether the operations actually mean anything yet.

We will not say anything more about parsing; we trust the human reader to do it unconciously and the programmer to have good algorithms for it.
Thus, in our formal description of terms, the mathematical objects we call ``terms'' will be representations of parse trees.
And as trees, they will be elements of some other free algebra --- but a simpler one than the one whose derivations we are using them to represent.
For instance, for the cut-ful type theory of categories under \cG, whose judgments are of the form $A\types B$ for $A,B\in\cG_0$ (and in particular there are $\cG_0\times \cG_0$ of them), the terms will be elements of a \emph{one-sorted} free algebra with a nullary operation $\id_A$ and a binary operation $\circ_A$ for each $A\in\cG_0$.
Thus this free algebra contains many ``ill-typed'' terms such as $g\circ_B \id_A$ where $g\in\cG(C,D)$; the goal of type-checking is to discard these undesirables.
(For technical reasons, rather than a single set of terms as here, in the general case we will allow each judgment of our intended theory to be assigned a different set of ``potential terms''; see below.)

Now in practice, the input to type-checking is usually a parsed term \emph{together with} a putative type for that term, and so the term notations only need to contain enough information to reconstruct the derivation tree when supplemented with the latter.
For instance, we have noted that the cut-ful type theory for categories technically involves a different composition operation $\circ_{A,B,C}$ for each triple of objects, so that terms would technically have to be written as $h\circ_{A,C,D} (g\circ_{A,B,C} f)$.
However, if we are given a term whose outer operation is a composition and that claims to represent a derivation of a judgment $A\types D$, then the composition must be $\circ_{A,?,D}$.
Thus in general it suffices to indicate the object being composed over, as in $h\circ_C (g\circ_B f)$.

\begin{rmk}
In many cases we can omit further information because it can be inferred from context; for instance, if we know that $h:C\to D$ then a term of the form ``$h\circ (-)$'' can only mean ``$h\circ_C (-)$''.
Human mathematicians omit information informally and unsystematically, and we have done the same throughout the book.
The implementors of electronic mathematicians have elaborate and precise algorithms for ``inferring from context'' enabling the omission of information, but most of these are far beyond our scope.
% \footnote{One that is worth mentioning, however, is that a canonical/atomic calculus like that of \cref{sec:atomcan-catprod} can be type-checked in a ``bidirectional'' way. canonical terms $M\can A$ are type-checked as usual with both $M$ and $A$ being given, while atomic terms $M\atom A$ instead ``type-synthesize'': only $M$ is given and its type $A$ is deduced.
%   Paradoxically, this sometimes enables the omission of more information.
%   For instance, if when type-checking a function application $f(a)\can B$ the term $f$ can synthesize its type $A\to B$ (note that it is atomic since application is an elimination rule for a negative type), then we can extract the type $A$ at which we have to type-check the argument $a\can A$; thus the notation for function application doesn't need to notate the type $A$.}
\end{rmk}

With type-checking (and also ``proof search'') in mind, type theorists tend to read the rules of a deductive system ``bottom-up''.
That is, instead of thinking of a rule
\begin{mathpar}
  \inferrule*{\cJ_1 \\ \cJ_2}{\cJ}
\end{mathpar}
as meaning ``if we have $\cJ_1$ and $\cJ_2$ we can deduce $\cJ$'', they instead think ``if we want to deduce $\cJ$, it suffices to have $\cJ_1$ and $\cJ_2$''.
This is the direction that a type-checking algorithm applies the rule: given a parsed term $M$ and a putative judgment $\cJ$, the rule tells us how to break down the job of checking that $M:\cJ$ into simpler type-checking tasks that can be done recursively.\footnote{However, some more advanced theories are type-checked in a ``bidirectional'' way, with some judgments being read upwards in this way and others being read downwards as ``type synthesis'', where only the term is given and the type is inferred by the algorithm.}
It is also the direction that the rule is often applied when \emph{searching} for a derivation of \cJ, by the same sort of recursive procedure.

With all of this in mind, we make the following formal definition.

\begin{defn}\label{defn:terms}
  Let $\sig$ be a signature; a \textbf{term system} for $\sig$ is a $\sig$-algebra \dT, whose elements are called (potentially ill-typed) \textbf{terms}, such that
  \begin{enumerate}
  \item For any judgment $c\in\sig_0$ and term $t\in \dT_c$, there is at most one rule $m:(d_1,\dots,d_n)\to c$ and terms $s_j\in \dT_{d_j}$ such that $t = \act m(s_1,\dots,s_n)$.\label{item:terms-uniq}
  \item If we define a relation $s\preceeds t$ on $\bigsqcup_i \dT_i$ to hold just when $t = \act m(s_1,\dots,s_n)$ and $s=s_j$ for some $j$, then $\preceeds$ is well-founded: there are no infinite chains $t_1 \succ t_2 \succ t_3 \succ \cdots$.\label{item:term-wf}
  \end{enumerate}
\end{defn}

Since a term system \dT is a $\sig$-algebra, there is a unique $\sig$-algebra morphism $W\sig \to \dT$.
This is the function that assigns to each derivation a unique representing term.
Axiom~\ref{item:terms-uniq} above ensures that a derivation is determined by its term:

\begin{lem}\label{thm:term-system}
  If \dT is a term system, then the unique $\sig$-algebra morphism $W\sig \to \dT$ is injective.
\end{lem}
\begin{proof}
  Let $x,y\in W\sig_i$ have the same image in $\dT_i$.
  By axiom~\ref{item:terms-uniq}, and the fact that $W\sig \to \dT$ is a $\sig$-algebra morphism, we must have $x=\act m(x_1,\dots,x_n)$ and $y=\act m(y_1,\dots,y_n)$ for the same operation $m$ and each pair $x_j,y_j$ having the same image in $\dT$.
  By structural induction, therefore, each $x_j=y_j$, and thus $x=y$.
\end{proof}

However, the converse of \cref{thm:term-system} does not hold.
Indeed, its conclusion does not even imply axiom~\ref{item:terms-uniq} (which is all that was used in its proof): the former is only about ``globally'' well-typed terms, while the latter is a ``local'' condition that says something even about ill-typed terms.

The reason for the extra strength of~\ref{item:terms-uniq}, and for condition~\ref{item:term-wf}, is to make type-checking a ``deterministic terminating recursive algorithm'', as follows.
Given a term $t$, we check whether it is of the form $\act m(s_1,\dots,s_n)$.
If so, then by~\ref{item:terms-uniq} $m$ and the $s_j$'s are uniquely determined, and we can recursively consider each $s_j$.
If the answer is ever no, then the term $t$ is ill-typed.
Otherwise, axiom~\ref{item:term-wf} ensures that the algorithm must terminate (by bottoming out at nullary rules), and we have now constructed a derivation (the tree of rules $m$) represented by the term $t$.

In the main text, we generally stated our ``terms are derivations'' lemmas in the simple form of ``if a term judgment is derivable, then it has a unique derivation.''
As stated this is only the conclusion of \cref{thm:term-system}, but in all cases our proofs had the simple inductive form that actually establishes all of \cref{defn:terms}.

Of course, for this to actually be an algorithm in the computer science sense, the test for whether $t=\act m(s_1,\dots,s_n)$ would have to be ``computable''.
Making that precise is far beyond our current scope, but it may be worth mentioning that it generally holds because \dT is constructed using an initial algebra for some other signature, and initial algebras are very computable (they are ``abstract datatypes'').
Such a construction of \dT generally also ensures axiom~\ref{item:term-wf} immediately.

Notationally, we regard the common ``annotation of rules'' as specifying a signature along with a term syntax for it.
For instance, when we annotate the composition rule in the cut-ful type theory of categories
\[ \inferrule{A\types B \\ B\types C}{A\types C} \]
by terms to get
\[ \inferrule{\phi:(A\types B) \\ \psi:(B\types C)}{\psi\circ_B \phi : (A\types C)} \]
we mean that if $m$ is this rule, then the corresponding operation $\act m$ on $\dT$ is given by the operation $(-\circ_B -)$.
Technically, this requires us to specify in advance the set (or sets) \dT of terms, so that the annotated rules are describing which \emph{previously existing operations} on \dT we are using to represent each rule.
However, since in most cases \dT is a free algebra for a different signature with an operation corresponding directly to each rule in $\sig$ (though not in a one-to-one manner), we can generally omit this preliminary step and assume that \dT is freely generated as necessary by the operations named in the annotations.

% The most notable example is dependent type theories, which among other things have terms that do not exactly represent derivations because of the ``coercion'' rule:
% \begin{mathpar}
%   \inferrule*{M:A \\ A\equiv B}{M:B}
% \end{mathpar}

\section{Variable binding and $\alpha$-equivalence}
\label{sec:alpha}

Finally, we come to the vexing question of $\alpha$-equivalence.
We could wave our hands at it by claiming to use de Bruijn variables everywhere, but this would be a bit dishonest.
As is evident, we actually do use named variables all over the place, so it behooves us to say something about what they mean.
In this section we will describe a general way to construct ``terms with binders'' such as $\match$ and $\lambda$ and define a notion of $\alpha$-equivalence.
There are many ways to do this; our approach follows~\cite{gp:asib,gp:aswvb,pg:freshml} (see also~\cite{crole:alpha}).

Let \dA be a fixed infinite set (usually countable), whose elements we call \textbf{variables}.
Let $\sig$ be a signature, one-sorted for simplicity, together with injective functions $v,b:\dA \to \sig_1$ such that $\ay(v(x))=0$ and $\ay(b(x))=1$ for all $x\in\dA$.
What we have in mind is that the initial $\sig$-algebra will supply the set of terms in a term syntax for some other signature, with the operations of $\sig$ corresponding to the term notations for the rules in that other signature.

The inclusion $v$ simply says that variables can occur in terms, while the operation $b(x)$ is intended to ``bind'' the variable $x$ in its argument; usually $b(x)(M)$ is written $x.M$.
When term notations bind variables, their corresponding operations will put a specially named $\sig$-operation together with one or more uses of $b$.
For instance, when describing the terms in the unary type theory for categories with coproducts, there will be operations $\acase AB$ of arity 3, which we combine with two uses of $b$ to represent the terms annotating $\plusE$:
\[\acase AB(M,u.P,v.Q) = \acase AB(M,b(u)(P),b(v)(Q)).\]

As usual, let $W\sig$ be the initial $\sig$-algebra; and let $\nAut(\dA)$ be the group of automorphisms (permutations) of the set \dA.
We write the action of $\sigma\in\nAut(\dA)$ on $x\in \dA$ by $x^\sigma$.
Now we define, by recursion, an action of $\nAut(\dA)$ on $W\sig$ as follows:
\begin{align*}
  \sigma \cdot \act{v(x)} &= \act{v(x^\sigma)}\\
  \sigma \cdot \act{b(x)}(M) &= \act{b(x^\sigma)}(\sigma\cdot M)
\end{align*}
with $\sigma\cdot M$ defined recursively in the latter.
In all other cases, $\sigma \cdot (-)$ simply recurses into all subtrees.
It is easy to show that this is a group action.

Because all operations in $\sig$ have finite arity\footnote{If $\sig$ were allowed to contain infinitary operations, then to make this work, the cardinality of \dA would have to be of cofinality greater than any of their arities.} and all trees in $W\sig$ are well-founded, only finitely many variables can occur in any such tree (either through $v$ or $b$).
So, since \dA is infinite, for any $M\in W\sig$ there is some variable $z\in \dA$ that does not occur in $M$.
We call such a $z$ \textbf{fresh} (relative to $M$) and write $z\notin M$.

We now define $\alpha$-equivalence $\equiv$ on $W\sig$, by defining a new signature $\sig^\equiv$ similar to how we did it in \cref{sec:axioms}.
We include operations making $\equiv$ a congruence for all operations of $\sig$ except $b$.
In the case of $v$, this means we have ``reflexivity at variables'' $v(x)\equiv v(x)$.
We also include one further operation that in rule form looks like this:
\begin{equation}
  \inferrule{z\notin M \\ z\notin N \\ z\neq x \\ z \neq y \\ (zx)\cdot M \equiv (zy)\cdot N}
  {b(x)(M) \equiv b(y)(N)}\label{eq:alpha-gen}
\end{equation}
Here $(zx)$ and $(zy)$ denote the transposition permutations that swap $z$ with $x$ and $z$ with $y$, respectively.
Since $z$ does not occur in $M$ and $N$, the permutation actions $(zx)\cdot M$ and $(zy)\cdot N$ amount to replacing all occurences of $x$ in $M$ and $y$ in $N$ (even bound ones) by $z$.
The rule then says that if these two results are $\alpha$-equivalent, then so are the terms $x.M$ and $y.N$ with new bound variables.

For instance, $x.x$ and $y.y$ are $\alpha$-equivalent because $(zx)\cdot x = z$ and $(zy)\cdot y = z$.
We also have $x.(x.x) \equiv x.(y.y)$ because $(zx)\cdot (x.x) = (z.z)$ and $(zy)\cdot (y.y) = (z.z)$ as well, so the inner bound $x$ really does ``shadow'' the outer one, making the latter invisible even though it has the same name.
But neither of these is equivalent to $x.(y.x)$, since $(zx)\cdot (y.x) = (y.z)$.

Note also that if $M\in W\sig$, then $x.M$ is $\alpha$-equivalent to $y.((yx)\cdot M)$ for any variable $y$ not occurring in $M$, since if neither $y$ nor $z$ occur in $M$ then
\[(zy)\cdot (yx)\cdot M = (yx)\cdot (zx)\cdot M = (zx)\cdot M.\]
Thus, we can always replace a bound variable by any another fresh variable.

Unlike in \cref{sec:axioms}, we do not explicitly include operations making $\equiv$ an equivalence relation.
However, we can nevertheless prove that it is; this is itself a sort of cut-admissibility.

\begin{lem}\label{thm:alpha-adm}
  $\alpha$-equivalence $\equiv$, as defined above, has the following properties:
  \begin{enumerate}
  \item Equivariance: if $M\equiv N$, then $\sigma\cdot M \equiv \sigma \cdot N$ for any $\sigma\in \nAut(\dA)$.\label{item:alpha-eqvadm}
  \item Congruence for binding: if $M\equiv N$, then $x.M\equiv x.N$.\label{item:alpha-bindadm}
  \item Rule~\eqref{eq:alpha-gen} is invertible: if $x.M\equiv y.N$, then $(zx)\cdot M \equiv (zy)\cdot N$ for some fresh $z$.\label{item:alpha-gen-inv}
  \item Reflexivity: $M\equiv M$ for any $M\in W\sig$.\label{item:alpha-refl}
  \item Symmetry: if $M\equiv N$ then $N\equiv M$.\label{item:alpha-symm}
  \item Transitivity: if $M\equiv N$ and $N\equiv P$, then $M\equiv P$.\label{item:alpha-trans}
  \item Bound variables can be altered freely: if $z\notin M$ then $x.M \equiv z.((zx)\cdot M)$.\label{item:alpha-rename}
  \end{enumerate}
\end{lem}
\begin{proof}
  Perhaps surprisingly, the tricky and important one is~\ref{item:alpha-eqvadm}.
  Of course, the proof is by induction on the derivation of $M\equiv N$, and all the congruence rules are immediate, so it remains to deal with~\eqref{eq:alpha-gen}.
  That is, suppose $x.M\equiv y.N$ is obtained from $(zx)\cdot M \equiv (zy)\cdot N$, and let $\sigma\in\nAut(\dA)$.
  Now $\sigma\cdot(x.M) = x^\sigma.(\sigma\cdot M)$ and similarly for $N$, so to conclude $\sigma\cdot(x.M) \equiv \sigma\cdot(y.N)$ it will suffice to show $(wx^\sigma)\cdot \sigma\cdot M \equiv (wy^\sigma)\cdot\sigma\cdot N$ for some fresh $w$.
  The obvious choice for $w$ is $z^\sigma$.
  Then if we let $\tau = (z^\sigma y^\sigma)\sigma(zx)\in\nAut(\dS)$, we have
  \begin{align*}
    (z^\sigma x^\sigma)\cdot \sigma\cdot M
    &= \tau \cdot (zx)\cdot M\\
    &\equiv \tau \cdot (zy)\cdot N\\
    &= (z^\sigma y^\sigma)\cdot\sigma\cdot N
  \end{align*}
  using the inductive hypothesis of equivariance for $(zx)\cdot M \equiv (zy)\cdot N$.

  Now~\ref{item:alpha-bindadm} is immediate, since $M\equiv N$ implies $(zx)\cdot M \equiv (zx)\cdot N$, whence~\eqref{eq:alpha-gen} gives $x.M\equiv x.N$.
  And~\ref{item:alpha-gen-inv} is clear since~\eqref{eq:alpha-gen} is the only rule that can produce an $\alpha$-equivalence between terms of the form $x.M$ and $y.N$ (since we did not include~\ref{item:alpha-bindadm} or~\ref{item:alpha-refl}--\ref{item:alpha-trans} as primitive).
  Combining the primitive congruence rules with~\ref{item:alpha-bindadm} yields straightforward inductive proofs of~\ref{item:alpha-refl} and~\ref{item:alpha-symm}.

  For~\ref{item:alpha-trans} we induct on both $M\equiv N$ and $N\equiv P$.
  By inspection of the form of $N$, they must both be derived from the same rule.
  If it is a primitive congruence rule, we just apply the inductive hypothesis to all the inputs and then congruence again.
  The interesting case is~\eqref{eq:alpha-gen}, where we have $(ux)\cdot M \equiv (uy)\cdot N$ and also $(vy)\cdot N\equiv (vz)\cdot P$ for potentially different variables $u$ and $v$, with $u$ fresh for $M,N,x,y$ and $v$ fresh for $N,P,y,z$.
  Since \dA is infinite there exists a variable $w$ that is fresh for all of $M,N,P,x,y,z$, and so we have
  \[ (wx)\cdot M = (wu)\cdot (ux)\cdot M \equiv (wu)\cdot (uy)\cdot N = (wy)\cdot N\]
  using~\ref{item:alpha-eqvadm}.
  Similarly, $(wy)\cdot N \equiv (wz)\cdot P$, so we ought to be able to conclude by the inductive hypothesis that $(wx)\cdot M\equiv (wz)\cdot P$ and so $x.M\equiv z.P$ by~\eqref{eq:alpha-gen}.
  However, this is not the usual structural inductive hypothesis, since the derivations of and $(wx)\cdot M \equiv (wy)\cdot N $ and $(wy)\cdot N \equiv (wz)\cdot P$ are produced by~\ref{item:alpha-eqvadm} and are not subtrees of our given derivations of $x.M\equiv y.N$ and $y.N\cdot z.P$.
  Instead we have to do something like assign a natural number ``height'' to all derivations, observe that~\ref{item:alpha-eqvadm} preserves the height of derivations, and then induct on height.

  Finally, for~\ref{item:alpha-rename} we choose a fresh $w$ and observe that $(wx)\cdot M = (wz)\cdot (zx)\cdot M$.
  Thus, by reflexivity~\ref{item:alpha-refl} we have $(wx)\cdot M \equiv (wz)\cdot (zx)\cdot M$ and hence by~\eqref{eq:alpha-gen} $x.M \equiv z.((zx)\cdot M)$.
\end{proof}

The quotient $W\sig/\equiv$ of this equivalence relation is, of course, our set of ``terms modulo $\alpha$-equivalence of bound variables''.
Since $\equiv$ is a congruence for all the operations of $\sig$, these all descend to the quotient, including (by \cref{thm:alpha-adm}\ref{item:alpha-bindadm}) variable binding; we also denote this operation by $x.M$ where now $M\in W\sig/\equiv$.

Our goal is to use this quotient as the term syntax for another signature.
In practice we will write terms as elements of $W\sig$ itself, but we regard them formally as representing their equivalence class.
We also usually want to restrict to some subsets of terms that have the right number of variables bound to represent the context.

For instance, in unary type theories (\cref{chap:unary}) we have said that a term judgment such as $x:A\types M:B$ can be read as $x:M: (A\types B)$.
We really do want this $x$ to be a \emph{bound} variable in the formal sense of this section, since derivations to determine unique terms we have to quotient by renaming the variables in the context as well.
That is, we represent ``free'' variables as variables that are bound ``on the outside''.
Thus, we should take our set \dT of terms to be the subset of $W\sig/\equiv$ consisting of terms having a variable binding outermost.
Similarly, in a simple type theory (\cref{chap:simple}) the terms for $\Gamma\types B$ should have $n$ variable bindings outermost, where $n$ is the length of $\Gamma$ (this is why in \cref{sec:terms} we allowed different judgments to have different sets of potential terms).

We then need to define operations on these sets \dT that represent the rules of our desired signature.
These will generally be constructed from basic operations in $\sig$ combined with one or more variable bindings.

Let us consider $\case$ from \cref{sec:catcoprod} as a paradigmatic example.
Since the rule $\plusE$ has three premises, what we have to give is an operation $\dT\times \dT\times \dT \to \dT$, where $\dT$ is the set of $\alpha$-equivalence classes of terms of the form $x.M$.
We have presumably included ``$\case$'' as a 3-ary operation in our term signature $\sig$, but this does not take account yet of the binding structure.
The inputs to our desired operation will be (given the above restriction defining $\dT$) of the form $x.M$, $u.P$, and $v.Q$.
The basic 3-ary operation in $\sig$ could give $\case(x.M,u.P,v.Q)$, but of course we want ``$x.\case(M,u.P,v.Q)$'' instead.

To define this, we first choose representatives for the equivalence classes of $x.M$, $u.P$, and $v.Q$.
By \cref{thm:alpha-adm}\ref{item:alpha-rename} we can do this so that $x$ does not appear in $u.P$ or $v.Q$ (which have only finitely many variables each).
Now we can write $x.\case(M,u.P,v.Q)$; but for this to define an operation $\dT\times \dT\times \dT \to \dT$ we have to check that it is independent of the chosen representatives.
For $u.P$ and $v.Q$ this is easy since $\equiv$ is a congruence for all operations of $\sig$, including $\case$.
And if $x.M\equiv y.N$, then by \cref{thm:alpha-adm}\ref{item:alpha-gen-inv} we have $(zx)\cdot M \equiv (zy)\cdot N$ for some $z$, which we may also take to not appear in $u.P$ or $v.Q$.
Thus, using the congruence rules and transitivity, we have
\begin{align*}
  x. \case(M,u.P,v.Q)
  &\equiv z.\case((zx)\cdot M,u.P,v.Q)\\
  &\equiv z.\case((zy)\cdot N,u.P,v.Q)\\
  &\equiv y.\case(N,u.P,v.Q).
\end{align*}

The same principle applies to all other term systems using variable binding.
Sometimes we also need to poke down into all the terms to ensure that certain variables in their context are disjoint or equal.
For instance, the term operation representing $\timesI$ takes as given $x.M$ and $y.N$, but its output has only one shared variable.
Thus we have to first note $x.M \equiv z.((zx)\cdot M)$ and $y.N \equiv z.((zy)\cdot N)$ for some $z$ that is fresh for both, and then write $z.\pair{(zx)\cdot M}{(zy)\cdot N}$ for the pairing term.
Based on these examples, we trust that the reader could formulate precise definitions of all the terms used in this book as operations on $\alpha$-equivalence classes.

Of course, in any particular case it is still (technically) necessary to prove that what we get \emph{is} a term system in the sense of \cref{defn:terms}.
Since \dT is a subset of an initial algebra, and our operations are built using at least one operation of that algebra, \ref{defn:terms}\ref{item:term-wf} is straightforward.
Finally, the proof of \ref{defn:terms}\ref{item:terms-uniq} essentially means checking that we chose the operations of the term signature to contain enough information to reconstruct a derivation step-by-step.
This is a formal version of what in the main text we called \emph{type-checking is possible} or \emph{terms are derivations}.
Note that since all our ``terms'' are actually $\alpha$-equivalence classes, we never have to prove anything about $\alpha$-equivalence.

% [TODO: Ought we to say something about substitution at the level of untyped terms?]


\bibliographystyle{alpha}
\bibliography{all}

\end{document}
