\documentclass{book}
\usepackage{mathpartir,cancel,cmll}
\newif\ifcref\creftrue
\input{decls}
\title{Categorical logic from a categorical point of view}
\author{}
\date{\today}
\makeatletter
\autodefs{\bSet\bPoset\bRelGr\bCat\bGr\bmSLat\ftype\bPrCat\bMCat\bMGr\bMPos\bRelMGr\bMonPos\bMonCat\bSMPos\bSMC}
\let\sect\section
\def\idfunc{\mathsf{id}}
\def\finsubset{\subset_{\textrm{fin}}}
% judgments
\let\types\vdash
\def\type{\;\ftype}
\newcommand{\pc}{\mathrel{\mathord{:}?}}
\newcommand{\atom}{\mathrel{\downarrow}}
\newcommand{\can}{\mathrel{\uparrow}}
\newcommand{\atomcan}{\ensuremath{\mathord{\downarrow\uparrow}}}
% substitution
\newcommand{\hsub}[1]{\llbracket #1\rrbracket}
% free functors
\newcommand{\F}[1]{\mathfrak{F}_{#1}}
% meet
\let\meet\wedge
\def\meetL{\mathord{\meet}L}
\def\meetR{\mathord{\meet}R}
\def\meetE{\mathord{\meet}E}
\def\meetI{\mathord{\meet}I}
% join
\let\join\vee
\def\joinL{\mathord{\join}L}
\def\joinR{\mathord{\join}R}
\def\joinE{\mathord{\join}E}
\def\joinI{\mathord{\join}I}
% unit
\def\unit{\mathbf{1}}
\def\ttt{\mathsf{tt}}
% product
\def\timesE{\ensuremath{\mathord{\times}E}}
\def\timesI{\ensuremath{\mathord{\times}I}}
\def\pair#1#2{\langle #1,#2\rangle}
% coproduct
\def\plusE{\mathord{+}E}
\def\plusI{\mathord{+}I}
\def\inl{\mathsf{inl}}
\def\inr{\mathsf{inr}}
\def\case{\mathsf{case}}
% empty
\def\emptyt{\mathbf{0}}
\def\abort{\mathsf{abort}}
% one
\def\one{\mathbf{1}}
\def\discard#1in{\mathsf{discard}\; #1 \; \mathsf{in} \;}
% tensor
\let\tensor\otimes
\def\tensorL{\mathord{\tensor}L}
\def\tensorR{\mathord{\tensor}R}
\def\tensorI{\mathord{\tensor}I}
\def\tensorE{\mathord{\tensor}E}
\let\bigtensor\bigotimes
\def\flet#1:{\mathsf{let}\;#1 \@ifnextchar:\@fletdoublecolon\@fletsinglecolon}
\def\fletp#1:{\mathsf{let}'\;#1 \@ifnextchar:\@fletdoublecolon\@fletsinglecolon}
\def\@fletdoublecolon:=#1in{\Coloneqq #1\;\mathsf{in}\;}
\def\@fletsinglecolon=#1in{\coloneqq #1\;\mathsf{in}\;}
\makeatother
\begin{document}
\maketitle
\setcounter{tocdepth}{1}
\tableofcontents

\setcounter{chapter}{-1}
\chapter{Introduction}
\label{chap:intro}


\section{Appetizer: inverses in group objects}
\label{sec:intro}

In this section we consider an extended example.
We do not expect the reader to understand it very deeply, but we hope it will give some motivation for what follows, as well as a taste of the power and flexibility of categorical logic as a tool for category theory.

Our example will consist of several varations on the following theorem:

\begin{thm}
  If a monoid has inverses (hence is a group), then those inverses are unique.
\end{thm}

When ``monoid'' and ``group'' have their usual meaning, namely sets equipped with structure, the proof is easy.
For any $x$, if $i(x)$ and $j(x)$ are both two-sided inverse of $x$, then we have
\[ i(x) = i(x) \cdot e = i(x) \cdot (x \cdot j(x)) = (i(x)\cdot x)\cdot j(x) = e\cdot j(x) = j(x) \]
However, the theorem is true much more generally than this.
We consider first the case of monoid/group objects in a category with products.
A \emph{monoid object} is an object $M$ together with maps $m:M\times M \to M$ and $e:1\to M$ satisfying associativity and unitality axioms:
\begin{equation}
  \vcenter{\xymatrix{
      M\times M\times M\ar[r]^-{1\times m}\ar[d]_{m\times 1} &
      M\times M\ar[d]^m\\
      M\times M\ar[r]_m &
      M
    }}
  \qquad
  \vcenter{\xymatrix{ M \ar[r]^-{(1,e)} \ar[dr]_{1} &
    M\times M \ar[d]_m & M \ar[l]_-{(e,1)} \ar[dl]^{1} \\
    & M }}
\end{equation}
An \emph{inverse operator} for a monoid object is a map $i:M\to M$ such that
\begin{equation}
  \vcenter{\xymatrix@C=1pc{& M\times M \ar[rr]^{i\times 1} && M\times M \ar[dr]^m \\
      M \ar[ur]^{\Delta} \ar[dr]_{\Delta} \ar[rr]^{!} && 1 \ar[rr]^{e} && M \\
      & M\times M \ar[rr]_{1\times i} && M\times M \ar[ur]_{m}}}
\end{equation}
The internalized claim, then, is that any two inverse operators are equal.
The \emph{internal logic of a category with products} allows us to prove this by using essentially the same argument that we did in the case of ordinary monoids in \bSet.
The morphisms $m$ and $e$ are represented in this logic by the notations
\begin{mathpar}
  x:M,y:M \types x\cdot y :M \and
  \types e:M.
\end{mathpar}
Don't worry if this notation doesn't make a whole lot of sense yet.
The symbol $\types$ (called a ``turnstile'') is the logic version of a morphism arrow $\to$, and the entire notation is called a \emph{sequent} or a \emph{judgment}.
The fact that $m$ is a morphism $M\times M \to M$ is indicated by the fact that $M$ appears twice to the left of $\types$ and once to the right; the comma ``$,$'' in between $x:M$ and $y:M$ represents the product $\times$, and the variables $x,y$ are there so that we have a good notation ``$x\cdot y$'' for the morphism $m$.
In particular, the notation $x:M,y:M \types x\cdot y :M$ should be bracketed as
\[ ((x:M),(y:M)) \types ((x\cdot y) :M). \]
Similarly, the associativity, unit, and inverse axioms are indicated by the notations
\begin{mathpar}
  x:M,y:M,z:M \types (x\cdot y)\cdot z = x\cdot (y\cdot z) : M \\
  x:M \types x\cdot e = x : M \and
  x:M \types e\cdot x = x : M \\F
  x:M \types x\cdot i(x) = e : M \and
  x:M \types i(x) \cdot x = e : M
\end{mathpar}
Now the \bSet-based proof can be essentially copied in this notation:
\[ x:M \types i(x) = i(x) \cdot e = i(x) \cdot (x \cdot j(x)) = (i(x)\cdot x)\cdot j(x) = e\cdot j(x) = j(x) : M.\]
The essential point is that the notation \emph{looks set-theoretic}, with ``variables'' representing ``elements'', and yet (as we will see) its formal structure is such that it can be interpreted into \emph{any} category with products.
Therefore, writing the proof in this way yields automatically a proof of the general theorem that any two inverse \emph{operators} for a monoid \emph{object} in a category with products are equal.

To be sure, such a proof could be written in standard categorical style using commutative diagrams as well (\cref{ex:fp-inv-uniq}), and experienced category theorists become quite good at translating proofs in this way.
The point of categorical logic is that this sort of work is \emph{so} straightforward that it's actually completely unnecessary to do at all: we can prove a ``meta-theorem'' that does all the work for us.

Before leaving this appetizer section, we mention some further generalizations of this result.
While type theory allows us to use set-like notation to prove facts about any category with finite products, the allowable notation is fairly limited, essentially restricting us to algebraic calculations with variables.
However, if our category has more structure, then we can ``internalize'' more set-theoretic arguments.

As an example, note that for ordinary monoids in sets the uniqueness of inverses can be expressed ``pointwise'' rather than in terms of inverse-assigning operators.
In other words, for each element $x\in M$, if $x$ has two two-sided inverses $y$ and $z$, then $y=z$.
If we think hard enough, we can express this diagrammatically in terms of the category \bSet is to consider the following two sets:
\begin{align*}
  A &= \setof{(x,y,z)\in M^3 | xy=e, yx=e, xz=e, zx=e}\\
  B &= \setof{(y,z)\in M^2 | y=z}
\end{align*}
In other words, $A$ is the set of elements $x$ equipped with two inverses, and $B$ is the set of pairs of equal elements.
Then the uniqueness of pointwise inverses can be expressed by saying there is a commutative diagram
\[ \xymatrix{ A \ar[d] \ar[r] & B \ar[d] \\ M^3 \ar[r]_{\pi_{23}} & M^2 } \]
where the vertical arrows are inclusions and the lower horizontal arrow projects to the second and third components.

This is a statement that makes sense for a monoid object $M$ in any category with finite \emph{limits}.
The object $B$ can be constructed categorically as the equalizer of the two projections $M\times M \toto M$ (which is in fact isomorphic to $M$ itself), while the object $A$ is a ``joint equalizer'' of four parallel pairs, one of which is
\[ \vcenter{\xymatrix{ & M \times M \ar[dr]^m \\
    M\times M\times M \ar[ur]^{\pi_{12}} \ar[dr]_{!} && M \\
    & 1 \ar[ur]_e }} \]
and the others are similar.
We can then try to \emph{prove}, in this generality, that there is a commutative square as above.
We can do this by manipulating arrows, or by appealing to the Yoneda lemma, but we can also use the \emph{internal logic of a category with finite limits}.
This is a syntax like the internal logic for categories with finite products, but which also allows us to \emph{hypothesize equalities}.
The judgment in question is
\begin{equation}\label{eq:pointwise-unique-inverses}
  x:M, y:M, z:M, x\cdot y = e, y\cdot x=e, x\cdot z = e, z\cdot x = e \types y=z.
\end{equation}
As before, the comma binds the most loosely, so this should be read as
\[ ((x:M), (y:M), (z:M), (x\cdot y = e), (y\cdot x=e), (x\cdot z = e), (z\cdot x = e)) \types (y=z). \]
We can prove this by set-like equational reasoning, essentially just as before.
The ``interpretation machine'' then produces from this a morphism $A\to B$, for the objects $A$ and $B$ constructed above.

Next, note that in the category \bSet, the uniqueness of inverses ensures that if every element $x\in M$ has an inverse, then there is a \emph{function} $i:M\to M$ assigning inverses --- even without using the axiom of choice.
(If we define functions as sets of ordered pairs, as is usual in set-theoretic foundations, we could take $i = \setof{(x,y) | xy=e}$; the pointwise uniqueness ensures that this is indeed a function.)
This fact can be expressed in the internal logic of an \emph{elementary topos}.
We postpone the definition of a topos until later; for now we just remark that its structure allows both sides of the turnstile $\types$ to contain \emph{logical formulas} such as $\exists x, \forall y, \phi(x,y)$ rather than just elements and equalities.
In this language we can state and prove the following:
\[ \forall x:M, \exists y:M, x\cdot y = e \land y\cdot x = e \types
\exists i:M^M, \forall x:M, (x\cdot i(x) = e \land i(x)\cdot x = e)
\]
As before, the proof is essentially exactly like the usual set-theoretic one.
Moreover, the interpretation machine allows us to actually extract an ``inverse operator'' morphism in the topos from this proof.
As before, such a result can also be stated and proved using arrows and commutative diagrams, but as the theorems get more complicated, the translation gets more tedious to do by hand, and the advantage of type-theoretic notation becomes greater.

So much for adding extra structure.
In fact, we can also take structure away!
A monoid object can be defined internal to any \emph{monoidal} category, not just a cartesian monoidal one; now the structure maps are $m:M\otimes M\to M$ and $e:I\to M$, and the commutative diagrams are essentially the same.

To define an inverse operator in this case, however, we need some sort of ``diagonal'' $\Delta:M\to M\otimes M$ and also a ``projection'' or ``augmentation'' $\varepsilon:M\to I$.
The most natural hypothesis is that these maps make $M$ into a \emph{comonoid} object, i.e.\ a monoid in the opposite monoidal category, and that the monoid and comonoid structures preserve each other; this is the notion of a \emph{bimonoid} (or ``bialgebra'').
(\cref{ex:cartmon-bimon-uniq}: in a cartesian monoidal category, every object is a bimonoid in a unique way.)

Now given a bimonoid $M$, we can define an ``inverse operator'' --- which in this context is usually called an \emph{antipode} --- to be a map $i:M\to M$ such that
\begin{equation}
  \vcenter{\xymatrix@C=1pc{& M\otimes M \ar[rr]^{i\otimes 1} && M\otimes M \ar[dr]^m \\
      M \ar[ur]^{\Delta} \ar[dr]_{\Delta} \ar[rr]^{\varepsilon} && I \ar[rr]^{e} && M \\
      & M\otimes M \ar[rr]_{1\otimes i} && M\otimes M \ar[ur]_{m}}}
\end{equation}
commutes, where now $\Delta$ and $\varepsilon$ are the comonoid structure of $M$ rather than the diagonal and projection of a cartesian product.
A bimonoid equipped with an antipode is called a \emph{Hopf monoid} (or ``Hopf algebra'').
The obvious question then is, if a bimonoid has two antipodes, are they equal?

In some cases it is possible to apply the previous results directly.
For instance, the category of \emph{(co)commutative} comonoids in a symmetric monoidal category inherits a monoidal structure that turns out to be \emph{cartesian} (\cref{ex:ccmon-cart}), so a cocommutative bimonoid is actually a monoid in a cartesian monoidal category, and we can apply the first version of our result.
Similarly, the category of commutative monoids is cocartesian, so a commutative bimonoid is a comonoid in a cocartesian monoidal category, so we can apply the dual of the first version of our result.
But what if neither the multiplication nor the comultiplication is commutative?

Internal logic is up to this task.
In a monoidal category we can consider judgments with multiple outputs as well as multiple inputs.\footnote{For the benefit of readers who are already experts, I should mention that this is \emph{not} ordinary ``classical linear logic'': the comma represents the same monoidal structure $\tensor$ on both sides of the turnstile, rather than $\tensor$ on the left and $\parr$ on the right.}
This allows us to describe monoids and comonoids in a roughly ``dual'' way.
Don't worry about the precise syntax being used on the right; it will be explained in \cref{sec:prop-smpos,sec:prop-smc}.
\begin{alignat*}{2}
  x:M, y:M &\types x\cdot y:M &\qquad x:M &\types (x_1,x_2):(M,M)\\
  &\types e:M &\qquad x:M &\types (\mid\cancel{x}):()\\
  x:M,y:M,z:M &\types (x\cdot y)\cdot z = x\cdot (y\cdot z) :M &\qquad x:M &\types (x_{11},x_{12},x_2)=(x_1,x_{21},x_{22}):(M,M,M)\\
  x:M &\types x\cdot e=x:M &\qquad x:M &\types (x_1\mid\cancel{x_2}) = x:M\\
  x:M &\types e\cdot x=x:M &\qquad x:M &\types (x_2\mid\cancel{x_1}) = x:M
\end{alignat*}
In this language, the bimonoid axioms are
\begin{align*}
  x:M,y:M &\types (x_1\cdot y_1,x_2\cdot y_2) = ((x\cdot y)_1,(x\cdot y)_2) :(M,M)\\
          &\types (e_1,e_2)=(e,e):(M,M)\\
  x:M,y:M &\types (\mid\cancel{x\cdot y}) = (\mid\cancel{x},\cancel{y}) : ()\\
  &\types (\mid\cancel{e})=():()
\end{align*}
And an antipode is a map $x:M \types i(x):M$ such that
\begin{align*}
  x:M &\types x_1\cdot i(x_2) = (e\mid\cancel{x}) :M\\
  x:M &\types i(x_1)\cdot x_2 = (e\mid\cancel{x}) :M
\end{align*}
Now if we have another antipode $j$, we can compute
\begin{align*}
  x:M \types i(x)
  &= i(x)\cdot e\\
  &= (i(x_1)\cdot e\mid\cancel{x_2})\\
  &= i(x_1)\cdot (x_{21} \cdot j(x_{22}))\\
  &= (i(x_1)\cdot x_{21}) \cdot j(x_{22})\\
  &= (e \cdot j(x_{2})\mid\cancel{x_1})\\
  &= e\cdot j(x)\\
  &= j(x) \qquad :M
\end{align*}
yielding the same result $i=j$.
So even in a non-cartesian situation, we can use a very similar set-like argument, as long as we keep track of where elements get ``duplicated and discarded''.

This concludes our ``appetizer''; I hope it has given you a taste of what categorical logic looks like, and what it can do for category theory.
In the next section we will rewind back to the beginning and start with very simple cases.

\subsection*{Exercises}

\begin{ex}\label{ex:fp-inv-uniq}
  Prove, using arrows and commutative diagrams, that any two inverse operators for a monoid object in a category with finite products are equal.
\end{ex}

\begin{ex}\label{ex:cartmon-bimon-uniq}
  Prove that in a cartesian monoidal category, every object is a bimonoid in a unique way.
\end{ex}

\begin{ex}\label{ex:ccmon-cart}
  Show that the category of cocommutative comonoids in a symmetric monoidal category inherits a monoidal structure, and that this monoidal structure is cartesian.
\end{ex}

\begin{ex}\label{ex:antipode}
  Prove, using arrows and commutative diagrams, that any two antipodes for a bimonoid (not necessarily commutative or cocommutative) are equal.
\end{ex}


\section{On type theory and category theory}
\label{sec:generalities}

Since there are other introductions to categorical logic [TODO: cite], it seems appropriate to say a few words about what distinguishes this one.
These words may not make very much sense to the beginner who doesn't yet know what we are talking about, but it may help to orient the expert, and as the beginner becomes more expert he or she can return to it later on.

Our perspective is very much that of the category theorist: our primary goal is to use type theory as a convenient syntax to prove things about categories.
The way that it does this is by giving concrete presentations of \emph{free} categorical structures, so that by working in those presentations we can deduce conclusions about \emph{any} such structure.
There are other such syntaxes for category theory, notably string diagram calculi, that function in a similar way (giving a concrete presentation of free structures) to the extent that they are made precise.
Indeed, the \emph{usual} way of reasoning in category theory, in which we speak explicitly about objects, arrows, commutative diagrams, and so on, can also be interpreted, from this point of view, to be simply making use of the \emph{obvious} presentation of a free structure rather than some fancier one.

In particular, this means that we are not interested in aspects of type theory such as computability, canonicity, proof search, and so on \emph{for their own sake}.
However, at the same time we recognize their importance for type theory as a subject in its own right, which suggests that they should not be ignored by the category theorist.
In fact, our perspective is that it is precisely the esoteric-sounding notion of \emph{cut elimination} (or \emph{admissibility of substitution}) that essentially \emph{defines} what we mean by a \emph{type theory}.
Of course this is not literally true; a more careful statement would be that type theories with cut elimination are those that exhibit the most behavior most characteristic of type theories.
(Jean-Yves Girard remarked that ``a logic without cut-elimination is like a car without an engine.'')
A ``type theory without cut elimination'' may still yield explicit presentations of free structures, but reasoning with such a presentation will not yield the characteristic benefits of categorical logic.

So what is this mysterious cut-elimination, from a categorical perspective?
Informally, it says that the morphisms in a free categorical structure can be presented \emph{without explicit reference to composition}.
This is a bit of a cheat, because as we will see, in fact what we do is to build just enough ``implicit'' reference to composition into our rules to ensure that we no longer need to talk about composition explicitly.
However, this process yields important insight, and in particular leads us to \emph{term calculi} that exhibit the characteristic advantages of categorical logic: using ``set-like'' reasoning to prove things about arbitrary categories.
From this perspective, the admissibility of substitution (which is another name for cut-elimination) says that \emph{the meaning of a notation can be evaluated simply on the basis of the notation as written, without having to guess at the thought processes of the person who wrote it down}.
This is obviously a desirable feature, and arguably even a necessary one if our ``notation'' is to be worthy of the name.

Another unusual feature of our treatment is the emphasis on multicategories (of various generalized sorts).
One concrete advantage of this is a more direct correspondence between the type theory and the category theory: type theory distinguishes between a sequent $A,B\types C$ and a sequent $A\times B\types C$ (even though they are bijectively related), so it seems natural to work with a categorical structure that also distinguishes between morphisms $(A,B)\to C$ and $A\times B\to C$.

However, the correspondence and motivation goes deeper than that.
We may ask \emph{why} type theory distinguishes these two kinds of sequents?
We will discuss this in more detail in \cref{sec:why-multicats}, but the short answer is that ``it makes cut-elimination work''.
More specifically, it enables us to formulate type theory in such a way that \emph{each rule refers to at most one type former}; this enables us to ``commute these rules past each other'' in the proof of cut-elimination.
Moreover, including sequents such as $A,B\types C$ allows us to describe certain operations in a type-theoretic style that would not otherwise be possible, such as a monoidal tensor product.
A type theorist speaks of this in terms of \emph{focusing on the judgmental structure first} and then defining the connectives to ``internalize'' various aspects of that structure.

From a categorical point of view, the move to (generalized) multicategories has the feature that \emph{it gives things universal properties}.
For instance, the tensor product in a monoidal category has no universal property, but the tensor product in a multicategory does.
In general, from a 2-monad $T$ we can define a notion of ``$T$-multicategory'' in which $T$-algebra structure acquires a universal property (specifically, $T$ is replaced by a lax- or colax-idempotent 2-monad).
In type theoretic language, the move to $T$-multicategories corresponds to including the desired operations in the judgmental structure.
The fact that the $T$-operations then have universal properties is what enables us to write down the usual sort of type-theoretic left/right or introduction/elimination rules for them.

Making this correspondence explicit is helpful for many reasons.
Pedagogically, it can help the category theorist, who believes in universal properties, understand why type theories are formulated the way they are.
It also helps understand the ``initiality theorems'' more modularly: first we model the judgmental structure with a multicategory, and then we add more type formers corresponding to objects with various universal properties.
Finally, it provides a guide for new applications of categorical logic: when seeking a categorical structure to model a given type theory, we should look for a kind of multicategory corresponding to its judgments; while when seeking an internal logic for a categorical structure, we should represent it using universal properties in some kind of multicategory, from which we can extract an appropriate judgmental structure.


\chapter{Unary type theories}
\label{chap:unary}

[TODO: Some kind of introduction]

\section{Posets}
\label{sec:poset}

To begin our formal investigation of categorical logic, we start with the simplest sort of categories: those in which each hom-set has at most one element.
These are well-known to be equivalent to \emph{preordered sets}, where the existence of an arrow $A\to B$ is regarded as the assertion that $A\le B$.
I will abusively call them \emph{posets}, although traditionally posets (partially ordered sets) also satisfy the antisymmetry axiom (if $A\le B$ and $B\le A$ then $A=B$); from a category-theoretic perspective, antisymmetry means asking a category to be skeletal, which is both unnatural and pointless.
Conveniently, posets also correspond to the simplest version of logic, namely \emph{propositional} logic.

From a category-theoretic perspective, the question we are concerned with is the following.
Suppose we have some objects in a poset, and some ordering relations between them.
For instance, we might have
\begin{mathpar}
  A\le B \and A\le C \and D\le A \and B \le E \and D\le C
\end{mathpar}
Now we ask, given two of these objects --- say, $D$ and $E$ --- is it necessarily the case that $D\le E$?
In other words, is it the case in \emph{any} poset containing objects $A,B,C,D,E$ satisfying the given relations that $D\le E$?
In this example, the answer is yes, because we have $D\le A$ and  $A\le B$ and $B\le E$, so by transitivity $D\le E$.
More generally, we would like a method to answer all possible questions of this sort.

There is an elegant categorical way to do this based on the notion of \emph{free structure}.
Namely, consider the category \bPoset of posets, and also the category \bRelGr of \emph{relational graphs}, by which I mean sets equipped with an arbitrary binary relation.
There is a forgetful functor $U:\bPoset \to \bRelGr$, which has a left adjoint $F$.

Now, the abstract information about ``five objects $A,B,C,D,E$ satisfying five given relations'' can be regarded as an object $\cG$ of \bRelGr, and to give five such objects satisfying those relations in a poset \cP is to give a map $\cG \to U\cP$ in \bRelGr.
By the adjunction, therefore, this is equivalent to giving a map $F\cG \to \cP$ in \bPoset.
Therefore, a given inequality such as $A\le E$ will hold in \emph{all} posets if and only if it holds in the \emph{particular, universal} poset $F\cG$ freely generated by the assumed data.

Thus, to answer all such questions at once, it suffices to give a concrete presentation of the free poset $F\cG$ generated by a relational graph \cG.
In this simple case, it is easy to give an explicit description of $F$: it is the reflexive-transitive closure.
But since soon we will be trying to generalize vastly, we want instead a general method to describe free objects.
From our current perspective, this is the role of type theory.

As noted in \cref{sec:intro}, when we move into type theory we use the symbol $\types$ instead of $\to$ or $\le$.
Type theory is concerned with \emph{(hypothetical) judgments}, which (roughly speaking) are syntactic gizmos of the form ``$\Gamma\types\Delta$'', where $\Gamma$ and $\Delta$ are syntactic gadgets whose specific nature is determined by the specific type theory under consideration (and, thus, by the particular kind of categories we care about).
We call $\Gamma$ the \emph{antecedent} or \emph{context}, and $\Delta$ the \emph{consequent} or \emph{co-context}.
In our simple case of posets, the judgments are simply
\[ A \types B \]
where $A$ and $B$ are objects of our (putative) poset; such a judgment represents the relation $A\le B$.
In general, the categorical view is that a hypothetical judgment represents a sort of \emph{morphism} (or, as we will see later, a sort of \emph{object}) in some sort of categorical structure.

In addition to a class of judgments, a type theory consists of a collection of \emph{rules} by which we can operate on such judgments.
Each rule can be thought of as a partial $n$-ary operation on the set of possible judgments for some $n$ (usually a finite natural number), taking in $n$ judgments (its \emph{premises}) that satisfy some compatibility conditions and producing an output judgment (its \emph{conclusion}).
We generally write a rule in the form
\begin{mathpar}
  \inferrule{\cJ_1 \\ \cJ_2 \\ \cdots \\ \cJ_n}{\cJ}
\end{mathpar}
with the premises above the line and the conclusion below.
A rule with $n=0$ is sometimes called an \emph{axiom}.
The categorical view is that we have a given ``starting'' set of judgments representing some objects and putative morphisms in the ``underlying data'' of a categorical structure, and the closure of this set under application of the rules yields the objects and morphisms in the \emph{free} structure it generates.

This is all very general and abstract, so let's bring it back down to earth in our very simple example.
Since the properties distinguishing a poset are reflexivity and transitivity, we have two rules:
\begin{mathpar}
  \inferrule{ }{A\types A} \and
  \inferrule{A\types B \\ B\types C}{A\types C}
\end{mathpar}
in which $A,B,C$ represent arbitrary objects.
In other words, the first rule is that for any object $A$ we have a $0$-ary rule whose conclusion is $A\types A$, while the second is that for any objects $A,B,C$ we have a $2$-ary rule whose premises are $A\types B$ and $B\types C$ (that is, any two judgments of which the consequent of the first is the antecedent of the second) and whose conclusion is $A\types C$.
We will refer to the pair of these two rules as the \textbf{free type theory of posets}.

Hopefully it makes sense that we can construct the reflexive-transitive closure of a relational graph by expressing its relations in this funny syntax and then closing up under these two rules, since they are exactly reflexivity and transitivity.
Categorically, of course, that means identities and composition.
In type theory the composition/transitivity rule is often called \emph{cut}, and plays a unique role, as we will see later.
% we will touch on this later on, though it is not as important from a purely categorical standpoint.

In the example we started from,
\begin{mathpar}
  A\le B \and A\le C \and D\le A \and B \le E \and D\le C
\end{mathpar}
we have the two instances of the transitivity rule
\begin{mathpar}
  \inferrule{D\types A \\ A\types B}{D\types B}\and
  \inferrule{D\types B \\ B\types E}{D\types E}
\end{mathpar}
allowing us to conclude $D\types E$.
When applying multiple rules in sequence to reach a conclusion, it is customary to write them in a ``tree'' structure like so:
\begin{mathpar}
  \inferrule*{\inferrule*{D\types A \\ A\types B}{D\types B} \\ B\types E}{D\types E}
\end{mathpar}
Such a tree is called a \emph{derivation}.
The way to typeset rules and derivations in \LaTeX\ is with the \texttt{mathpartir} package; the above diagram was produced with
\begin{verbatim}
  \inferrule*{
    \inferrule*{D\types A \\ A\types B}{D\types B} \\
    B\types E
  }{
    D\types E
  }
\end{verbatim}
Note that \texttt{mathpartir} has only recently made it into standard distributions of \LaTeX, so if you have an older system you may need to download it manually.

Formally speaking, what we have observed is the following \emph{initiality theorem}.

\begin{thm}\label{thm:poset-initial-1}
  For any relational graph \cG, the free poset $\F{\bPoset}\cG$ that it generates is has the same objects and its morphisms are the judgments that are derivable from \cG in free type theory of posets.
\end{thm}
\begin{proof}
  In the preceding discussion we assumed it as known that the free poset on a relational graph is its reflexive-transitive closure, which makes this theorem more or less obvious.
  However, it is worth also presenting an explicit proof that does not assume this, since same pattern of proof will reappear many times for more complicated type theories where we don't know the answer in advance.

  Thus, let us define $\F{\bPoset}\cG$ as stated in the theorem.
  The reflexivity and transitivity rules imply that $\F{\bPoset}\cG$ is in fact a poset.
  Now suppose $\cA$ is any other poset and $P:\cG\to\cA$ is a map of relational graphs.
  The objects of $\F{\bPoset}\cG$ are the same as those of \cG, so $P$ extends uniquely to a map on underlying sets $\F{\bPoset}\cG\to\cA$.
  Thus it suffices to show that this map is order-preserving, i.e.\ that if $A\types B$ is derivable from \cG in the free type theory of posets, then $P(A)\le P(B)$.

  For this purpose we \emph{induct on the derivation of $A\types B$}.
  There are multiple ways to phrase such an induction.
  One is to define the \emph{height} of a derivation to be the number of rules appearing in it, and then induct on the height of the derivation of $A\types B$.
  \begin{enumerate}
  \item If there are no rules at all, then $A\types B$ must come from a relation $A\le B$ in \cG; hence $P(A)\le P(B)$ since $P$ is a map of relational graphs.
  \item If there are $n>0$ rules, then consider the last rule.
    \begin{enumerate}
    \item If it is the identity rule $A\types A$, then $P(A)\le P(A)$ in \cA since \cA is a poset and hence reflexive.
    \item Finally, if it is the transitivity rule, then each of its premises $A\types B$ and $B\types C$ must have a derivation with strictly smaller height, so by the (strong) inductive hypothesis we have $P(A)\le P(B)$ and $P(B)\le P(C)$.
      Since \cA is a poset and hence transitive, we have $P(A)\le P(C)$.\qedhere
    \end{enumerate}
  \end{enumerate}
\end{proof}

A different way to phrase such an induction, which is more flexible and more type-theoretic in character, uses what is called \emph{structural induction}.
[TODO: Say something about that.]

However, it is proved, \cref{thm:poset-initial-1} enables us to reach conclusions about arbitrary posets by deriving judgments in type theory.
In our present trivial case this is not very useful, but as we will see it becomes more useful for more complicated structures.

Another way to express the initiality theorem is to incorporate \cG into the rules.
Given a relational graph \cG, we define the \textbf{type theory of posets under \cG} to be the free type theory of posets together with a 0-ary rule
\begin{mathpar}
  \inferrule{ }{A\types B}
\end{mathpar}
for any relation $A\le B$ in \cG.
Now a derivation can be written without any ``leaves'' at the top, such as
\begin{mathpar}
  \inferrule*{\inferrule*{\inferrule*{ }{D\types A} \\ \inferrule*{ }{A\types B}}{D\types B} \\ \inferrule*{ }{B\types E}}{D\types E}
\end{mathpar}
Clearly this produces the same judgments; thus the initiality theorem can also be expressed as follows.

\begin{thm}\label{thm:poset-initial-2}
  For any relational graph \cG, the free poset $\F{\bPoset}\cG$ that it generates is has the same objects and its morphisms are the derivable judgments in the type theory of posets under \cG.\qed
\end{thm}

We can extract from this our first general statement about categorical logic: it is \emph{a syntax for generating free categorical structures using derivations from rules}.
The reader may be forgiven at this time for wondering what the point is; but bear with us and things will get less trivial.


\section{Categories}
\label{sec:categories}

Let's now generalize from posets to categories.
The relevant adjunction is now between categories \bCat and \emph{directed graphs} \bGr; the latter are sets $\cG$ of ``vertices'' equipped with a set $\cG(A,B)$ of ``edges'' for each $x,y\in \cG$.
Thus, we hope to generate the free category $\F{\bCat}\cG$ on a directed graph \cG type-theoretically.

Our judgments $A\types B$ will still represent morphisms from $A$ to $B$, but now of course there can be more than one such morphism.
Thus, to specify a particular morphism, we need more information than the simple \emph{derivability} of a judgment $A\types B$.
Na\"ively, the first thing we might try is to identify this extra information with the \emph{derivation} of such a judgment, i.e.\ with the tree of rules that were applied to reach it.
This makes the most sense if we take the approach of \cref{thm:poset-initial-2} rather than \cref{thm:poset-initial-1}, so that distinct edges $f,g\in \cG(A,B)$ can be regarded as distinct \emph{rules}
\begin{mathpar}
  \inferrule*[right=$f$]{ }{A\types B} \and
  \inferrule*[right=$g$]{ }{A\types B} \and
\end{mathpar}
Thus, for instance, if we have also $h\in \cG(B,C)$, the distinct composites $h\circ g$ and $h\circ f$ will be represented by the distinct derivations
\begin{mathpar}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{B\types C} \\
    \inferrule*[right=$g$]{ }{A\types B}
  }{
    A\types C
  }\and
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{B\types C} \\
    \inferrule*[right=$f$]{ }{A\types B}
  }{
    A\types C
  }\and
\end{mathpar}
Note that when we have distinct rules with the same premises and conclusion, we have to label them so that we can tell which is being applied.
For consistency, we begin labeling the identity and composition rules too, with $\circ$ and $\idfunc$.

Of course, this na\"ive approach founders on the fact that composition in a category is supposed to be associative and unital, since the two composites $h\circ (g\circ f)$ and $(h\circ g)\circ f$, which ought to be equal, nevertheless correspond to distinct derivations:
\begin{equation}\label{eq:assoc}
  \begin{array}{c}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$h$]{ }{C\types D} \\
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$g$]{ }{B\types C} \\
      \inferrule*[right=$f$]{ }{A\types B}
    }{
      A\types C
    }}{
    A\types D
  }\\\\
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$h$]{ }{C\types D} \\
      \inferrule*[right=$g$]{ }{B\types C}
    }{
      B\types D
    }\\
    \inferrule*[right=$f$]{ }{A\types B}
  }{
    A\types D
  }
  \end{array}
\end{equation}
Thus, with this type theory we don't get the free category on \cG, but rather some free category-like structure that lacks associativity and unitality.
There are two ways to deal with this problem; we consider them in turn.

\subsection{Explicit cuts}
\label{sec:category-cutful}

The first solution is to simply quotient by an equivalence relation.
Our equivalence relation will have to identify the two derivations in~\eqref{eq:assoc}, and also the similar pairs for identities:
\begin{mathpar}
  \inferrule{\inferrule*[right=$\idfunc$]{ }{A\types A}\\ {A\types B}}{A\types B}\;\circ
  \qquad\equiv\qquad A\types B
  \\
  \inferrule{A\types B \\ \inferrule*[right=$\idfunc$]{ }{B\types B}}{A\types B}\;\circ
  \qquad\equiv\qquad A\types B
\end{mathpar}
Our equivalence relation must also be a ``congruence for the tree-construction of derivations'', meaning that these identifications can be made anywhere in the middle of a long derivation, such as:
\begin{mathpar}
  \inferrule{\inferrule*{}{\sD_1\\\\\vdots} \\
    \inferrule*[right=$\circ$]{\inferrule*[right=$\idfunc$]{ }{A\types A}\\ \inferrule*{\sD_2\\\\\vdots}{A\types B}}{A\types B}
  }{\vdots\\\\\sD_3}
  \qquad\equiv\qquad
  \inferrule{\inferrule*{}{\sD_1\\\\\vdots} \\
    \inferrule*{\sD_2\\\\\vdots}{A\types B}
  }{\vdots\\\\\sD_3}
\end{mathpar}
We will also have to close it up under reflexivity, symmetry, and transitivity to make an equivalence relation.

Of course, it quickly becomes tedious to draw such derivations, so it is convenient to adopt a more succinct syntax for them.
We begin by labeling each judgment with a one-dimensional syntactic representation of its derivation tree, such as:
\begin{mathpar}
  \inferrule*[right=$\circ$]{
    \inferrule*[right=$g$]{ }{g:(B\types C)} \\
    \inferrule*[right=$\circ$]{
      \inferrule*[right=$\idfunc$]{ }{\idfunc_B:(B\types B)} \\
      \inferrule*[right=$f$]{ }{f:(A\types B)}
    }{
      (\idfunc_B\circ f):(A\types B)
    }}{
    (g\circ (\idfunc_B\circ f)) : (A\types C)
  }
\end{mathpar}
These labels are called \emph{terms}.
Of course, in this case they are none other than the usual notation for composition and identities.
Formally, this means the rules are now:
\begin{mathpar}
  \inferrule{f\in\cG(A,B)}{f:(A\types B)}\and
  \inferrule{A\in\cG}{\idfunc_A : (A\types B)}\and
  \inferrule{\phi:(A\types B) \\ \psi:(B\types C)}{\psi\circ \phi:(A\types C)}
\end{mathpar}
% TODO: Mention somewhere the assumptions of "external" facts as premises
Here $\phi,\psi$ denote arbitrary terms, and if they contain $\circ$'s themselves then we put parentheses around them, as in the example above.
Now the generators of our equivalence relation look even more familiar:
\begin{align*}
  \phi \circ (\psi \circ \chi) &\equiv (\phi\circ\psi)\circ\chi\\
  \phi \circ \idfunc_A &\equiv \phi\\
  \idfunc_B \circ \phi &\equiv \phi
\end{align*}
Again $\phi,\psi,\chi$ denote arbitrary terms, corresponding to the fact that arbitrary derivations can appear at the top of our identified trees; and similarly these identifications can also happen anywhere inside another term, so that for instance
\[ k\circ (h\circ (g\circ f)) \equiv k\circ ((h\circ g)\circ f).  \]
Of course, we only impose these relations when they make sense.
We can describe the conditions under which this happens using rules for a secondary judgment $\phi\equiv \psi : (A\types B)$, like so:
\begin{mathpar}
  \inferrule{\phi:(A\types B) \\ \psi:(B\types C) \\ \chi:(C\types D)}{(\phi \circ (\psi \circ \chi) \equiv (\phi\circ\psi)\circ\chi) : (A\types D)}\\
  \inferrule{\phi:(A\types B)}{(\phi \circ \idfunc_A \equiv \phi):(A\types B)}\and
  \inferrule{\phi:(A\types B)}{(\idfunc_B \circ \phi \equiv \phi):(A\types B)}
\end{mathpar}
When the rules for $\circ$ and $\idfunc$ are augmented by these rules generating a congruence $\equiv$, and we add axioms for the edges of a given directed graph \cG, we call the result the \textbf{cut-ful type theory for categories under \cG}.
It may seem obvious that this produces the free category on \cG, but again we write it out carefully to help ourselves get used to the patterns.
In particular, we want to emphasize the role played by the following lemma:

\begin{lem}\label{thm:category-uniqderiv}
  If $\phi :(A\types B)$ is derivable in the cut-ful type theory for categories under \cG, then it has a unique derivation.
\end{lem}
\begin{proof}
  The point is that the terms produced by all the rules have disjoint forms.
  If $\phi$ is of the form ``$f$'' for some $f\in\cG(A,B)$, then it can only be derived by the first rule; if it is of the form ``$\idfunc_A$'' then it can only be derived by the identity rule, and if it is of the form ``$\psi\circ\phi$'' it can only be derived by the composition rule.
  In the third case, we apply induction to show that $\phi$ and $\psi$ also have unique derivations.
\end{proof}

In other words, the terms (before we impose the relation $\equiv$ on them) really are simply one-dimensional representations of derivations, as we intended.
This will not be true of all type theories; but those for which it fails tend to be much more complicated to analyze and prove the initiality theorem for.

\begin{rmk}
  Technically, there is either more or less happening here than may appear (depending on your point of view).
  A term as we write it on the page is really just a string of symbols, whereas in the proof of \cref{thm:category-uniqderiv} we have assumed that a term such as ``$f\circ (g\circ h)$'' can uniquely be read as $\circ$ applied to ``$f$'' and ``$g\circ h$''.
  This simple string of symbols could technically be regarded as $\circ$ applied to ``$f\circ (g$'' and ``$h)$'', but of course that would make no sense because those are not meaningful terms in their own right (in particular, they contain unbalanced parentheses).

  Thus, something \emph{more} must be happening, and that something else is called \emph{parsing} a term.
  Human mathematicians do it instinctively without thinking; electronic mathematicians have to be programmed to do it.
  In either case, the result of parsing a string of symbols is an ``internal'' representation (a mental idea for humans, a data structure for computers) that generally has the form of a tree, indicating the ``outermost'' operation as the root with its operands as branches, and so on, for instance:
  \[ f\circ (g\circ h) \qquad\leadsto\qquad \vcenter{\xymatrix@-1pc{ & \circ \ar@{-}[dl] \ar@{-}[dr]\\
      f && \circ \ar@{-}[dl]\ar@{-}[dr]\\
      & g && h }} \]

  Of course, this ``internal'' tree representation of a term is nothing but the corresponding derivation flipped upside-down.
  So in that sense \cref{thm:category-uniqderiv} is actually saying \emph{less} than one might think; the actual work is being done by the silent step of parsing.
  We will not say much more about parsing, however; we trust the human reader to do it on their own, and we trust programmers to have good algorithms for it.
  Accordingly, for type theories that satisfy the ``terms are derivations'' principle we will usually simply observe this fact, leaving it to the concerned reader to verify that the parse trees really do correspond to the derivation trees.
  (However, one small additional issue regarding this will arise in the next subsection.)
\end{rmk}

Now we can prove the initiality theorem.

\begin{thm}\label{thm:category-initial-1}
  The free category on a directed graph $\cG$ has the same objects as \cG, and its morphisms $A\to B$ are the terms $\phi$ such that $\phi :(A\types B)$ is derivable in the cut-ful type theory for categories under \cG, modulo the equivalence relation $\phi\equiv \psi:(A\types B)$.
\end{thm}
\begin{proof}
  Let $\F\bCat\cG$ be defined as described in the theorem; the identity and composition rules give it the structure necessary to be a category, and the transitivity and unitality relations make it a category.

  Now suppose \cA is any category and $P:\cG\to\cA$ is a map of directed graphs.
  Then $P$ extends uniquely to the objects of $\F\bCat\cG$, since they are the same as those of \cG.
  But unlike the case of posets, we have to define it on the morphisms of $\F\bCat\cG$ as well.

  If $\phi :(A\types B)$ is derivable, then by \cref{thm:category-uniqderiv} it has a unique derivation; thus we can define $P(\phi)$ by recursion on the derivation of $\phi$.
  Of course, if the derivation of $\phi$ ends with $f\in\cG(A,B)$, then we define $P(\phi)=P(f)$; if it ends with $\idfunc_A$ we define $P(\phi)=\idfunc_{P(A)}$; and if it ends with $\psi\circ\chi$ we define $P(\phi) = P(\psi)\circ P(\chi)$.

  We also have to show that this definition respects the equivalence relation $\equiv$.
  This is clear since $\cA$ is a category; formally it would be another induction on the derivations of $\equiv$ judgments.

  Finally, we have to show that this $P:\F\bCat\cG\to\cA$ is a functor.
  This follows by definition of the category structure of $\F\bCat\cG$ and the action of $P$ on its arrows.
\end{proof}

Of course, once again very little seems to be happening; we are just using a complicated funny syntax to build a free algebraic structure.
Therefore, it is the second way to deal with the problem of associativity that is more interesting.

\subsection{Cut admissibility}
\label{sec:category-cutadm}

In this case what we do is \emph{remove the composition rule $\circ$ entirely}; instead we ``build (post)composition into the axioms''.
That is, the only rule independent of \cG is identities:
\[ \inferrule{ }{A\types A}\,\idfunc \]
while for every edge $f\in \cG(A,B)$ we take the following rule:
\[ \inferrule{X\types A}{X\types B} \,f \]
for any $X$.
Informally, one might say that we represent $f$ by its ``image under the Yoneda embedding''.

Note that we have made a choice to build in \emph{postcomposition}; we could also have chosen to build in precomposition.
In the current context, either choice would work just as well; but later on we will see that there were reasons to choose postcomposition here.
We will call this the \textbf{cut-free type theory for categories under \cG}.

In this theory, if we have $f\in\cG(A,B)$, $g\in\cG(B,C)$, and $h\in \cG(C,D)$ there is \emph{only one way} to derive $A\types D$:
\begin{mathpar}
  \inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{A\types A}
      }{
        A\types B
      }
    }{
      A\types C
    }
  }{
    A\types D
  }
\end{mathpar}
Thus, we no longer have to worry about distinguishing between $h\circ (g\circ f)$ and $(h\circ g)\circ f$.
Of course, we have a new problem: if we are trying to build a category, then we \emph{do} need to be able to compose arrows!
So we need the following theorem:

\begin{thm}\label{thm:category-cutadm}
  If we have derivations of $A\types B$ and $B\types C$ in the cut-free type theory for categories under \cG, then we can construct a derivation of $A\types C$.
\end{thm}
\begin{proof}
  We induct on the number of rules appearing in the given derivation of $B\types C$.

  If there is only one rule, then that rule must be $\idfunc$, since all the other rules have a premise and would need a second rule to feed into them.
  In this case, it be that $B=C$; so our given derivation of $A\types B$ is also a derivation of $A\types C$.

  If there are $n+1$ rules, then we must have $f\in\cG(D,C)$ and our derivation of $B\types C$ ends like this:
  \begin{mathpar}
    \inferrule*[right=$f$]{\inferrule*{\sD\\\\\vdots}{B\types D}}{B\types C}
  \end{mathpar}
  In particular, it contains a derivation \sD of $B\types D$ with only $n$ rules.
  Thus, by the inductive hypothesis we have a derivation, say $\sD'$, of $A\types D$.
  Now we can simply follow this with the rule for $f$:
  \begin{equation*}
    \inferrule*[right=$f$]{\inferrule*{\sD'\\\\\vdots}{A\types D}}{A\types C}\qedhere
  \end{equation*}
\end{proof}

In type-theoretic lingo, \cref{thm:category-cutadm} says that \textbf{the cut rule is admissible} in the cut-free type theory for categories under \cG.
In other words, although the cut/composition rule
\begin{mathpar}
  \inferrule*[right=$\circ$]{A\types B \\ B\types C}{A\types C}
\end{mathpar}
is not \emph{part of the type theory} as defined, it is nevertheless true that whenever we have derivations of the premises of this rule, we can construct a derivation of its conclusion.

Another way to say what is going on is that the morphisms in the free category on a directed graph \cG have an explicit description as \emph{finite strings of composable edges} in \cG.
We have just given an inductive definition of ``finite string of composable edges'': there is a finite string (of length 0) from $A$ to $A$; and if we have such a string from $X$ to $A$ and an edge $f\in\cG(A,B)$, we can construct a string from $X$ to $B$.

We could prove the initiality theorem by appealing to this known fact about free categories, but as before, we prefer to give a more explicit proof to illustrate the patterns of type theory.
For this purpose, we have to introduce terms, as we did in the previous section for the cut-ful theory.
We can do this with terms directly constructed so that their parse tree will mirror the derivation tree, for instance writing the rules as
\begin{mathpar}
  \inferrule{ }{\idfunc_A:(A\types A)}\,\idfunc\and
  \inferrule{\phi:(X\types A)}{f\mathord{\circ}(\phi):(X\types B)} \,f
\end{mathpar}
Then a term derivation and corresponding parse tree would look like
\begin{mathpar}
\inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{\idfunc_A:(A\types A)}
      }{
        f\mathord{\circ}(\idfunc_A):(A\types B)
      }
    }{
      g\mathord{\circ}(f\mathord{\circ}(\idfunc_A)):(A\types C)
    }
  }{
    h\mathord{\circ}(g\mathord{\circ}(f\mathord{\circ}(\idfunc_A))):(A\types D)
  }
  \and\leadsto\and
  \vcenter{\xymatrix@-1pc{
      h\mathord\circ \ar@{-}[d] \\
      g\mathord\circ \ar@{-}[d] \\
      f\mathord\circ \ar@{-}[d] \\
      \idfunc_A
    }}
\end{mathpar}
However, now there is another option available to us, which begins to show more of the characteristic behavior of type-theoretic terms.
Rather than describing the entire judgment $A\types B$ with a term, the way we did for the cut-ful theory, we assign a \emph{formal variable} such as $x$ to the domain $A$, and then an expression containing $x$ to the codomain $B$.
For the theory of plain categories that we are working with here, the only possible expressions are repeated applications of function symbols to the variable, such as $h(g(f(x)))$.
We write this as
\[ x:A \types h(g(f(x))) : B\]
The identity rules and axiom rules can now be written as
\begin{mathpar}
  \inferrule{ }{x:A\types x:A}\,\idfunc \and
  \inferrule{x:X\types M:A}{x:X\types f(M):B} \,f
\end{mathpar}
Here $M$ denotes an arbitrary term, which will generally involve the variable $x$.
Thus, for instance, the composite of $h$, $g$, and $f$ would be written like so:
\begin{mathpar}
  \inferrule*[Right=$h$]{
    \inferrule*[Right=$g$]{
      \inferrule*[Right=$f$]{
        \inferrule*[Right=$\idfunc$]{ }{x:A\types x:A}
      }{
        x:A\types f(x):B
      }
    }{
      x:A\types g(f(x)): C
    }
  }{
    x:A\types h(g(f(x))):D
  }
\end{mathpar}
Of course, the term $h(g(f(x)))$ has essentially the same parse tree as the term $h\mathord{\circ}(g\mathord{\circ}(f\mathord{\circ}(\idfunc_A)))$ shown above, so it can clearly represent the same derivation.
The main difference is that instead of $\idfunc_A$ we have the variable $x$ representing the identity rule.

This is our first encounter with how type theory permits a ``set-like'' syntax when reasoning about arbitrary categorical structures.
It is also one reason why we chose to build in postcomposition rather than precomposition.
If we used precomposition instead, then the analogous syntax would be backwards: we would have to represent $f:A\to B$ as $f(u):A \types u:B$ rather than $x:A \types f(x):B$.
At a formal level, there would be little difference, but it feels much more familiar to apply functions to variables than to co-apply functions to co-variables.
(We can still dualize at the level of the categorical models; we already mentioned in \cref{sec:intro} that we could apply the type theory of categories with finite products to the opposite of the category of commutative rings.)

There is one further remark to make about the correspondence between terms and derivations in this theory.
As written on the page, the judgments $x:A \types f(x):B$ and $y:A \types f(y):B$ are distinct; but clearly they represent the same morphism.
So there is still an equivalence relation hanging around in the background, but it is a fairly harmless one; it just says that we can rename all the variables as long as we do it consistently.
(When we consider theories with bound variables later on, the meaning of ``consistently'' will become more complicated, but the principle is the same.)
In type-theoretic lingo, the equivalence relation of renaming variables is called \textbf{$\alpha$-equivalence}, and unless explicitly stated otherwise we \emph{always} consider judgments modulo $\alpha$-equivalence.\footnote{There is even a technical trick called ``de Bruijn indices'' that enables a computer to represent terms without having to choose variable names at all, avoiding the issue entirely.  But this notation is so difficult for humans to understand that (according to Conor McBride) Bob Atkey once described it as a reverse Turing test.}

Now that we have a nice notation to distinguish between derivations, we should observe that \cref{thm:category-cutadm} is not just a statement about derivability.
Rather, its proof constructs, recursively, an \emph{operation} on derivations, and hence on terms: given derivable term judgments $x:A\types M:B$ and $y:B\types N:C$, we can construct a derivable judgment $x:A\types P:C$.
For instance, suppose we start with $x:A \types f(x):B$ and $y:B\types h(g(y)):C$; then the construction proceeds in the following steps.
\begin{itemize}
\item The second derivation ends with an application of $h$, so we apply the inductive hypothesis to $x:A \types f(x):B$ and $y:B\types g(y):D$.
\item Now the second derivation begins with an application of $g$, so we recurse again on $x:A \types f(x):B$ and and $y:B\types y:B$.
\item This time the second derivation is just the identity rule, so the result is the first given derivation $x:A \types f(x):B$.
\item Backing out of the induction one step, we apply $g$ to this result to get $x:A\types g(f(x)):D$.
\item Finally, backing out one more time, we apply $h$ to the previous result to get $x:A\types h(g(f(x))):C$.
\end{itemize}
Intuitively, the result $h(g(f(x)))$ has been obtained by \emph{substituting} the term $f(x)$ for the variable $y$ in the term $h(g(y))$.
Thus, we refer to the operation on terms defined by \cref{thm:category-cutadm} as \textbf{substitution}.
In general, given $x:A\types M:B$ and $y:B\types N:C$ we denote the substitution of $M$ for $y$ in $N$ by $N[M/y]$ (although unfortunately one also finds other notations in the literature; including, quite confusingly, $[M/y]N$ and $N[y/M]$).
Note that this is ``meta-notation''; the square brackets are not part of the syntax of terms, instead they denote an operation \emph{on} terms.
The proof of \cref{thm:category-cutadm} \emph{defines} the notion of substitution recursively in the following way:
\begin{align*}
  y[M/y] &\coloneqq M\\
  f(N)[M/y] &\coloneqq f(N[M/y])
\end{align*}
When terms are regarded as syntax in their own right rather than as mere proxies for derivations, it is common to define substitution as an operation on terms first, and then to state (the analogue of) \cref{thm:category-cutadm} as ``if $x:A\types M:B$ and $y:B\types N:C$ are derivable, then so is $x:A\types M[N/y]:C$''.
From this point of view, one says that \textbf{substitution is admissible} in the cut-free type theory for categories under \cG.

Before proving the initiality theorem, let us first observe that substitution does, in fact, define a category:

\begin{lem}\label{thm:category-subassoc}
  Substitution is associative: given $x:A\types M:B$ and $y:B\types N:C$ and $z:C\types P:D$, we have $P[N/z][M/y] = P[N[M/y]/z]$.
  (This is a literal equality of terms or derivations.)
\end{lem}
\begin{proof}
  By induction on the derivation of $P$.
  If it ends with the identity, so that $P=z$, then
  \[P[N/z][M/y] = z[N/z][M/y] = N[M/y] = z[N[M/y]/z] = P[N[M/y]/z] \]
  If it ends with an application of a morphism $f$, so that $P = f(Q)$, then
  \begin{multline*}
    f(Q)[N/z][M/y] = f(Q[N/z])[M/y] = f(Q[N/z][M/y])\\
    = f(Q[N[M/y]/z]) = f(Q)[N[M/y]/z]
  \end{multline*}
  using the inductive hypothesis for $Q$ in the third step.
\end{proof}

\begin{thm}\label{thm:category-initial-2}
  The free category on a directed graph $\cG$ has the same objects as \cG, and its morphisms are the derivable term judgments $x:A\types M:B$ in the cut-free type theory for categories under \cG.
\end{thm}
\begin{proof}
  Let $\F\bCat\cG$ be defined as in the statement, with composition given by substitution constructed as in \cref{thm:category-cutadm}.
  By \cref{thm:category-subassoc}, composition is associative.
  For unitality, we have $y[M/y] = y$ by definition, while $N[x/x] = N$ is another easy induction on the structure of $N$.
  Thus, $\F\bCat\cG$ is a category.

  Now suppose \cA is any category and $P:\cG\to\cA$ is a map of directed graphs.
  We define $P:\F\bCat\cG \to\cA$ by recursion on the rules of the type theory: the identity $x:A\types x:A$ goes to $\idfunc_{P(A)}$, while $x:A\types f(M):B$ goes to $P(f) \circ P(M)$, with $P(M)$ defined recursively.
  Since $x:A\types f(M):B$ is the composite of $x:A\types M:C$ and $y:C\types f(y):B$ in $\F\bCat\cG$, this is the only possible definition that could make $P$ a functor.
  It remains to check that it actually \emph{is} a functor, i.e.\ that it preserves \emph{all} composites; that is, we must show that $P(N[M/y]) = P(N) \circ P(M)$.
  This follows by yet another induction on the derivation of $N$.
\end{proof}

Note that we did not have to impose any equivalence relation on the terms in this theory (apart from the fairly trivial $\alpha$-equivalence).
This suggests a second, more interesting, general statement about categorical logic: it is
{a syntax for generating free categorical structures using derivations from rules}
\emph{that yield elements in canonical form}, eliminating the need for quotients.
This statement is actually too narrow; as we will see later on, type theory is not \emph{just} about canonical forms.
However, canonical forms do play a very important role.

From the perspective of category theory, the reason for the importance of canonical forms is that we can easily decide whether two canonical forms are equal.
In the cut-free type theory for categories, two terms present the same morphism in a free category just when they are literally equal (modulo $\alpha$-conversion); whereas to check whether two terms are equal in the cut-ful theory we have to remove the identities and reassociate them all to the left or the right.

In fact, a good algorithm for checking equality of terms in the cut-ful theory is to \emph{interpret them into the cut-free theory}!
That is, we note that every rule of the cut-ful theory is admissible in the cut-free theory, so any term (i.e.\ derivation) in the cut-ful theory yields a derivation in the cut-free theory.
For instance, to translate the cut-ful term $h\circ ((\id_C\circ g) \circ f)$ into the cut-free theory, we first write it as a derivation
\begin{mathpar}
  \inferrule*[Right=$\circ$]{
    h:(C\types D)\\
    \inferrule*[Right=$\circ$]{
      \inferrule*[Right=$\circ$]{
        \id_C:(C\types C)\\
        g:(B\types C)}
      {(\id_C\circ g):(B\types C)}\\
      f:(A\types B)
    }{((\id_C\circ g) \circ f):(A\types C)}
  }{(h\circ ((\id_C\circ g) \circ f)):(A\types D)}
\end{mathpar}
and then annotate the same derivation by cut-free terms, using substitution for composition:
\begin{mathpar}
  \inferrule*[Right=$\circ$]{
    z:C\types h(z):D\\
    \inferrule*[Right=$\circ$]{
      \inferrule*[Right=$\circ$]{
        z:C\types z:C\\
        y:B\types g(y):C}
      {y:B\types g(y):C}\\
      x:A\types f(x):B
    }{x:A\types g(f(x)):C}
  }{x:A\types h(g(f(x))):D}
\end{mathpar}
Since, as we have proven, both the cut-ful and the cut-free theory present the same free structure, it follows that \emph{two terms in the cut-ful theory are equal modulo $\equiv$ exactly when their images in the cut-free theory are identical}.
Informally, we are just comparing two  ``removing all the identities and the parentheses''; but as we will see, in a more complicated theory much more can be going on.

In this sense, type theory can be considered to be about solving \emph{coherence problems} in category theory.
In general, the coherence problem for a categorical structure is to decide when two morphisms ``constructed from its basic data'' are equal (or isomorphic, etc.)
For instance, the classical coherence theorem of MacLane for monoidal categories says, informally, that two parallel morphisms constructed from the basic constraint isomorphisms of a monoidal category are \emph{always} equal; whereas the analogous theorem for braided monoidal categories says that they are equal if and only if they have the same underlying braid.
A type-theoretic calculus of canonical forms gives a way to answer this question, by translating a cut-ful theory into a cut-free one.

A related remark is that categorical logic is about \emph{showing that two different categories have the same\footnote{Of course, technically, an object of one category is not generally also an object of another one.  So what we mean is that there is an easy way to transform the initial object of one category into the initial object of another.} initial object}.
The rules of a type theory can be regarded as defining a certain algebraic theory, and the judgments with derivations form (fairly trivially) the initial algebra for this theory, i.e.\ the initial object in a certain category.
The initiality theorems we care about, however, show that these initial objects are \emph{also} initial in some other, quite different, category that is of more intrinsic categorical interest.
To emphasize the difference between these two categories, notice that an arbitrary model of the algebraic rules of a type theory is not even a category, e.g.\ it may not satisfy the cut rule; the cut admissibility theorem says only that the \emph{initial} such model happens to actually be a category, and in fact the initial category with some structure.


\subsection*{Exercises}

% [I think we should do this one in the text.]
% \begin{ex}
%   Prove \cref{thm:category-initial-2}.
%   You will have to start by proving that the composition defined in \cref{thm:category-cutadm} is in fact associative and unital, so that it forms a category.
% \end{ex}

\begin{ex}\label{ex:categories-over}
  Let \sM be a fixed category; then we have an induced adjunction between $\bCat/\sM$ and $\bGr/\sM$.
  Describe a cut-free type theory for presenting the free category-over-\sM on a directed-graph-over-\sM, and prove the initiality theorem (the analogue of \cref{thm:category-initial-2}).
  Note that you will have to prove that cut is admissible first.
  \textit{(Hint: index the judgments by arrows in \sM, so that for instance $A\types_\alpha B$ represents an arrow lying over a given arrow $\alpha$ in $\sM$.)}
\end{ex}


\section{Meet-semilattices}
\label{sec:mslat}

Moving gradually up the ladder of nontriviality, we now consider categories with finite products, or more precisely binary products and a terminal object.
In fact, let us revert back to the posetal world first and consider posets with binary meets and a top element, i.e.\ meet-semilattices.
We will make all this structure algebraic, so that our meet-semilattices are posets (which, recall, is not necessarily skeletal) \emph{equipped with} a chosen top element and an operation assigning to each pair of objects a meet.
We then have an adjunction relating the category \bmSLat of such meet-semilattices (and morphisms preserving all the structure strictly) with the category \bRelGr of relational graphs, and we want to describe the free meet-semilattice on a relational graph \cG.

One new feature this introduces is that the objects of $\F\bmSLat \cG$ will no longer be the same as those of \cG: we need to add a top element and freely apply the meet operation.
In order to describe this type-theoretically, we introduce a new judgment ``$\types A\type$'', meaning that $A$ will be one of the objects of the poset we are generating.
The rules for this judgment are
\begin{mathpar}
  \inferrule{ }{\types \top\type} \and
  \inferrule{\types A\type \\ \types B\type}{\types A\meet B\type}
\end{mathpar}
When talking about type theory under \cG, we additionally include ``axiom'' rules saying that each object of \cG is a type:
\begin{mathpar}
  \inferrule{A\in\cG}{\types A\type}
\end{mathpar}
Thus, for instance, if $A,B\in\cG$ we have a derivation
\begin{mathpar}
  \inferrule*{
    \inferrule*{A\in\cG}{\types A\type}\\
    \inferrule*{\inferrule*{ }{\types \top\type} \\ \inferrule*{B\in\cG}{\types B\type}}{\types \top\meet B\type}
  }{
    \types (A\meet (\top\meet B))\type
  }
\end{mathpar}
so that $A\meet (\top\meet B)$ will be one of the objects of $\F\bmSLat\cG$.

Now we need to describe its morphisms, i.e.\ the relation $\le$ in $\F\bmSLat\cG$.
The obvious thing to do is to assert the universal property of the meet and the top element:
\begin{mathpar}
  \inferrule{ }{A\types \top}\and
  \inferrule{ }{A\meet B \types A}\and
  \inferrule{ }{A\meet B \types B}\and
  \inferrule{A\types B \\ A\types C}{A\types B\meet C}
\end{mathpar}
This works, but it forces us to go back to asserting transitivity/cut.
For instance, if $A,B,C\in \cG$ we have the following derivation:
\begin{mathpar}
  \inferrule*{
    \inferrule*{ }{(A\meet B)\meet C \types A\meet B}\\
    \inferrule*{ }{A\meet B \types A}
  }{
    (A\meet B)\meet C \types A
  }
\end{mathpar}
but there is no way to deduce this without using the cut rule.
Thus, this \textbf{cut-ful type theory for meet-semilattices under \cG} works, but to have a better class of ``canonical forms'' for its relations we would also like a cut-free version.

What we need to do is to treat the ``projections'' $A\meet B \to A$ and $A\meet B\to B$ similarly to how we treated the edges of \cG in \cref{sec:categories}.
However, at this point we have to make a choice of whether to build in postcomposition or precomposition:
\[
\inferrule{A\types C}{A\meet B \types C} \qquad\text{or}\qquad
\inferrule{C\types A\meet B}{C\types A} \quad ?
\]
Both choices work (that is, they make cut admissible), and lead to different kinds of type theories with different properties.
The first leads to a kind of type theory called \textbf{sequent calculus}, and the second to a kind of type theory called \textbf{natural deduction}.
We consider each in turn.

\subsection{Sequent calculus for meet-semilattices}
\label{sec:seqcalc-mslat}

To be precise, for a relational graph \cG, the \textbf{unary sequent calculus for meet-semilattices under \cG} has the following rules (in addition to the rules for the judgment $\types A\type$ mentioned above).
We label each rule on the right to make them easier to refer to later on.
\begin{mathpar}
  \inferrule{A\in \cG}{A\types A}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ X\types A}{X\types B}\;fR\and
  \inferrule{\types A\type}{A\types \top}\;\top R\and
  \inferrule{A\types C \\ \types B\type}{A\meet B \types C}\;\meetL1\and
  \inferrule{B\types C \\ \types A\type}{A\meet B \types C}\;\meetL2\and
  \inferrule{A\types B \\ A\types C}{A\types B\meet C}\;\meetR
\end{mathpar}

\begin{rmk}
  The word \emph{unary} applied to a type theory is not standard in the literature.
  We are using it to indicate that there is only one type on each side of our sequents $A\types B$.
  This will be the case throughout the current chapter; in later chapters we will generalize away from this restriction.
\end{rmk}

There are several things to note about this.
The first is that we have included in the premises some judgments of the form $\types A\type$.
This ensures that whenever we can derive a sequent $A\types B$, that $A$ and $B$ are well-formed as types.
However, we don't need to assume explicitly as premises that \emph{all} types appearing in any sequent are well-formed, only those that are introduced without belonging to any previous sequents; this is sufficient for the following inductive proof.

\begin{thm}\label{thm:seqcalc-mslat-wftype}
  In the unary sequent calculus for meet-semilattices under \cG, if $A\types B$ is derivable, then so are $\types A\type$ and $\types B\type$.
\end{thm}
\begin{proof}
  By induction on the derivation of $A\types B$.
  \begin{itemize}
  \item If it is the $\idfunc$ rule, then $A\in\cG$ and so $\types A\type$.
  \item If it ends with the rule $f$ for some $f\in\cG(A,B)$, then $B\in \cG$ and so $\types A\type$, while $X\types A$ and so $\types X\type$ by the inductive hypothesis.
  \item If it ends with the rule $\top R$, then $\types A\type$ by assumption.  
  \item If it ends with the rule $\meetL1$, then $\types B\type$ by assumption, while $\types A\type$ and $\types C\type$ by the inductive hypothesis; thus also $\types A\meet B\type$.
  \item The cases for $\meetL2$ and $\meetR$ are similar.\qedhere
  \end{itemize}
\end{proof}

The second thing to note is that we only assert the identity rule $A\types A$ when $A$ is a \emph{generating object} (also called a \emph{base type}), i.e.\ an object of \cG.
This is sufficient because in the sequent calculus, we can derive the identity rule for any type:

\begin{thm}\label{thm:seqcalc-mslat-idadm}
  In the unary sequent calculus for meet-semilattices under \cG, if $A$ is a type (that is, if $\types A\type$ is derivable), then $A\types A$ is derivable.
\end{thm}
\begin{proof}
  We induct on the derivation of $\types A\type$.
  There are three cases:
  \begin{enumerate}
  \item $A$ is in \cG.  In this case $A\types A$ is an axiom.
  \item $A=\top$.  In this case $\top\types\top$ is a special case of the rule that anything $\types\top$.
  \item $A=B\meet C$ and we have derivations $\sD_B$ and $\sD_C$ of $\types B \type$ and $\types C\type$ respectively.
    Therefore we have, inductively, derivations $\sD_1$ and $\sD_2$ of $B\types B$ and $C\types C$, and we can put them together like this:
    \begin{equation*}
      \inferrule*{
        \inferrule*{
          \inferrule*{\sD_1\\\\\vdots}{B\types B} \\
          \inferrule*{\sD_C\\\\\vdots}{\types C\type}
        }{
          B\meet C \types B
        }\\
        \inferrule*{
          \inferrule*{\sD_2\\\\\vdots}{C\types C}\\
          \inferrule*{\sD_B\\\\\vdots}{\types B\type}
        }{
          B\meet C\types C
        }
      }{
        B\meet C\types B\meet C
      }\qedhere
    \end{equation*}
  \end{enumerate}
\end{proof}

In other words, the general identity rule
\[ \inferrule{\types A\type}{A\types A} \]
is also \emph{admissible}.
This is a general characteristic of sequent calculi.

Next we prove that the cut rule is admissible for this sequent calculus too.

\begin{thm}\label{thm:seqcalc-mslat-cutadm}
  In the unary sequent calculus for meet-semilattices under \cG, if $A\types B$ and $B\types C$ are derivable, then so is $A\types C$.
\end{thm}
\begin{proof}
  By induction on the derivation of $B\types C$.
  \begin{enumerate}
  \item If it is $\idfunc$, then $B=C$.
    Now $A\types C$ is just $A\types B$ and we are done.
  \item If it is $f\in\cG(C',C)$, then we have a derivation of $B\types C'$.
    So by the inductive hypothesis we can derive $A\types C'$, whence also $A\types C$ by the rule for $f$.
  \item If it ends with $\top R$, then $C=\top$.
    Since $A\types B$ is derivable, by \cref{thm:seqcalc-mslat-wftype} $\types A\type$ is also derivable; thus by $\top R$ we have $A\types \top$.
  \item If it ends with $\meetR$, then $C=C_1\meet C_2$ and we have derivations of $B\types C_1$ and $B\types C_2$.
    By the inductive hypothesis we can derive both $A\types C_1$ and $A\types C_2$, to which we can apply $\meetR$ to get $A\types C_1\meet C_2$.
  \item If it ends with $\meetL1$, then $B=B_1\meet B_2$ and we can derive $B_1\types C$.
    We now do a secondary induction on the derivation of $A\types B$.
    \begin{enumerate}
    \item It cannot end with $\idfunc$ or $f$ or $\top R$, since $B=B_1\meet B_2$ is not in $\cG$ and not equal to $\top$.
    \item If it ends with $\meetL1$, then $A=A_1\meet A_2$ and we can derive $A_1\types B$.
      By the inductive hypothesis, we can derive $A_1 \types C$, and hence by $\meetL1$ also $A \types C$.
      The case of $\meetL2$ is similar.
    \item Finally, if it ends with $\meetR$, then we can derive $A\types B_1$ and $A\types B_2$.
      Recall that we are also assuming a derivation of $B_1\types C$.
      Thus, by the inductive hypothesis on $A\types B_1$ and $B_1\types C$, we can derive $A\types C$.
      \label{item:mslat-principal-cut}\qedhere
    \end{enumerate}
  \end{enumerate}
\end{proof}

This simple proof already displays many of the characteristic features of a cut-admissibility argument.
The final case~\ref{item:mslat-principal-cut} is called the \textbf{principal case} for the operation $\meet$, when the type $B$ we are composing over (also called the \textbf{cut formula}) is obtained from $\meet$ and both sequents are also obtained from the $\meet$ rules.

Finally, we have the initiality theorem:

\begin{thm}\label{thm:seqcalc-mslat-initial}
  For any relational graph $\cG$, the free meet-semilattice $\F\bmSLat \cG$ it generates is described by the unary sequent calculus for meet-semilattices under \cG: its objects are the $A$ such that $\types A\type$ is derivable, with $A\le B$ just when $A\types B$ is derivable.
\end{thm}
\begin{proof}
  \cref{thm:seqcalc-mslat-idadm,thm:seqcalc-mslat-cutadm} show that this defines a poset; let us denote it $F\cG$.
  The rule $\top R$ implies that $\top$ is a top element, while the rules $\meetL1$, $\meetL2$, and $\meetR$ imply that $A\meet B$ is a binary meet.
  Therefore, we have a meet-semilattice.
  Moreover, the rules $\idfunc$ and $f$ yield a map of posets $\cG\to F\cG$.

  Now suppose $\cM$ is any other meet-semilattice with a map $P:\cG\to\cM$.
  Recall that a meet-semilattices is equipped with a chosen top element and meet function.
  We extend $P$ to a map from the objects of $F\cG$ by recursion on the construction of the latter, sending $\top$ to the chosen top element of \cM, and $A\meet B$ to the chosen meet in \cM of the (recursively defined) images of $A$ and $B$.
  This is clearly the only possible meet-semilattice map extending $P$, and it clearly preserves the chosen meets and top element, so it suffices to check that it is a poset map.
  This follows by a straightforward induction over the rules for deriving the judgment $A\types B$.
\end{proof}

To finish, we observe that this sequent calculus has another important property.
Inspecting the rules, we see that the operations $\meet$ and $\top$ only ever appear in the \emph{conclusions} of rules.
Each operation $\meet$ and $\top$ has zero or more rules allowing us to introduce it on the right of the conclusion, and likewise zero or more rules allowing us to introduce it on the left.
(Specifically, $\meet$ has two left rules and one right rule, while $\top$ has zero left rules and one right rule.)
This is convenient if we are given a sequent $A\types B$ and want to figure out whether it is derivable: we can choose rules to apply ``in reverse'' by breaking down $A$ and $B$ according to their construction out of $\meet$ and $\top$.

The phrase \emph{sequent calculus}, like \emph{type theory}, is difficult to define precisely, but sequent calculi generally exhibit the properties we have observed in this subsection: admissibility of the identity rule (based on an axiom applying only to ground types), admissibility of cut, and type operations appearing only in the conclusions of rules.

\subsection{Natural deduction for meet-semilattices}
\label{sec:natded-mslat}

Now suppose we make the other choice about how to treat projections.
We call this the \textbf{unary natural deduction for meet-semilattices under \cG}; its rules (in addition to those for $\types A\type$) are
\begin{mathpar}
  \inferrule{\types X\type}{X\types X}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ X\types A}{X\types B}\;fI\and
  \inferrule{\types X\type}{X\types \top}\;\top I\and
  \inferrule{X\types B\meet C}{X \types B}\;\meetE1\and
  \inferrule{X\types B\meet C}{X \types C}\;\meetE2\and
  \inferrule{X\types B \\ X\types C}{X\types B\meet C}\;\meetI
\end{mathpar}

We observe first that this theory has the same well-formedness property as the sequent calculus:

\begin{thm}\label{thm:natded-mslat-wftype}
  In the unary natural deduction for meet-semilattices under \cG, if $A\types B$ is derivable, then so are $\types A\type$ and $\types B\type$.\qed
\end{thm}

Unlike the sequent calculus, however, the general identity rule is not admissible: there is no way to derive $A\meet B \types A\meet B$ from $A\types A$ and $B\types B$ without it.
Thus, we assert the $\idfunc$ for all types, not just those coming from \cG.

Cut, however, is still admissible:

\begin{thm}\label{thm:natded-mslat-cutadm}
  In the unary natural deduction for meet-semilattices under \cG, if $A\types B$ and $B\types C$ are derivable, then so is $A\types C$.
\end{thm}
\begin{proof}
  We induct on the derivation of $B\types C$.
  \begin{enumerate}
  \item The cases when it ends with $\idfunc$, $f$, $\top I$, and $\meetI$ are just like those in \cref{thm:seqcalc-mslat-cutadm} for $\idfunc$, $f$, $\top R$, and $\meetR$.
  \item If it ends with $\meetE1$, then we have $B\types C\meet D$ for some $D$.
    Thus, $A\types C\meet D$ by the inductive hypothesis, so $A\types C$ by $\meetE1$.
    The case of $\meetE2$ is similar.\qedhere
  \end{enumerate}
\end{proof}

The proof is noticeably simpler than that of \cref{thm:seqcalc-mslat-cutadm}; we don't need the secondary inner induction.
This is essentially due to the fact that all the rules of this theory involve an \emph{arbitrary} type $X$ on the left.
This is characteristic of \emph{natural deduction} theories.
Accordingly, instead of the rules of sequent calculus that introduce operations like $\meet$ and $\top$ on the left and right, we have rules like $\top I$ and $\meetI$ that introduce them on the right, and also rules that \emph{eliminate} them on the right like $\meetE1$ and $\meetE2$.
(Later on we will be able to give a more convincing explanation of the origin of the phrase ``natural deduction''.)

Of course, we should also prove the initiality theorem:

\begin{thm}\label{thm:natded-mslat-initial}
  For any relational graph $\cG$, the free meet-semilattice $\F\bmSLat \cG$ it generates is described by the unary natural deduction for meet-semilattices under \cG: its objects are the $A$ such that $\types A\type$ is derivable, with $A\le B$ just when $A\types B$ is derivable.
\end{thm}
\begin{proof}
  Almost exactly like \cref{thm:seqcalc-mslat-initial}.
\end{proof}

\subsection*{Exercises}

\begin{ex}\label{ex:mslat-idem}
  Using the unary sequent calculus for meet-semilattices, prove that $A\meet A \cong A$ in any meet-semilattice.
  (Recall that meet-semilattices are categories with at most one morphism in each hom-set, so for two objects to be isomorphic it suffices to have a morphism in each direction.)
  Then prove the same thing using the natural deduction.
\end{ex}

\begin{ex}\label{ex:mslat-monoid}
  Using either the sequent calculus or the natural deduction for meet-semilattices (your choice), prove that in any meet-semilattice we have
  \begin{mathpar}
    A\meet \top \cong A\and
    \top \meet A \cong A\and
    A\meet B \cong B\meet A\and
    A\meet (B\meet C) \cong (A\meet B)\meet C\and
  \end{mathpar}
\end{ex}

\begin{ex}\label{ex:mslat-invertible}
  Prove that the rules $\bot R$, $\meetL1$, $\meetL2$, and $\meetR$ in the unary sequent calculus for meet-semilattices are \emph{invertible}, in the sense that whenever we have a derivation of their conclusions, we also have a derivation of all their premises.
\end{ex}

\begin{ex}\label{ex:jslat}
  Describe a sequent calculus for \emph{join-semilattices} (posets with a bottom element and binary joins), and prove the initiality theorems for it (including the identity and cut admissibility theorems).
  The rules for $\bot$ and $\join$ should be exactly dual to the rules for $\top$ and $\meet$.
\end{ex}

\begin{ex}\label{ex:lattices}
  By putting together the rules for meet- and join-semilattices, describe a sequent calculus for \emph{lattices} (posets with a top and bottom element and binary meets and joins), and prove the initiality theorem.
\end{ex}

\begin{ex}\label{ex:lattices-invertible}
  Prove that in your sequent calculus for lattices from \cref{ex:lattices}, the rules for $\bot,\top,\meet,\join$ are all invertible in the sense of \cref{ex:mslat-invertible}.
\end{ex}

\begin{ex}\label{ex:seqcalc-poset-fib}
  A map of posets $P:\sA\to\sM$ is called a \emph{(cloven) fibration} if whenever $b\in\sA$ and $x\le P(b)$, there is a chosen $a\in \sA$ such that $P(a)=x$ and $a\le b$ and moreover for any $c\in\sA$, $c\le b$ and $P(c)\le x$ together imply $c\le a$.
  The object $a$ can be written as $x^*(b)$.
  \begin{enumerate}
  \item Given a fixed poset \sM, describe a sequent calculus for fibrations over \sM by adding rules governing the operations $x^*$ to the cut-free theory of \cref{ex:categories-over}.
  \item Prove the initiality theorem for this sequent calculus.
  \item Use this sequent calculus to prove that in any fibration $P:\sA\to\sM$, if we have $b\in \sA$ and $x\le y\le P(b)$, then $x^*(y^*(b))\cong x^*(b)$.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:natded-poset-fib}
  Now describe instead a natural deduction for fibrations over \sM, prove the initiality theorem, and re-prove that $x^*(y^*(b))\cong x^*(b)$ using this theory.
\end{ex}

\begin{ex}\label{ex:mslat-fib}
  Suppose we augment your sequent calculus for fibrations over \sM from \cref{ex:seqcalc-poset-fib} with the following additional rules for ``fiberwise meets''.
  Here $\types A\type_x$ is a judgment indicating that $A$ will be an object of our fibration in the fiber over $x\in\sM$.
  \begin{mathpar}
    \inferrule{ }{\types \top_x \type_x} \and
    \inferrule{\types A\type_x \\ \types B\type_x}{\types A\meet_x B\type_x} \and
    \inferrule{\types A\type_x}{A\types_{x\le y} \top_y}\and
    \inferrule{A\types_{x\le y} C \\ \types B\type_x}{A\meet_x B \types_{x\le y} C}\and
    \inferrule{B\types_{x\le y} C \\ \types A\type_x}{A\meet_x B \types_{x\le y} C}\and
    \inferrule{A\types_{x\le y} B \\ A\types_{x\le y} C}{A\types_{x\le y} B\meet_y C}\and
  \end{mathpar}
  Consider the sequents
  \begin{align*}
    x^*(A\meet_y B) &\types_{x\le x} x^*A \meet_x x^*B\\
    x^*A \meet_x x^*B &\types_{x\le x} x^*(A\meet_y B)
  \end{align*}
  for $x\le y$, $\types A\type_y$, and $\types B\type_y$.
  \begin{enumerate}
  \item Construct derivations of these sequents in the above sequent calculus.
  \item Write down an analoguous natural deduction and derive the above sequents therein.
  \item What categorical structure do you think these type theories construct the initial one of?
    If you feel energetic, prove the initiality theorem.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:poset-bifib}
  A map of posets $P:\sA\to\sM$ is called an \emph{opfibration} if $P\op:\sA\op\to\sM\op$ is a fibration.
  The analogous operation takes $a\in \sA$ and $P(a)\le y$ to a $b\in \sA$ with $P(b)=y$ and $a\le b$ and a universal property; we write this $b$ as $y_!(a)$.
  We say $P$ is a \emph{bifibration} if it is both a fibration and an opfibration.
  Describe a sequent calculus for bifibrations over a fixed \sM, and prove the initiality theorem.
\end{ex}

\begin{ex}\label{ex:poset-bifib-adj}
  Use your sequent calculus from \cref{ex:poset-bifib} to prove that in a bifibration, if $x\le y$ in $\sM$, we have an adjunction $y_! \dashv x^*$.
\end{ex}


\section{Categories with products}
\label{sec:catprod}

Now we move back from posets to categories.
Let \bPrCat be the category of categories with specified binary products and a specified terminal object, and functors preserving these strictly.
Then we have an adjunction relating \bPrCat to \bGr, and we want to describe the left adjoint with a type theory.

As in \cref{sec:natded-mslat}, we could take either the sequent calculus route or the natural deduction route.
At first glance, it seems that in \emph{both} cases we need to impose an equivalence relation on the derivations, as there are single morphisms that can be derived in multiple ways.
However, the ways in which this happens in the two cases are different.

On one hand, if we have an arrow $f:A\to C$ in a directed graph \cG, then there is a morphism $A\times B \to A \to A\times C$ in the free category-with-products on \cG.
In a sequent calculus, there are two distinct derivations of this morphism:
\begin{mathpar}
  \inferrule*{
    \inferrule*{A\types A}{A\times B \types A}\\
    \inferrule*{\inferrule*[Right=$f$]{A\types A}{A\types C}}{A\times B \types C}
  }{
    A\times B \types A\times C
  }\and
  \inferrule*{
    \inferrule*{A\types A \\ \inferrule*[Right=$f$]{A\types A}{A\types C}}{A\types A\times C}
  }{
    A\times B \types A\times C
  }\and
\end{mathpar}
whereas in a natural deduction there will be only one:
\begin{mathpar}
  \inferrule*{
    \inferrule*{A\times B\types A\times B}{A\times B \types A}\\
    \inferrule*[Right=$f$]{\inferrule*{A\times B\types A\times B}{A\times B\types A}}{A\times B \types C}
  }{
    A\times B \types A\times C
  }
\end{mathpar}
This sort of thing is true quite generally.
A sequent calculus includes both left and right rules, so to derive a given sequent we must choose whether a left or a right rule is to be applied last.
By contrast, both kinds of rules in a natural deduction (introduction and elimination) act on the right, so there is less choice about what rule to apply last.

On the other hand, if we have an arrow $A\to B$ in \cG, then in a natural deduction there are (at least) two derivations of the identity $A\to A$:
\begin{mathpar}
  \inferrule*{\inferrule*{A\types A \\ \inferrule*{A\types A}{A\types B}}{A\types A\times B}}{A\types A}\and
  \inferrule*{ }{A\types A}\and
\end{mathpar}
while in a sequent calculus there is only one:
\begin{mathpar}
  \inferrule*{ }{A\types A}
\end{mathpar}
This is also true quite generally.
A natural deduction includes both introduction and elimination rules, so we will always be able to introduce a type and then eliminate it, essentially ``doing nothing''.
By contrast, in a sequent calculus we have ``only introduction rules'' (of both left and right sorts), so this cannot happen.

In fact, it is possible to eliminate the redundancy in both cases!
For expositional purposes, we will first describe a theory (based on natural deduction) that does contain an equivalence relation, and then the two ``theories of canonical forms'' that do without one.


\subsection{$\beta$ and $\eta$}
\label{sec:beta-eta}

It is convenient to describe the reduction rules using a term syntax for derivations, as introduced in \cref{sec:category-cutadm}.
In this syntax, the natural deduction rules from \cref{sec:natded-mslat} become:
\begin{mathpar}
  \inferrule{\types X\type}{x:X\types x:X}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ x:X\types M:A}{x:X\types f(M):B}\;fI\and
  \inferrule{\types X\type}{x:X\types \ttt:\unit}\;\unit I\and
  \inferrule{x:X\types M:B\times C}{x:X \types \pi_1(M):B}\;\timesE1\and
  \inferrule{x:X\types M:B\times C}{x:X \types \pi_2(M):C}\;\timesE2\and
  \inferrule{x:X\types M:B \\ x:X\types N:C}{x:X\types \pair MN:B\times C}\;\timesI
\end{mathpar}
Note how well the natural-deduction choice of ``all rules acting on the right'' matches the use of abstract variables: in all cases we can think of ``applying functions to arguments'' in a familiar way.
(It is possible to describe sequent calculus derivations using terms as well, but they are less pretty.)
The need to impose the identity rule for all types (not just those coming from \cG) also makes perfect sense here: a variable in any type is also a term of that type.

Now the above equivalence between the two derivations of the identity $A\to A$ can be written as
\begin{equation}
  \pi_1(\pair M N) \equiv M\label{eq:beta-prodcat-1}
\end{equation}
and of course we should also have
\begin{equation}
  \pi_2(\pair M N) \equiv N\label{eq:beta-prodcat-2}
\end{equation}
Note that in these equalities we allow $M$ and $N$ to be arbitrary terms.
Categorically speaking, therefore, we are asserting that the maps $X\to A\times B$ induced by the universal property of the product (the $\timesI$ rule) do in fact have the desired composites with the projections.

The other half of the universal property is the uniqueness of maps into a product.
This corresponds to a dual family of simplifications: we want to identify the following derivations of $A\times B\to A\times B$.
\begin{mathpar}
  \inferrule*{
    \inferrule*{\inferrule*{ }{A\times B\types A\times B}}{A\times B \types A} \\
    \inferrule*{\inferrule*{ }{A\times B\types A\times B}}{A\times B \types B}
  }{
    A\times B \types A\times B
  }\and
  \inferrule*{ }{A\times B\types A\times B}
\end{mathpar}
In term syntax, this means that
\begin{equation}
  \pair{\pi_1(M)}{\pi_2(M)} \equiv M\label{eq:eta-prodcat}
\end{equation}
In type-theoretic lingo, the equalities~\eqref{eq:beta-prodcat-1} and~\eqref{eq:beta-prodcat-2} are called \textbf{$\beta$-reductions} while the equality~\eqref{eq:eta-prodcat} is called an \textbf{$\eta$-reduction}.
They generate an equivalence relation on terms, which we also require to be a congruence for everything else.
We can describe this more formally with an additional judgment ``$x:X\types M\equiv N :A$'', with rules shown in  \cref{fig:catprod-equality}.
In addition to the $\beta$- and $\eta$-reductions, we assert reflexivity, symmetry, and transitivity, and also that all the previous rules preserve equality (this ensures by induction that it is a congruence for substitution).

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{x:X\types M:A \\ x:X\types N:B}{x:X\types \pi_1(\pair M N) \equiv M :A}\and
    \inferrule{x:X\types M:A \\ x:X\types N:B}{x:X\types \pi_2(\pair M N) \equiv N :B}\and
    \inferrule{x:X\types M:A\times B}{x:X\types \pair{\pi_1(M)}{\pi_2(M)} \equiv M:A\times B}\and
    \inferrule{x:X\types M:A}{x:X \types M\equiv M:A}\and
    \inferrule{x:X\types M\equiv N:A}{x:X\types N\equiv M:A}\and
    \inferrule{x:X\types M\equiv N:A \\ x:X\types N\equiv P:A}{x:X\types M\equiv P:A}\and
    \inferrule{f\in \cG(A,B) \\ x:X\types M\equiv N:A}{x:X\types f(M)\equiv f(N):B}\and
    \inferrule{x:X\types M\equiv N:B\times C}{x:X \types \pi_1(M)\equiv \pi_1(N):B}\and
    \inferrule{x:X\types M\equiv N:B\times C}{x:X \types \pi_2(M)\equiv \pi_2(N):B}\and
    \inferrule{x:X\types M\equiv M':B \\ x:X\types N\equiv N':C}{x:X\types \pair MN\equiv \pair{M'}{N'}:B\times C}
  \end{mathpar}
  \caption{Equality rules for categories with products}
  \label{fig:catprod-equality}
\end{figure}

This completes the definition of the \textbf{unary type theory for categories with products under \cG}.

\begin{rmk}\label{rmk:beta-reduction}
  Note that unlike the equalities $h\circ (g\circ f) \equiv (h\circ g)\circ f$ from \cref{sec:category-cutful}, the $\beta$- and $\eta$-reductions are intutively ``directional'', with one side being ``simpler'' than the other.
  This suggests that we should be able to ``reduce'' an arbitrary term to a ``simplest possible form'' by successively applying $\beta$- and $\eta$-reductions.
  Such is indeed the case (although for technical reasons the $\eta$-reduction is usually applied in reverse as an ``expansion'').
  This \emph{process of reduction} belongs to the ``computational'' side of type theory, which (though of course important in its own right) is somewhat tangential to our category-theoretic emphasis, so we will not discuss it in detail.
  However, in \cref{sec:catprod-atomcan} we will directly characterize the \emph{result of reduction}, and thereby implicitly the process thereof.
\end{rmk}

\begin{thm}\label{thm:catprod-subadm}
  Substitution is admissible in the unary type theory for categories with products under \cG.
  That is, if we have derivations of $x:A\types M:B$ and $y:B \types N:C$, then we have a derivation of $x:A \types N[M/y]:C$.
\end{thm}
\begin{proof}
  We induct on the derivation of $y:B \types N:C$.
  \begin{enumerate}
  \item If it is $\idfunc$, then $N[M/y]$ is just $M$.  
  \item If it is $fI$ for $f\in \cG(C',C)$, then we have $y:B \types N':C'$, so by induction we have $x:A\types N'[M/y]:C'$ and hence $x:A\types f(N'[M/y]):C'$.
    Thus $f(N'[M/y])$ is $(f(N'))[M/y]$.
  \item If it is $\unit I$, then by $\unit I$ we have $x:A \types \ttt:\unit$ as well (of course, $\ttt[M/y]$ is just $\ttt$).
  \item If it is $\timesE1$, then we have $y:B\types N':C\times C'$, so by induction we have $x:A \types N'[M/y]:C\times C'$, hence $x:A \types \pi_1(N'[M/y]):C$ by $\timesE1$.
    Of course, $\pi_1(N'[M/y])$ is $(\pi_1(N'))[M/y]$.
    The case for $\timesE2$ is similar.
  \item Finally, if it is $\timesI$, we have $y:B\types N_1:C_1$ and $y:B\types N_2:C_2$, so by induction we have $x:A \types N_1[M/y]:C_1$ and $x:A \types N_2[M/y]:C_2$, hence $x:A \types \pair{N_1[M/y]}{N_2[M/y]}:C_1\times C_2$.
    Of course, $\pair{N_1[M/y]}{N_2[M/y]}$ is $\pair{N_1}{N_2}[M/y]$.\qedhere
  \end{enumerate}
\end{proof}

As with \cref{thm:category-cutadm}, this proof can be regarded as \emph{defining} what it means to ``substitute'' $M$ for $y$ in $N$.
Similarly, we also have:

\begin{thm}\label{thm:catprod-subcong}
  The relation $\equiv$ is a congruence for substitution in the unary type theory for categories with products under \cG.
  In other words, if we have derivations of $x:X \types M\equiv M':B$ and $y:B \types N\equiv N':C$, then we can derive $x:A \types N[M/y] \equiv N'[M'/y]:C$.\qed
\end{thm}

And we have the initiality theorem.

\begin{thm}\label{thm:catprod-initial}
  For any directed graph \cG, the free category-with-products $\F\bPrCat \cG$ it generates is described by the unary type theory for categories with products under \cG: its objects are the $A$ such that $\types A\type$ is derivable, and its morphisms $A\to B$ are the terms $M$ such that $x:A \types M:B$ is derivable, modulo the equivalence relation $\equiv$.\qed
\end{thm}


\subsection{Canonical and atomic}
\label{sec:catprod-atomcan}

As observed in \cref{rmk:beta-reduction}, the $\beta$-reduction rule can be seen as a ``directed simplification'': clearly $M$ is simpler than $\pi_1(\pair M N)$.
Put differently, the derivation leading to the term $\pi_1(\pair M N)$ is ``redundant'' since it represents a morphism that already had a representative, namely $M$.
We now present a theory that prevents such redundancy from ever occurring.

The idea is to prevent application of elimination rules (such as $\pi_1$) to terms resulting from introduction rules (such as $\pair M N$).
We do, of course, have to be able to apply elimination rules to some terms, in order to derive say $x:A\times B \types \pi_1(x):A$.
To distinguish between the ``terms that can be eliminated'' and those that can't, we replace the single judgment $x:A\types M:B$ with two:
\begin{mathpar}
  x:A \types M\atom B\and
  x:A \types M\can B.
\end{mathpar}
We read $M\atom B$ as ``$M$ is an \textbf{atomic} term of type $B$'' and $M\can B$ as ``$M$ is a \textbf{canonical} term of type $B$''.
We assert the same natural deduction rules, but now with annotations describing which terms are canonical and which are atomic.
\begin{mathpar}
  \inferrule{\types X\type}{x:X\types x\atom X}\;\idfunc\and
  \inferrule{f\in \cG(A,B) \\ x:X\types M\can A}{x:X\types f(M)\can B}\;fI\and
  \inferrule{\types X\type}{x:X\types \ttt\can \unit}\;\unit I\and
  \inferrule{x:X\types M\atom B\times C}{x:X \types \pi_1(M)\atom B}\;\timesE1\and
  \inferrule{x:X\types M\atom B\times C}{x:X \types \pi_2(M)\atom C}\;\timesE2\and
  \inferrule{x:X\types M\can B \\ x:X\types N\can C}{x:X\types \pair MN\can B\times C}\;\timesI
\end{mathpar}
Note particularly that in the $\timesE$ rules, $M$ must be atomic; whereas in the $\timesI$ rule, $\pair MN$ is only canonical; thus $\pi_1(\pair MN)$ cannot be formed.
We allow $M$ and $N$ in the $\timesI$ rule to also be canonical, so that we can form iterated pairs such as $\pair M{\pair NP}$; while similarly we say that $\pi_1(M)$ and $\pi_2(M)$ are again atomic, so that we can form iterated projections such as $\pi_1(\pi_2(M))$.
All variables are atomic, so that we can form $x:A\times B \types \pi_1(x):A$.
We take terms of the form $f(M)$ to be canonical and act on canonical terms, but this choice isn't really essential.

Of course, this division into atomic and canonical is an artifact of the syntax; it has no natural counterpart in category theory.
Thus, to extract a description of a free category with products, we need some way to combine the two kinds of terms to describe one kind of morphism.
It turns out that the best way to do this is to assert that any atomic term \emph{belonging to a base type} (i.e.\ an object of \cG) is also canonical:
\begin{mathpar}
  \inferrule{B\in\cG \\ x:A \types M\atom B}{x:A\types M\can B}\;\atomcan
\end{mathpar}
and then regard the canonical terms as the morphisms.
Intuitively, the canonical terms are the subset of the terms from \cref{sec:beta-eta} with the property of being ``canonical representatives'' of a morphism in a free category with products.
The atomic terms are a different subset of the same terms, whose meaning is more technical.

The restriction $B\in\cG$ in $\atomcan$ is what deals with the \emph{other} generator of $\equiv$, namely the $\eta$-reduction $\pair{\pi_1(M)}{\pi_2(M)}\equiv M$.
However, it deals with it in a surprising way: it is the seemingly ``more complicated'' term $\pair{\pi_1(M)}{\pi_2(M)}$ that we regard as canonical!
For example, when $x=M$ we have the following derivation in our new system:
\begin{mathpar}
  \inferrule*[Right=\timesI]{
    \inferrule*[Right=\atomcan]{
      \inferrule*[Right=\timesE]{\inferrule*{ }{x:A\times B\types x\atom A\times B}}{x:A\times B \types \pi_1(x)\atom A}
    }{x:A\times B \types \pi_1(x)\can A} \\
    \inferrule*[Right=\atomcan]{
      \inferrule*[Right=\timesE]{\inferrule*{ }{x:A\times B\types x\atom A\times B}}{x:A\times B \types \pi_2(x)\atom B}
    }{x:A\times B \types \pi_2(x)\can B}
  }{
    x:A\times B \types \pair{\pi_1(x)}{\pi_2(x)}\can A\times B
  }
\end{mathpar}
but while we have $x:A\times B\types x\atom A\times B$, we cannot apply $\atomcan$ to deduce $x:A\times B\types x\can A\times B$ since $A\times B$ is not a base type.
Put differently, when transforming an arbitrary term into a canonical one, we regard the $\beta$-equivalence as a reduction, but the $\eta$-equivalence as an \emph{expansion}.

There are various reasons for this choice, foremost among which is that it works well.
A category theorist can gain some intuition for it by thinking of $\beta$-reduction as analogous to the multiplication transformation $T^2\to T$ of a monad, while the $\eta$-expansion is analogous to the unit transformation $\mathsf{Id}\to T$.
In an example such as the free monoid monad, the multiplication ``simplifies'' a list $((a,b),(),(c,d,e))$ by removing redundant parentheses to get $(a,b,c,d,e)$, while the unit makes an element $a$ into a list $(a)$ by \emph{adding} parentheses.
Na\"ively one might say that $(a)$ is more complicated than $a$, but it has the virtue of being in the same ``canonical form'' as every other list.

\begin{rmk}
  Recall that we generally want to regard terms as simply a different syntactic representation of derivations, with a parse tree that inverts the derivation tree.
  This is almost true for our current system: the only rule that breaks it is $\atomcan$, which uses the same term $M$ to represent two different derivations (one of $M\atom B$, and one of $M\can B$ obtained by following the former with $\atomcan$).
  However, this can be regarded as simply a convenient abuse of notation: since otherwise the syntaxes producing atomic and canonical terms are disjoint, if when parsing $M\can B$ we see that $M$ looks like an atomic term, it must be that it was deduced from $\atomcan$ and so we can insert an appropriate node into the tree.
\end{rmk}

The central lemma that justifies this theory is, as usual, the admissibility of cut/substitution.

\begin{lem}\label{thm:catprod-atomcan-subadm}\ 
  \begin{enumerate}
  \item If $x:A\types M\can B$ and $y:B\types N\atom C$ are derivable, then $x:A\types P\can C$ is derivable for some $P$.\label{item:catprod-atomcan-subadm-1}
  \item If $x:A\types M\can B$ and $y:B\types N\can C$ are derivable, then $x:A\types P\can C$ is derivable for some $P$.\label{item:catprod-atomcan-subadm-2}
  \end{enumerate}
\end{lem}
For purposes of composition in a free category with products, we are mainly interested in~\ref{item:catprod-atomcan-subadm-2}.
However, for the induction to go through we must also prove~\ref{item:catprod-atomcan-subadm-1}.
\begin{proof}
  We first prove~\ref{item:catprod-atomcan-subadm-1}, by induction on the derivation of $y:B\types N\atom C$ as usual.
  \begin{itemize}
  \item If it ends with $\idfunc$, then $B=C$ and $N=y$, whence $x:A\types M\can B$ is our desired result.
  \item If it ends with $\timesE1$, then we have $N=\pi_1(N')$ and $y:B\types N'\atom C\times D$.
    Thus, inductively we have $x:A\types P'\can C\times D$.
    This is where the interesting thing happens: since $P'$ is canonical rather than atomic, we can't apply $\timesE1$ directly to obtain $\pi_1(P')$.
    Instead we observe that \emph{any canonical term of $C\times D$ must be of the form $\pair PQ$}, simply because the only rule that builds canonical terms of a product type is $\timesI$.
    Thus, we must have $P'=\pair PQ$ where $x:A\types P\can C$ and $x:A\types Q\can D$.
    But $x:A\types P\can C$ is exactly what we wanted.
    (Of course, the case of $\timesE2$ is symmetric.)
  \end{itemize}
  Now we prove~\ref{item:catprod-atomcan-subadm-2}, again by induction on the derivation of $y:B\types N\can C$.
  \begin{itemize}
  \item If it ends with $fI$ for some $f\in\cG(D,C)$, then $N=f(N')$ and $y:B\types N'\can D$.
    Thus, inductively we have $x:A\types P':D$ and hence $x:A\types f(P'):C$.
  \item If it ends with $\unit I$, then $C=\unit$ and so $x:A\types \ttt\can \unit$.
  \item If it ends with $\timesI$, then $N=\pair{N_1}{N_2}$ with $y:B\types N_1\can C_1$ and $y:B\types N_2\can C_2$.
    Thus, inductively we have $x:A\types P_1:C_1$ and $x:A\types P_2:C_2$, whence $x:A\types \pair{P_1}{P_2}\can C$.
  \item If it ends with $\atomcan$, then we have $y:B\types N\atom C$, and hence by~\ref{item:catprod-atomcan-subadm-1} we have $x:A\types P\atom C$.
    Since we already applied $\atomcan$ at type $C$, $C$ must be a base type, so we can apply it again to conclude $x:A\types P\can C$.\qedhere
  \end{itemize}
\end{proof}

Like \cref{thm:category-cutadm,thm:catprod-subadm}, the proof of \cref{thm:catprod-atomcan-subadm} defines a ``substitution'' operation (well, technically, two operations) on terms, which we write as $N\hsub{M/y}$.
However, compared to ordinary substitution, this operation is more powerful: it also performs ``$\beta$-simplification'' as it goes.
For instance, we have
\[ \pi_1(y)\hsub{\pair M N/y} = M \]
whereas ordinary substitution would give $\pi_1(\pair M N)$.
In type-theoretic lingo, this ``reducing substitution'' is called \textbf{hereditary substitution}.

[TODO: Associativity, initiality, reduction of terms in the previous calculus]


\subsection{Focusing}
\label{sec:catprod-focusing}

For completeness, we now describe how the sequent calculus for meet-semilattices can also be modified to present free categories with products without needing an equivalence relation on terms.
This version is not as commonly used in categorical logic, but it is important in other applications of type theory such as automated proof search; it is called \emph{focusing}.


\subsection*{Exercises}

\begin{ex}\label{ex:catprod-subcong}
  Prove \cref{thm:catprod-subcong}.
\end{ex}

\begin{ex}
  Prove \cref{thm:catprod-initial}.
\end{ex}

\begin{ex}
  Use the type theory for categories with products to prove that in any category with products we have
  \begin{mathpar}
    A\times B \cong B\times A\and
    A\times (B\times C) \cong (A\times B)\times C\and
    A\times \unit \cong A\and
    \unit \times A \cong A.
  \end{mathpar}
  Note that since we are in categories now rather than posets, to show that two types $A$ and $B$ are isomorphic we must derive $x:A\types M:B$ and $y:B\types N:A$ and also show that their substitutions in both orders are equal (modulo $\equiv$) to identities.
\end{ex}

\begin{ex}\label{ex:catfib}
  A functor $P:\sA\to \sM$ is called a \textbf{fibration} if for any $b\in \sA$ and $f:x\to P(b)$, there exists a morphism $\phi:a\to b$ in \sA such that $P(\phi)=f$ and $\phi$ is \emph{cartesian}, meaning that for any $\psi:c\to b$ and $g:P(c)\to x$ such that $P(\psi)=fg$, there exists a unique $\chi:c\to a$ such that $P(\chi)=g$ and $\phi\chi=\psi$.
  The object $c$ is denoted $f^*(b)$.
  \begin{enumerate}
  \item Generalize your natural deduction for fibrations of posets from \cref{ex:natded-poset-fib} to a type theory for fibrations of categories.
  \item Prove the initiality theorem for this type theory.
  \item Use this type theory to prove that in any fibration $P:\sA\to\sM$:
    \begin{enumerate}
    \item For any $f:x\to y$ in \sM, $f^*$ is a functor from the fiber over $y$ to the fiber over $x$.
    \item For any $B\in \sA$ and $x\xto{f} y \xto{g} P(B)$ in \sM, we have $f^*(g^*(B)) \cong (gf)^*(M)$.
    \end{enumerate}
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:catprod-fib}
  Generalize \cref{ex:mslat-fib} from posets to categories.
\end{ex}


\section{Categories with coproducts}
\label{sec:catcoprod}

[TODO: I'm unsure whether this should be here at all.
It seems a natural thing to expect, but on the other hand it involves variable binding in terms, which might be easier to introduce first with simply-typed $\lambda$-calculus in \cref{chap:simple}.]

In \cref{ex:jslat}, you obtained a \emph{sequent calculus} for join-semilattices by dualizing the sequent calculus for meet-semilattices.
However, \emph{natural deductions} don't dualize as straightforwardly, due to the insistence that all rules act only on the right.
(Of course, we could dualize them to ``co-natural deductions'' in which all rules act only on the \emph{left}, but that would destroy the familiar behavior of terms on variables, as well as make it tricky to combine left and right universal properties, such as for lattices.)
To describe joins in a natural deduction, we need to ``build an extra cut'' into their universal property:
\begin{mathpar}
  \inferrule{X\types A}{X\types A\join B}\;\joinI1\and
  \inferrule{X\types B}{X\types A\join B}\;\joinI2\and
  \inferrule{A\types C \\ B\types C \\ X\types A\join B}{X\types C}\;\joinE
\end{mathpar}
Note that $\joinE$ is precisely the result of cutting $\joinL$ with an arbitrary sequent:
\begin{mathpar}
  \inferrule*[Right=cut]{X\types A\join B \\ \inferrule*[Right=$\joinL$]{A\types C \\ B\types C}{A\join B\types C}}{X\types C}
\end{mathpar}
We treat the bottom element similarly:
\begin{mathpar}
  \inferrule{X\types \bot \\\types C\type}{X\types C}
\end{mathpar}
Rather than take the time to analyze a natural deduction for join-semilattices, we skip directly to the analogous \textbf{unary type theory for categories with coproducts}, shown in \cref{fig:catcoprod}.
\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\types X\type}{x:X\types x:X}\;\idfunc\and
    \inferrule{f\in \cG(A,B) \\ x:X\types M:A}{x:X\types f(M):B}\;fI\and
    \inferrule{x:X \types M:\emptyt \\ \types C\type}{x:X \types \abort(M):C}\;\emptyt E\and
    \inferrule{x:X\types M:A}{x:X\types \inl(M):A + B}\;\plusI1\and
    \inferrule{x:X\types N:B}{X\types \inr(N):A + B}\;\plusI2\and
    \inferrule{u:A\types P:C \\ v:B\types Q:C \\ x:X\types M:A + B}{x:X\types \case(M,u.P,v.Q):C}\;\plusE
  \end{mathpar}
  \caption{Unary type theory for categories with coproducts}
  \label{fig:catcoprod}
\end{figure}
The notation $\case(M,u.P,v.Q)$ is intended to suggest that we ``break into cases'' based on whether $M$ lies in $A$ or in $B$, and do $P$ in the first case and $Q$ in the second.
More precisely, we substitute $M$ for $u$ or $v$ in $P$ or $Q$.
(We have to indicate in the notation which variables $u$ and $v$ were used in $P$ and $Q$, because otherwise in simply reading the term ``$\case(M,u.P,v.Q)$'' there would be no way to guess.
Technically the notation ``$u.P$'' is ``binding'' the variable $u$; we will come back to that later.)
Thus, the $\beta$-reduction rules must be
\begin{mathpar}
  \inferrule{u:A\types P:C \\ v:B\types Q:C \\ x:X\types M:A}{x:X \types \case(\inl(M),u.P,v.Q) \equiv P[M/u] : C}
  \and
  \inferrule{u:A\types P:C \\ v:B\types Q:C \\ x:X\types N:B}{x:X \types \case(\inr(N),u.P,v.Q) \equiv Q[N/u] : C}
\end{mathpar}
saying that the map $A+B\to C$ induced by the universal property of a coproduct (the $\plusE$ rule) has the correct composites with the coproduct injections.
Similarly, the $\eta$-reduction rule says that morphisms out of a coproduct are determined uniquely by their composites with the projections: whenever $y:A+B \types P:C$ we should have
\[ \case(y,u.P[\inl(u)/y],v.P[\inr(v)/y]) \equiv P. \]
In fact, to ensure that $\equiv$ is a congruence, we should ``build in a cut'' to prevent $A+B$ from appearing on the left in the conclusion, as we always do in natural deduction theories:
\begin{mathpar}
  \inferrule{x:X \types M:A+B \\ y:A+B \types P:C}{x:X \types \case(M,u.P[\inl(u)/y],v.P[\inr(v)/y]) \equiv P[M/y] : C}
\end{mathpar}
[TODO: Complete this.  How much detail to give?]




\subsection*{Exercises}

\begin{ex}\label{ex:cat-opfib}
  A functor $P:\sA\to\sB$ is called an \textbf{opfibration} if $P\op:\sA\op\to\sM\op$ is a fibration (as in \cref{ex:catfib}).
  The dual of $f^*(b)$ is written $f_!(a)$.
  \begin{enumerate}
  \item Write down a type theory for opfibrations and prove the initiality theorem.
    (Remember that we always use natural deduction style when dealing with categories rather than posets, so you can't just dualize \cref{ex:catfib} or categorify \cref{ex:poset-bifib}.)
  \item Use this type theory to prove that $f_!$ is always a functor.
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:cat-prod-coprod}
  Combine the type theories of \cref{sec:catprod,sec:catcoprod} to obtain a unary type theory for categories with both products and coproducts.
  Prove the initiality theorem.
\end{ex}

\begin{ex}\label{ex:cat-bifib}
  A functor $P:\sA\to\sB$ is called a \textbf{bifibration} if it is both a fibration and an opfibration.
  \begin{enumerate}
  \item Combine the theories of \cref{ex:catfib,ex:cat-opfib} to obtain a type theory for bifibrations.
  \item If you aren't tired of proving initiality theorems yet, do it for this type theory.
  \item Use this type theory to prove that in any bifibration, $f_!$ is left adjoint to $f^*$.
  \end{enumerate}
\end{ex}

\chapter{Simple type theories}
\label{chap:simple}

\section{Towards multicategories}
\label{sec:why-multicats}

At this point we have done about all we can with \emph{unary} type theories, where the antecedent and consequent of each sequent consist only of a single type.
(In fact, most introductions to type theory skip over the unary case altogether, but I find it clarifying to start with cases that are as simple as possible.)
The most common type theories allow finite \emph{lists} of types as the antecedent (and sometimes as the consequent as well).
We now consider a few problems with unary type theory, from a categorical perspective, that all turn out to have this as their solution.

Let's begin by stating some general principles of type theory.
Looking back at the rules of all our type theories, we see that they can be divided into two groups.
On the one hand, there are rules that don't refer specifically to any operation, such as the identity rule $x:X \types x:X$ and the cut rule.
On the other hand, there are rules that introduce or eliminate a particular operation on types, such as product, coproduct, and so on --- and each such rule refers to only \emph{one} operation  (such as $\times,+,f^*$, etc.).

This ``independence'' between the rules for distinct operations is important for the good behavior of type theory.
For instance, notice that many of the exercises have involved combining the rules for multiple previously-studied operations.
If you did some of these exercises, you hopefully got a sense for how these operations tend to coexist ``without interacting'' in the metatheory: e.g.\
when proving the cut-admissibility theorems we essentially just commute the rules for different operations past each other.
This ``modularity'' means that we are always free to add new structure to a theory without spoiling the structure we already had.
We formulate this as a general principle:
\begin{equation}\label{princ:independence}
  \text{Each rule in a type theory should refer to only one operation}.\tag{$\ast$}
\end{equation}

Note that despite~\eqref{princ:independence}, we can often obtain nontrivial results about the interaction of operations.
For instance, in \cref{ex:mslat-monoid} you showed that $A\meet \top\cong A$, even though $\meet$ and $\top$ are distinct operations with apparently unrelated rules.
Similarly, in \cref{ex:mslat-fib,ex:catprod-fib} you showed that $f^*$ preserves $\meet$ and $\times$.
In general, this tends to happen when relating operations whose universal properties all have the same ``handedness''.
For instance, all the operations $\meet,\top,\times,\unit,f^*$ have ``mapping in'' or ``from the right'' universal properties.
Thus, we can expect to compare two objects built using more than one of them by showing that they have the same universal property, and this is essentially what type theory does.

We also observe that in all cases we were able to extract the rules for a given operation from the universal property of the objects it was intended to represent in category theory.
The left and right rules in a sequent calculus, or the introduction and elimination rules in a natural deduction, always expressed the ``two sides'' of a universal property: one of them ``structures the object'' and the other says that it is universal with this structure.
The ``principal case'' of cut admissibility for a sequent calculus, and the $\beta$-reduction equality rule for a natural deduction, both express the fact that morphisms defined by the universal property ``compose with the structure'' to the inputs, e.g.\ a map $X\to A\times B$ defined from $f:X\to A$ and $g:X\to B$ gives back $f$ and $g$ when composed with the product projections.
Similarly, the proof of identity admissibility for a sequent calculus, and the $\eta$-reduction rule for a natural deduction, express the uniqueness half of the universal property.
This leads us to formulate another general principle:
\begin{equation}\label{princ:ump}
  \parbox{3.5in}{\centering The operations in a type theory should correspond categorically to objects with universal properties.}\tag{$\dagger$}
\end{equation}

The point is that from the perspective of unary type theory, these two principles \emph{seem} overly restrictive.
For instance, we remarked above that by expressing universal properties in type theory we can compare operations whose universal properties have the same handedness; but often we are interested in categorical structures satisfying nontrivial relations between objects with universal properties of different handedness.
For instance, in any category with both products and coproducts, there is a canonical map $(A\times B)+(A\times C) \to A\times (B+C)$, and the category is said to be \emph{distributive} if this map is always an isomorphism.
(When the category is a poset, we call it a \emph{distributive lattice}.)
However, as you saw in \cref{ex:lattices,ex:cat-prod-coprod}, if we simply combine the unary type theoretic rules for $\times$ and $+$, we get a type theory for categories with products and coproducts, but no interaction between them.
So unary type theory cannot deal with distributive categories while adhering to~\eqref{princ:independence} and~\eqref{princ:ump}.

Perhaps surprisingly, there \emph{is} a way to present a type theory for distributive categories.
The idea is to move into a world where the product $A\times B$ \emph{also} has a ``mapping out'' universal property, so that we can compare $(A\times B)+(A\times C)$ and $A\times (B+C)$ by saying they have the same universal property.
As we will see, this requires moving to a type theory with multiple antecedents.

This is one motivation.
Another is that we might want to talk about operations whose universal property can't be expressed in unary type theory while adhering to~\eqref{princ:independence}.
For instance, a \emph{cartesian closed category} has exponential objects such that morphisms $X\to Z^Y$ correspond to morphisms $X\times Y\to Z$; but how can we express this without referring to $\times$?
It turns out that the solution is the same.

We might also want to talk about operations that \emph{have} no obvious universal property, obviously violating~\eqref{princ:ump}.
For instance, what about monoidal categories?
In the usual presentation of a monoidal category, the tensor product $A\otimes B$ has no universal property.
It turns out that there is a way to give it a universal property, and this also leads us to higher-ary antecedents.

Finally, there is also another motivation not having anything to do with~\eqref{princ:independence} and~\eqref{princ:ump}: we may want to generalize the ``input data'' \cG from which we generate our free objects.
The unary type theory for categories with products allows us to prove theorems of the form ``in any category with products, for any morphisms $f:A\to B$, $g:C\to D$ \dots'', but it doesn't permit these quantifications to include ``for any $f:A\times B\to C$''.
This is because the type theory generates a free category-with-products from a directed graph \cG whose vertices and edges are the objects and morphisms we hypothesize; but there is no way to express $f:A\times B\to C$ as an edge of \cG, since \cG has no products of objects yet.
This problem turns out to have the same solution as well.

As already mentioned, on the type-theoretic side what we will do in this chapter is allow multiple types in the antecedent of a judgment (but still only one type in the consequent; in \ref{chap:polycats} we remove this restriction as well).
We call these \emph{simple type theories}; this term is somewhat more common in the literature, but perhaps not with the exact meaning we are giving it.
In a simple type theory the antecedent is often called the \emph{context}.

On the categorical side, what we will study are \emph{multicategories} of various sorts.
An ordinary multicategory is like a category but whose morphisms can have any finite list of objects as their domain, say $f:(A_1,\dots,A_n) \to B$, with a straightforward composition law.
There are many possible variations on this definition: in a symmetric multicategory the finite lists can be permuted, in a cartesian multicategory we can add unrelated objects and collapse equal ones, and so on.
All of these categorical structures are known as \emph{generalized multicategories}.
There is an abstract theory of generalized multicategories that includes these examples and many others, but (at least in the current version of this chapter) we will simply work with concrete examples.

Our approach to the semantics of simple type theory can be summed up in the following additional principle:
\begin{equation}\label{princ:structural}
  \parbox{4in}{\centering The shape of the context and the structural rules in a simple type theory, including admissible rules such as cut, should mirror the categorical structure of a generalized multicategory.}\tag{$\ddagger$}
\end{equation}
The \emph{structural rules} are the rules that don't refer to any operation on types, such as identity and cut.
(Later on we will meet other structural rules, such as exchange, contraction, and weakening.)
Principle~\eqref{princ:ump} then tells us that the \emph{non-structural} rules (which are sometimes called \emph{logical} rules) should all correspond to objects with universal properties in a generalized multicategory.

In sum, we have the following table of correspondences:
\begin{center}
  \begin{tabular}{c|c}
    Type theory & Category theory\\\hline
    Structural rules & Generalized multicategory\\
    Logical rules & Independent universal properties
  \end{tabular}
\end{center}
Here by ``independent universal properties'' I mean that the universal property of each object can be defined on its own without reference to any other objects defined by universal properties (unlike, for instance, the exponential in a cartesian closed category).

This all been very abstract, so I recommend the reader come back to this section again at the end of this chapter.
However, for completeness let me point out now that this general correspondence is particularly useful when designing new type theories and when looking for categorical semantics of existing type theories.
On one hand, any type theory that adheres to~\eqref{princ:independence} should have semantics in a kind of generalized multicategory that can be ``read off'' from the shape of its contexts and its structural rules.
On the other hand, to construct a type theory for a given categorical structure, we should seek to represent that structure as a generalized multicategory in which all the relevant objects have independent universal properties; then we can ``read off'' from the domains of morphisms in those multicategories the shape of the contexts and the structural rules of our desired type theory.

We will not attempt to make this correspondence precise in any general way, and in practice it has many tweaks and variations that would probably be exceptions to any putative general theorem; but it is a very useful heuristic.


\section{Introduction to multicategories}
\label{sec:multicats-catth}

[TODO: Define non-symmetric multicategories, multiposets, and representability.
Probably need to mention the ``$\circ_i$'' version of composition as well as the multicomposition.]


\section{Multiposets}
\label{sec:multiposets}

We begin our study of type theory for multicategories with the posetal case.
Thus, consider the adjunction between the category \bMPos of multiposets and the category \bRelMGr of \emph{relational multigraphs}, i.e.\ sets of objects equipped with an $n$-ary relation ``$(a_1,\dots,a_{n-1})\le b$'' for all integers $n\ge 1$.
We would like to construct the free multiposet on a relational multigraph \cG using a type theory.

Its objects, of course, will be those of \cG, so we do not yet need a type judgment.
We represent its relations using a judgment written ``$A_1,A_2,\dots,A_n \types B$''.
As is customary, we use capital Greek letters such as $\Gamma$ and $\Delta$ to stand for finite lists (possibly empty) of types.
We write ``$\Gamma_1,\Gamma_2,\dots,\Gamma_n$'' for the concatenation of such lists, and we also write for instance ``$\Gamma,A,\Delta$'' to indicate a list containing the type $A$ somewhere the middle.

At the moment, the only rules for this judgment will be identities and those coming from \cG.
Based on the lessons we learned from unary type theory, we represent the latter in Yoneda-style.
\begin{mathpar}
  \inferrule{ }{A\types A}\and
  \inferrule{(A_1,\dots,A_n \le B)\in\cG \\ \Gamma_1\types A_1 \\ \dots \\ \Gamma_n \types A_n}{\Gamma_1,\dots,\Gamma_n\types B}
\end{mathpar}
We call this the \textbf{cut-free type theory for multiposets under \cG}.
Note that we have chosen to use the ``multi-composition'' in Yoneda-ifying the relations in \cG; this will be necessary for admissibility of cut.
By contrast, it is traditional to formulate the cut rule in terms of the $\circ_i$ compositions.

\begin{thm}\label{thm:multiposet-cutadm}
  In the cut-free type theory for multiposets under \cG, the following cut rule is admissible: if we have derivations of $\Gamma\types A$ and of $\Delta,A,\Psi\types B$, then we can construct a derivation of $\Delta,\Gamma,\Psi\types B$.
\end{thm}
\begin{proof}
  We induct on the derivation of $\Delta,A,\Psi\types B$.
  If it is the identity rule, then $A=B$ and $\Delta$ and $\Psi$ are empty, so our given derivation of $\Gamma\types A$ is all we need.
  Otherwise, it comes from some relation $A_1,\dots,A_n \le B$ in \cG, where we have derivations of $\Gamma_i \types A_i$, and there is an $i$ such that $\Gamma_i = \Gamma_i',A,\Gamma_i''$, while $\Delta = \Gamma_1,\dots,\Gamma_{i-1},\Gamma_i'$ and $\Psi = \Gamma_i'',\Gamma_{i+1},\dots,\Gamma_n$.
  Now by the inductive hypothesis, we can construct a derivation of $\Gamma_i',\Gamma,\Gamma_i''\types A_i$.
  Applying the rule for $A_1,\dots,A_n \le B$ again, with this derivation in place of the original $\Gamma_i \types A_i$, gives the desired result.
\end{proof}

\begin{thm}\label{thm:multiposet-initial}
  For any relational multigraph \cG, the free multiposet it generates has the same objects, and the relation $(A_1,\dots,A_n)\le B$ holds just when the sequent $A_1,\dots,A_n\types B$ is derivable in the cut-free type theory for multiposets under \cG.
\end{thm}
\begin{proof}
  \cref{thm:multiposet-cutadm}, together with the identity rule, tells us that this defines a multiposet $\F\bMPos\cG$.
  If $\cM$ is any other multiposet with a map $P:\cG\to\cM$ of relational multigraphs, then since the objects of $\F\bMPos\cG$ are those of \cG, there is at most one extension of $P$ to $\F\bMPos\cG$.
  It suffices to check that the relations in $\F\bMPos\cG$ hold in $\cM$; but this is clear since $\cM$ is a multiposet and the only rules are an identity and a particular multi-transitivity.
\end{proof}

\section{Monoidal posets}
\label{sec:monpos}

We now augment the type theory for multiposets with operations representing a tensor product.
Since the tensor product now has a universal property, this is essentially straightforward.
First of all, we need a type judgment $\types A\type$, with unsurprising rules:
\begin{mathpar}
  \inferrule{A\in\cG}{\types A\type}\and
  \inferrule{ }{\types \one\type}\and
  \inferrule{\types A\type \\ \types B\type}{\types A\tensor B\type}
\end{mathpar}

Second, in addition to the rules from \cref{sec:multiposets}, we have rules for $\tensor$.
Once again we need to make a choice between sequent calculus and natural deduction.

\subsection{Sequent calculus}
\label{sec:seqcalc-monpos}

In sequent calculus the left rule expresses the universal property; while
the right rule should be the universal relation $A,B\types A\tensor B$, but we have to Yoneda-ify it using the multicomposition.
The rules for $\one$ are similar.
\begin{mathpar}
  \inferrule{\Gamma,A,B,\Delta\types C}{\Gamma,A\tensor B,\Delta\types C}\;\tensorL\and
  \inferrule{\Gamma\types A \\ \Delta\types B}{\Gamma,\Delta\types A\tensor B}\;\tensorR\and
  \inferrule{\Gamma,\Delta\types A}{\Gamma,\one,\Delta\types A}\;\one L\and
  \inferrule{ }{\types \one}\;\one R
\end{mathpar}
This defines the \textbf{sequent calculus for monoidal posets under \cG}.

\begin{thm}\label{thm:monpos-identity}
  The general identity rule is admissible in the sequent calculus for monoidal posets under \cG: if $\types A\type$ is derivable, then so is $A\types A$.
\end{thm}
\begin{proof}
  By induction on the derivation of $\types A\type$.
  If $A\in \cG$, then $A\types A$ is an axiom.
  If $A=\one$, then $\one\types \one$ has the following derivation:
  \begin{mathpar}
    \inferrule*[Right=$\one L$]{\inferrule*[Right=$\one R$]{ }{\types \one}}{\one\types \one}
  \end{mathpar}
  And if $A=B\tensor C$, by the inductive hypothesis we have derivations $\sD_B$ and $\sD_C$ of $B\types B$ and $C\types C$, which we can put together like so:
  \begin{equation*}
    \inferrule*[Right=$\tensorL$]{
      \inferrule*[Right=$\tensorR$]{
        \inferrule*{\sD_B\\\\\vdots}{B\types B}\\
        \inferrule*{\sD_C\\\\\vdots}{C\types C}
      }{
        B,C\types B\tensor C
      }
    }{
      B\tensor C\types B\tensor C
    }\qedhere
  \end{equation*}
\end{proof}

\begin{thm}\label{thm:monpos-cutadm}
  Cut is admissible in the sequent calculus for monoidal posets under \cG: if we have derivations of $\Gamma\types A$ and of $\Delta,A,\Psi\types B$, then we can construct a derivation of $\Delta,\Gamma,\Psi\types B$.
\end{thm}
\begin{proof}
  As always, we induct on the derivation of $\Delta,A,\Psi\types B$.
  The cases of the identity and of relations from \cG are just as in \cref{thm:multiposet-cutadm}
  It cannot end with a $\one R$.
  If it ends with a $\tensorR$, we use the inductive hypotheses on its premises and apply $\tensorR$ again.

  The cases when it ends with a left rule introduce a new feature that we have not seen before in cut-admissibility proofs.
  Suppose it ends with a $\one L$.
  If $A$ is the $\one$ that was introduced by this rule, then we proceed basically as before: if $\Gamma\types A$ is $\one R$, so that $\Gamma$ is empty, then we are in the principal case and we can simply use the given derivation of $\Delta,\Psi\types B$; while if it is a left rule then we can apply a secondary induction.
  But it might also happen that $A$ is a different type, appearing in $\Delta$ or $\Psi$.
  However, this case is also easily dealt with by applying the inductive hypothesis to $\Gamma\types A$ and the given $\Delta,\Psi\types B$ (with $A$ appearing somewhere in its antecedents).

  The case of $\tensorL$ is similar.
  In the interesting (principal) case, when the sequent $\Delta,A_1\tensor A_2,\Psi\types B$ is derived from $\Delta,A_1,A_2,\Psi\types B$, while $\Gamma\types A_1\tensor A_2$ is derived using $\tensorR$ from $\Gamma_1\types A_1$ and $\Gamma_2\types A_2$ (so that necessarily $\Gamma = \Gamma_1,\Gamma_2$), then we can apply the inductive hypothesis twice as follows:
  \begin{equation*}
    \inferrule*[Right=cut]{
      \Gamma_2\types A_2\\
      \inferrule*[Right=cut]{
        \Gamma_1\types A_1\\
        \Delta,A_1,A_2,\Psi\types B
      }{
        \Delta,\Gamma_1,A_2,\Psi\types B
      }
    }{
      \Delta,\Gamma_1,\Gamma_2,\Psi\types B
    }
  \end{equation*}
  We have to apply it twice because we are formulating our cut rule using $\circ_i$ composition.
  If we were using multicomposition then we would only need to apply it once; but the notation for the whole argument would become much less wieldy that way.
\end{proof}

\begin{thm}\label{thm:monpos-initial}
  For any relational multigraph \cG, the free monoidal poset generated by \cG is described by the sequent calculus for monoidal posets under \cG: its objects are the $A$ such that $\types A\type$ is derivable, while the relation $(A_1,\dots,A_n)\le B$ holds just when the sequent $A_1,\dots,A_n\types B$ is derivable.
\end{thm}
\begin{proof}
  As before, \cref{thm:monpos-identity,thm:monpos-cutadm} show that this defines a multiposet $\F\bMonPos\cG$.
  Moreover, the rules for $\tensor$ and $\one$ tell us that it is representable, hence monoidal.

  Now suppose $P:\cG\to\cM$ is a map into the underlying multiposet of any other monoidal poset.
  We can extend $P$ uniquely to a function from the objects of $\F\bMonPos\cG$ to those of $\cM$ preserving $\tensor$ and $\one$ on objects, so it remains to check that this is a map of multiposets and preserves the universal properties of $\tensor$ and $\one$.
  However, $\tensorR$ and $\one R$ are preserved by the universal data of $\tensor$ and $\one$ in \cM, while the universal properties of these data mean that $\tensorL$ and $\one L$ are also preserved.
\end{proof}

\subsection{Natural deduction}
\label{sec:natded-monpos}

In natural deduction, the introduction rules $\tensorI$ and $\one I$ will coincide with the right rules $\tensorR$ and $\one R$ from the sequent calculus, but now we need elimination rules.
Since $\tensor$ and $\one$ in a multicategory have a ``mapping out'' universal property, these elimination rules will be reminiscent of the ``case analysis'' rules from \cref{sec:catcoprod}.
Formally speaking, they can be obtained by simply cutting $\tensorL$ and $\one L$ with an arbitrary sequent:
\begin{mathpar}
  \inferrule*[Right=cut]{
    \Psi \types A\tensor B \\
    \inferrule*[Right=$\tensorL$]{\Gamma,A,B,\Delta\types C}{\Gamma,A\tensor B,\Delta\types C}
  }{
    \Gamma,\Psi,\Delta \types C
  }\and
  \inferrule*[Right=cut]{
    \Psi\types \one \\
    \inferrule*[Right=$\one L$]{\Gamma,\Delta\types A}{\Gamma,\one,\Delta\types A}
  }{
    \Gamma,\Psi,\Delta\types C
  }
\end{mathpar}
As usual in a natural deduction, we also need to assert the identity rule for all types.
Thus our complete \textbf{natural deduction for monoidal posets under \cG} consists of (the rules for $\types A\type$ and):
\begin{mathpar}
  \inferrule{\types A\type}{A\types A}\and
  \inferrule{(A_1,\dots,A_n \le B)\in\cG \\ \Gamma_1\types A_1 \\ \dots \\ \Gamma_n \types A_n}{\Gamma_1,\dots,\Gamma_n\types B}\and
  \inferrule{\Gamma\types A \\ \Delta\types B}{\Gamma,\Delta\types A\tensor B}\;\tensorI\and
  \inferrule{
    \Psi \types A\tensor B \\
    \Gamma,A,B,\Delta\types C
  }{
    \Gamma,\Psi,\Delta \types C
  }\;\tensorE\\
  \inferrule{ }{\types \one}\;\one I\and
  \inferrule{
    \Psi\types \one \\
    \Gamma,\Delta\types A
  }{
    \Gamma,\Psi,\Delta\types C
  }\;\one E
\end{mathpar}
We leave the metatheory of this as an exercise (\cref{ex:natded-monpos}).

\subsection*{Exercises}

\begin{ex}\label{ex:natded-monpos}
  Prove the well-formedness, cut-admissibility, and initiality theorems for the natural deduction for monoidal posets.
\end{ex}

\begin{ex}\label{ex:monpos-mslat}
  Write down either a sequent calculus or a natural deduction for monoidal posets that are also meet-semilattices, and prove its initiality theorem.
\end{ex}

\begin{ex}\label{ex:monpos-jslat}
  Let us augment the sequent calculus for monoidal posets by the following versions of the rules for join-semilattices:
  \begin{mathpar}
    \inferrule{\types A\type \\ \types B\type}{\types A\join B\type}\and
    \inferrule{ }{\types \bot\type}\and
    \inferrule{\Gamma \types A}{\Gamma\types A\join B}\and
    \inferrule{\Gamma \types B}{\Gamma\types A\join B}\and
    \inferrule{\Gamma,A,\Delta \types C\\\Gamma,B,\Delta \types C}{\Gamma,A\join B,\Delta\types C}\and
    \inferrule{ }{\Gamma,\bot,\Delta\types C}
  \end{mathpar}
  \begin{enumerate}
  \item Construct derivations in this calculus of the following sequents:
    \begin{align*}
      (A\tensor B)\join (A\tensor C) &\types  A\tensor (B\join C)\\
      A\tensor (B\join C) &\types (A\tensor B)\join (A\tensor C)
    \end{align*}
  \item What categorical structure do you think models this type theory?
  \end{enumerate}
\end{ex}


\section{Multicategories and operads}
\label{sec:multicat-operads}

Now we are ready to move back up to categories, distinguishing between derivations by introducing a term calculus.
Categorically, we begin with the adjunction between the category $\bMCat$ of multicategories and the category $\bMGr$ of multigraphs.
Let \cG be a multigraph; we now augment the cut-free theory of \cref{sec:multiposets} with terms that exactly represent the structure of derivations, as we did in \cref{sec:categories,sec:catprod,sec:catcoprod}.
\begin{mathpar}
  \inferrule{A\in\cG}{x:A\types x:A}\and
  \inferrule{f\in \cG(A_1,\dots,A_n;B) \\ \Gamma_1\types M_1:A_1 \\ \dots \\ \Gamma_n \types M_n:A_n}{\Gamma_1,\dots,\Gamma_n\types f(M_1,\dots,M_n):B}\and
\end{mathpar}
We call this the \textbf{cut-free type theory for multicategories under \cG}.

\begin{thm}\label{thm:multicat-subadm}
  Substitution is admissible in the cut-free type theory for multicategories under \cG: given derivations of $\Gamma\types M:A$ and of $\Delta,x:A,\Psi\types N:B$, we can construct a derivation of $\Delta,\Gamma,\Psi\types M[N/x]:B$.
\end{thm}
\begin{proof}
  Just like \cref{thm:multiposet-cutadm}.
  As before, note that we can regard this as defining substitution.
\end{proof}

[TODO: Substitution is associative and interchanging.  Where should we deal with that the first time?]

\begin{thm}\label{thm:multicat-initial}
  For any multigraph \cG, the free multicategory generated by \cG can be described by the cut-free type theory for multicategories under \cG: its objects are those of \cG, and its morphisms $\Gamma\to B$ are the derivable judgments $\Gamma\types M:B$.
\end{thm}
\begin{proof}
  \cref{thm:multicat-subadm} tells us how to compose multimorphisms.
  [TODO]
\end{proof}


\section{Monoidal categories}
\label{sec:moncat}

Since the tensor product has a ``mapping out'' universal property like that of a coproduct, its elimination rules will be a sort of ``case analysis'' that decomposes an element of $A\tensor B$ into an element of $A$ and an element of $B$.
\begin{mathpar}
  \inferrule{\types A\type}{x:A\types x:A}\and
  \inferrule{f\in \cG(A_1,\dots,A_n;B) \\ \Gamma_1\types M_1:A_1 \\ \dots \\ \Gamma_n \types M_n:A_n}{\Gamma_1,\dots,\Gamma_n\types f(M_1,\dots,M_n):B}\and
  \inferrule{\Gamma\types M:A \\ \Delta\types N:B}{\Gamma,\Delta\types \pair{M}{N}:A\tensor B}\;\tensorI\and
  \inferrule{
    \Psi \types M:A\tensor B \\
    \Gamma,x:A,y:B,\Delta\types N:C
  }{
    \Gamma,\Psi,\Delta \types (\flet \pair x y:=M in N):C
  }\;\tensorE\\
  \inferrule{ }{\types \ttt:\one}\;\one I\and
  \inferrule{
    \Psi\types M:\one \\
    \Gamma,\Delta\types N:A
  }{
    \Gamma,\Psi,\Delta\types (\discard M in N):C
  }\;\one E
\end{mathpar}

[TODO: Try to explain the let-binding better.]



\section{Symmetric monoidal categories}
\label{sec:symmoncat}

[Symmetric multicategories and their type theory, exchange rule, structural rules in general]


\section{Introduction to cartesian multicategories}
\label{sec:cartmulti}

[Define cartesian multicategories, cartesian multiposets, representability, products have both universal properties]


\section{Cartesian monoidal categories}
\label{sec:cartmoncat}

[Not sure exactly where the section boundaries should be here, or how to mix introducing the categorical structures with the type theories.
Where to talk about distributive categories and distributive lattices?]


\section{Algebraic theories and operads}
\label{sec:algthy-opd}

[A multigraph is also known as a signature of function symbols.  An arbitrary set of equations between terms can be added to $\equiv$, thereby describing a ``presented'' rather than a free categorical structure.  For cartesian monoidal categories, this is a (multi-sorted) algebraic theory.  For (symmetric) monoidal categories, it is a syntactic approach to (colored) operads.]


\section{Closed monoidal categories}
\label{sec:clmoncat}


\section{Intuitionistic propositional logic}
\label{sec:int-logic}


\chapter[Polycategories]{Classical logic, linear logic, polycategories, and PROPs}
\label{chap:polycats}


\section{PROPs and symmetric monoidal posets}
\label{sec:prop-smpos}

In this section we consider sequents $\Gamma\types\Delta$ where the commas on both sides are intended to represent the \emph{same} tensor product $\tensor$.
For simplicity, we assume this tensor product is symmetric, so that we have an exchange rule on both sides.
This leads us to consider different identity and cut rules (in fact, two identity rules):
\begin{mathpar}
  \inferrule{ }{\quad\types\quad}\and
  \inferrule{\Gamma\types\Delta}{\Gamma,A\types \Delta,A}\and
  \inferrule*[Right=cut]{\Gamma\types\Xi,\Psi \\ \Psi,\Phi \types \Delta}{\Gamma,\Phi \types \Delta,\Xi}
\end{mathpar}
As always, of course, we intend for the cut rule to be admissible, but writing down at this point what it should be helps us build it into other rules appropriately.
For instance, the Yoneda-ified rule for the generating morphisms from a polygraph should be
\begin{mathpar}
  \inferrule{\Gamma\types\Xi,\Psi \\ f\in\cG(\Psi,\Phi; \Delta)}{\Gamma,\Phi \types \Delta,\Xi}
\end{mathpar}
With only this and the identity rule, we have the \textbf{cut-free type theory for PROPs under \cG}.

\begin{thm}\label{thm:prop-cutadm}
  For any polygraph \cG, the above cut rule is admissible in the cut-free type theory for PROPs under \cG.
\end{thm}
\begin{proof}
  As always, we induct on the derivation of $\Psi,\Phi \types \Delta$.
  \begin{enumerate}
  \item If it is the empty identity rule ``$\types$'', then $\Gamma\types\Xi,\Psi$ is just $\Gamma\types\Xi$ and is also the desired conclusion.
  \item If it ends with the other identity rule, then there are two cases.
    \begin{enumerate}
    \item If $A$ is in $\Phi$, then we have $\Phi=\Phi',A$ and $\Delta=\Delta',A$ and a derivation of $\Psi,\Phi'\types\Delta'$.
      Applying the inductive hypothesis to this we get $\Gamma,\Phi'\types \Xi,\Delta'$, and then applying the identity rule again gives the desired conclusion.
    \item If $A$ is in $\Psi$, then we have $\Psi=\Psi',A$ and $\Delta=\Delta',A$ and a derivation of $\Psi',\Phi\types\Delta'$.
      Now we can inductively cut this with the given $\Gamma\types \Xi,A,\Psi'$ along $\Psi'$ to get $\Gamma,\Phi\types \Xi,A,\Delta'$ which is the desired conclusion.
    \end{enumerate}
  \item Finally, suppose $\Psi,\Phi \types \Delta$ ends with the rule for an $f$.
    Thus, we have $\Delta=\Delta_1,\Delta_2$ and $\Psi=\Psi_1,\Psi_2$ and $\Phi=\Phi_1,\Phi_2$, where $f\in\cG(\Psi_2,\Phi_2,\Upsilon;\Delta_1)$ and a given derivation of $\Psi_1,\Phi_1\types \Delta_2,\Upsilon$.
    We first inductively cut the latter with our given $\Gamma\types \Xi,\Psi_1,\Psi_2$ along $\Psi_1$ to get $\Gamma,\Phi_1\types \Xi,\Psi_2,\Delta_2,\Upsilon$, then apply the $f$ rule on $\Psi_2,\Upsilon$ to get the desired $\Gamma,\Phi_1,\Phi_2\types \Xi,\Delta_1,\Delta_2$ as desired.\qedhere
  \end{enumerate}
\end{proof}

To prove an initiality theorem, we need an appropriate categorical structure.
We define a \textbf{PROP} to be a symmetric polygraph \cP together with the following data:
\begin{enumerate}
\item A morphism $\idfunc_{()}:()\to ()$.
\item For every $A\in\cP$, a morphism $\idfunc_A (A)\to (A)$.
  % For every $f:\Gamma\to\Delta$ and object $A$, a morphism $(f,\idfunc_A):(\Gamma,A) \to (\Delta,A)$.
  % By induction from this and the previous, we have $\idfunc_{\Gamma} :\Gamma\to\Gamma$ for any $\Gamma$.
\item For every $f:\Gamma\to (\Xi,\Psi)$ and $g:(\Psi,\Phi)\to \Delta$, a composite $g\circ_\Psi f : (\Gamma,\Phi) \to (\Xi,\Delta)$.
% \item Identities are invariant under permutation: $\sigma\idfunc_{\Gamma}\sigma = \idfunc_{\sigma\Gamma}$.
\item Composition is invariant under permutation: $\tau(g\circ_\Psi f)\sigma = (\tau g)\circ_\Psi (f\sigma)$ and $g\sigma \circ_\Psi f = g\circ_{\sigma \Psi} \sigma f$.
\item Composition is unital:
  % $g\circ_\Psi \idfunc_\Psi = g$ and $\idfunc_\Psi\circ_\Psi f = f$.
  $g\circ_A \idfunc_A = g$ and $\idfunc_A\circ_A f = f$.
\item Composition is associative: given $f:\Gamma\to (\Xi,\Theta,\Psi)$ and $g:(\Psi,\Phi)\to (\Delta,\Upsilon)$ and $h:(\Upsilon,\Theta,\Lambda)\to \Omega$, we have
  \[h \circ_{\Upsilon,\Theta} (g\circ_\Psi f) = (h\circ_\Upsilon g) \circ_{\Theta,\Psi} f \qquad \text{(as morphisms} (\Gamma,\Phi,\Lambda) \to (\Xi,\Delta,\Omega). \]
\item Composition is interchanging: $g\circ_{()}f = f\circ_{()}g$ (modulo a symmetry action).
\end{enumerate}
Note that by composing along empty lists, we can produce identity morphisms for lists: $\idfunc_{(A,B)} = \idfunc_A \circ_{()} \idfunc_B$ and so on.
(The order doesn't matter by the interchange axiom.)
It may not be obvious that these axioms are correct.
For now, however, we restrict to the posetal case, in which case the axioms are irrelevant.

Now there are actually \emph{three} ways we could imagine introducing tensor products in a PROP.
Fortunately, they are all equivalent.

\begin{thm}\label{thm:prop-tensor}
  For any two objects $A$ and $B$ in a PROP \cP, the following are equivalent.
  \begin{enumerate}
  \item A morphism $(A,B) \to A\tensor B$, precomposition with which defines bijections\label{item:prop-tensor-left}
    \[ \cP(\Gamma,A\tensor B;\Delta) \to \cP(\Gamma,A,B;\Delta) \]
  \item A morphism $A\tensor B \to (A,B)$, postcomposition with which defines bijections\label{item:prop-tensor-right}
    \[ \cP(\Gamma;\Delta,A\tensor B) \to \cP(\Gamma;\Delta,A,B) \]
  \item Morphisms $(A,B) \to A\tensor B$ and $A\tensor B \to (A,B)$ whose composites in both directions
    \begin{gather*}
      A\tensor B \too (A,B)  \too A\tensor B\\
      (A,B)\too A\tensor B \too (A,B)
    \end{gather*}
    are identities.\label{item:prop-tensor-iso}
  \end{enumerate}
\end{thm}
\begin{proof}
  If we have~\ref{item:prop-tensor-iso}, then pre- or post-composing with the other morphism yields the bijections in~\ref{item:prop-tensor-left} and~\ref{item:prop-tensor-right}.
  On the other hand, given~\ref{item:prop-tensor-left}, the identity $(A,B)\to (A,B)$ must be the composite $(A,B)\to A\tensor B \to (A,B)$ for some unique morphism $A\tensor B \to (A,B)$, and the other composite must be the identity since its precomposite with $(A,B)\to A\tensor B$ is $(A,B)\to A\tensor B$; so~\ref{item:prop-tensor-iso} holds.
  (This is basically the Yoneda lemma.)
  The case of~\ref{item:prop-tensor-right} is dual.
\end{proof}

The case with units is similar.

\begin{thm}\label{thm:prop-unit}
  In a PROP \cP, the following are equivalent.
  \begin{enumerate}
  \item A morphism $()\to \one$, precomposition with which defines bijections $\cP(\Gamma,\one;\Delta) \to \cP(\Gamma;\Delta)$.
  \item A morphism $\one\to()$, postcomposition with which defines bijections $\cP(\Gamma;\Delta,\one) \to \cP(\Gamma;\Delta)$.
  \item Morphisms $()\to \one$ and $\one\to()$ whose composites in both directions are identities.\qed
  \end{enumerate}
\end{thm}

\begin{thm}\label{thm:prop-smc}
  There is an equivalence between (1) symmetric monoidal categories and (2) PROPs with tensors and units in the sense of \cref{thm:prop-tensor,thm:prop-unit}.
\end{thm}
\begin{proof}
  Given a symmetric monoidal category \cC, we define a prop \cP by
  \[ \cP(A_1,\dots,A_n ; B_1,\dots,B_m) = \cC(A_1\tensor \cdots\tensor A_n, B_1\tensor\cdots\tensor B_m)\]
  or, more succinctly, $\cP(\Gamma;\Delta) = \cC(\bigtensor\Gamma, \bigtensor\Delta)$.
  Of course, $\bigtensor() = \one$.
  The identity rules come from $\idfunc_\one$ and $f\tensor \idfunc_A$, while the composite of $f:\Gamma\to (\Xi,\Psi)$ and $g:(\Psi,\Phi)\to \Delta$ is $(\bigtensor\Xi \tensor g) \circ (f\tensor \bigtensor \Phi)$.
  Of course, this PROP has tensors and units quite trivially.

  Conversely, given a PROP with tensors and units, we have an underlying category with an object $\one$ and an operation $\tensor$.
  The functoriality of $\tensor$ is defined on $f:A\to A'$ and $g:B\to B'$ as the composite
  \[ A\tensor B \too (A,B) \xto{(f,\idfunc)} (A',B) \xto{(\idfunc,g)} (A',B') \too A'\tensor B'. \]
  The axioms of a PROP ensure that this is the same as if we put $f$ and $g$ in the other order (or we could write simply $(f,g)$ using the yet-to-be-defined polycomposition).
  Functoriality is similarly easy.
  For associativity, we have
  \[ ((A\tensor B)\tensor C) \too (A\tensor B,C) \too (A,B,C) \too (A,B\tensor C) \to (A\tensor (B\tensor C)) \]
  and for unitality we have
  \[ (A\tensor \unit) \too (A,\unit) \too (A) \]
  and dually.
  It is straightforward to check that these are natural isomorphisms and satisfy the necessary axioms.
  For symmetry, we use the fact that our PROP is a \emph{symmetric} polygraph and act by symmetry on the morphisms $(A,B)\to A\tensor B$ and $A\tensor B\to (A,B)$, obtaining ``inverse isomorphisms'' $(B,A) \to A\tensor B$ and $A\tensor B \to (B,A)$.
  From these we can show $A\tensor B \cong B\tensor A$ and check naturality and the axioms.

  Finally, it is straightforward to check that these two constructions are inverses.
\end{proof}

Now, \cref{thm:prop-tensor,thm:prop-unit} give several ways to introduce $\tensor$ and $\unit$ in our type theory.
For a sequent calculus, the symmetrical approach is probably the most natural:
\begin{mathpar}
  \inferrule{\Gamma,A,B\types \Delta}{\Gamma,A\tensor B\types \Delta}\;\tensorL\and
  \inferrule{\Gamma\types \Delta,A,B}{\Gamma\types \Delta,A\tensor B}\;\tensorR\and
  \inferrule{\Gamma\types\Delta}{\Gamma,\one\types\Delta}\;\one L\and
  \inferrule{\Gamma\types\Delta}{\Gamma\types\Delta,\one}\;\one R\and
\end{mathpar}
Of course, we also include the identity and generator rules (the former only for base types coming from \cG) and the type judgment:
\begin{mathpar}
  \inferrule{ }{()\types()}\and
  \inferrule{\Gamma\types\Delta\\ A\in\cG}{\Gamma,A\types \Delta,A}\and
  \inferrule{\Gamma\types\Xi,\Psi \\ f\in\cG(\Psi,\Phi; \Delta)}{\Gamma,\Phi \types \Delta,\Xi}\and
  \inferrule{A\in\cG}{\types A\type}\and
  \inferrule{ }{\types \one\type}\and
  \inferrule{\types A\type \\ \types B\type}{\types A\tensor B\type}
\end{mathpar}
We call this the \textbf{classical sequent calculus for symmetric monoidal posets under \cG}.
(Posets because we are not yet distinguishing derivations or introducing terms.)

\begin{thm}\label{thm:seqcalc-smpos-invertible}
  All the rules for $\tensor$ and $\one$ in the classical sequent calculus for symmetric monoidal posets under \cG are invertible.
\end{thm}
\begin{proof}
  Suppose we have a derivation of $\Gamma,A\tensor B\types \Delta$; we want a derivation of $\Gamma,A,B\types \Delta$.
  We induct on the given derivation.
  \begin{enumerate}
  \item The easy case is when it ends with $\tensorL$ whose premise is what we want.
  \item If it ends with $\tensorL$ acting on another type, or any of $\tensorR$, $\one L$, or $\one R$, then we induct on the premise and then apply that rule to the result.
  \item If it ends with the identity rule for $C$, then since $C\in \cG$ it can't be $A\tensor B$, so we can again induct on the premise and apply the identity rule afterwards.
  \item Finally, if it ends with a generator rule for $f$, then since the domain of $f$ is a list of objects of $\cG$, none of them can be $A\tensor B$; so $A\tensor B$ must be in the sequent that the generator rule cuts with, and we can induct on that and then apply the generator rule.
  \end{enumerate}
  The other rules $\tensorR$, $\one L$, and $\one R$ are similar.
\end{proof}

\begin{thm}\label{thm:seqcalc-smpos-identity}
  The following general identity rule is admissible in the classical sequent calculus for symmetric monoidal posets under \cG:
  \begin{mathpar}
    \inferrule{\Gamma\types\Delta\\ \types A\type}{\Gamma,A\types \Delta,A}\and
  \end{mathpar}
  In particular, since $()\types()$, we have $A\types A$ whenever $\types A\type$.
\end{thm}
\begin{proof}
  We induct on the derivation of $\types A\type$.
  \begin{enumerate}
  \item If it is from \cG, we apply the postulated identity rule.
  \item If it is $\one$, we have the following derivation:
    \begin{mathpar}
      \inferrule*{\inferrule*{\inferrule*{ }{\quad\types\quad}}{\quad \types \one}}{\one\types\one}
    \end{mathpar}
  \item If it is $A_1\tensor A_2$, we have by induction a derivation $\sD_1$ of $\Gamma,A_1\types,\Delta,A_1$, and then (again by induction) a derivation $\sD_2$ of $\Gamma,A_1,A_2\types \Delta,A_1,A_2$.
    Now we augment this in a similar way:
    \begin{equation*}
      \inferrule*{
        \inferrule*{
          \inferrule*{\sD_2\\\\\vdots}{\Gamma,A_1,A_2\types \Delta,A_1,A_2}
        }{
          \Gamma,A_1\tensor A_2\types \Delta,A_1,A_2
        }}{
        \Gamma,A_1\tensor A_2\types \Delta,A_1\tensor A_2
      }\qedhere
    \end{equation*}
  \end{enumerate}
\end{proof}

\begin{thm}\label{thm:seqcalc-smpos-cutadm}
  The PROP cut rule is admissible in the classical sequent calculus for symmetric monoidal posets under \cG.
\end{thm}
\begin{proof}
  Suppose given derivations of $\Gamma\types\Xi,\Psi$ and $\Psi,\Phi \types \Delta$; we want to derive $\Gamma,\Phi \types \Delta,\Xi$.
  We induct on the derivation of $\Psi,\Phi \types \Delta$.
  \begin{enumerate}
  \item If it ends with an identity or a generator, we do as in \cref{thm:prop-cutadm}.
  \item If it ends with $\tensorR$ or $\one R$, we can simply apply the inductive hypothesis to cut with the premise of that rule.
  \item If it ends with $\tensorL$, there are two cases.
    If the introduced $A\tensor B$ is in $\Phi$, then we can inductively cut the premise along $\Psi$ again.
  \item On the other hand, if the introduced $A\tensor B$ is in $\Psi$, then our other derivation has the form $\Gamma\types\Xi,\Psi',A\tensor B$.
    Since $\tensorR$ is invertible by \cref{thm:seqcalc-smpos-invertible}, we can also derive $\Gamma\types\Xi,\Psi',A,B$, and then cut with the premise $\Psi',A,B,\Phi \types \Delta$ of our $\tensorL$.
  \item The case of $\one L$ is similar.\qedhere
  \end{enumerate}
\end{proof}

\begin{thm}\label{thm:seqcalc-smpos-initial}
  For any symmetric relational polygraph \cG, the free symmetric monoidal poset generated by \cG is described by the classical sequent calculus for symmetric monoidal posets under \cG.
\end{thm}
\begin{proof}
  \cref{thm:seqcalc-smpos-identity,thm:seqcalc-smpos-cutadm} imply in particular that the sequent calculus gives us a PROP-poset $\F\bSMPos\cG$.
  Moreover, the rules for $\tensor$ and $\one$ give $\F\bSMPos\cG$ the tensor and unit structure of \cref{thm:prop-tensor,thm:prop-unit}, so that it is a symmetric monoidal poset.
  Now if \cM is any other symmetric monoidal poset and $P:\cG\to\cM$ a map of symmetric relational polygraphs, there is a unique extension of $P$ to the objects of $\F\bSMPos\cG$.
  As usual, by induction on the rules of the sequent calculus and the fact that \cM is a PROP-poset, this extension preserves the relations $\Gamma\le\Delta$.
  Finally, it also preserves the tensor and unit structure, so it is a strict functor of symmetric monoidal posets.
\end{proof}


\section{Symmetric monoidal categories}
\label{sec:prop-smc}

So much for symmetric monoidal posets.
What we really want, however, is a type theory for symmetric monoidal \emph{categories}, in which we can talk about equality of morphisms, and that can also deal with tensors in the codomain.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{A\in\cG}{\types A\type}\and
    \inferrule{ }{\types \one\type}\and
    \inferrule{\types A\type \\ \types B\type}{\types A\tensor B\type}
    \inferrule{ }{()\types()}\and
    \inferrule{\Gamma\types\Delta\\ \types A\type}{\Gamma,A\types \Delta,A}\and
    \inferrule{\Gamma\types\Xi,\Psi \\ f\in\cG(\Psi; \Delta)}{\Gamma \types \Delta,\Xi}\and
    \inferrule{\Gamma\types \Delta,A,B}{\Gamma\types \Delta,A\tensor B}\;\tensorI\and
    % \inferrule{\Xi\types\Phi,\Psi,A\tensor B \\ \Gamma,\Psi,A,B\types \Delta}{\Gamma,\Xi\types \Phi,\Delta}\;\tensorE\and
    \inferrule{\Gamma\types \Delta,A\tensor B}{\Gamma\types \Delta,A,B}\;\tensorE\and
    \inferrule{\Gamma\types\Delta}{\Gamma\types\Delta,\one}\;\one I\and
    % \inferrule{\Xi\types \Phi,\Psi,\one \\ \Gamma,\Psi\types\Delta}{\Gamma,\Xi\types\Phi,\Delta}\;\one E\and
    \inferrule{\Gamma\types\Delta,\one}{\Gamma\types\Delta}\;\one E\and
  \end{mathpar}
  \caption{Classical natural deduction for symmetric monoidal categories}
  \label{fig:clnatded-smc}
\end{figure}

We begin, of course, by reformulating the sequent calculus from the last section as a natural deduction, as shown in \cref{fig:clnatded-smc}.
As usual, the terms for this theory will contain $\beta$- and $\eta$-redexes, so that we need a $\equiv$ relation.
However, there is now also another class of pairs of distinct derivations that we want to identify.
A simple case is when we have $f\in \cG(A;B)$ and $g\in\cG(C;D)$; then there are two distinct derivations of $f\tensor g$:
\begin{mathpar}
  \inferrule*[Right=$g$]{
    \inferrule*{
      \inferrule*[Right=$f$]{
        \inferrule*{\inferrule*{ }{()\types()}}{A\types A}
      }{
        A\types B
      }}{
      A,C\types B,C
    }}{
    A,C\types B,D
  }\and
  \inferrule*[Right=$f$]{
    \inferrule*{
      \inferrule*[Right=$g$]{
        \inferrule*{\inferrule*{ }{()\types()}}{C\types C}
      }{
        C\types D
      }}{
      A,C\types A,D
    }}{
    A,C\types B,D
  }\and
\end{mathpar}
If we write down a term calculus whose terms correspond exactly to derivations, as we usually do, then the desired equality between these two derivations would look something like
\[ x:A, y:C \types (f,\idfunc)((\idfunc,g)(x,y)) \equiv (\idfunc,g)((f,\idfunc)(x,y)) : (B,D) \]
Note that unlike the $\beta$- and $\eta$-reductions, this equality is not directional: it makes no sense to regard one or the other side as ``simpler'' or ``more canonical'' than the other.
Moreover, the terms like $(f,\idfunc)((\idfunc,g)(x,y))$ are fairly ugly as well.

Thus, for our multi-output type theory of symmetric monoidal categories, we will break the bijection between terms and deductions, in a way that enables us to represent the above two derivations by \emph{literally the same} term (or tuple of terms), namely
\[ x:A, y:C \types (f(x),g(y)):(B,D) \]
This makes the metatheory and the initiality theorem rather more complicated, but at the end of the day it leads to a much more congenial type theory.
A nice example is the study of antipodes in noncommutative bimonoids that we discussed informally in \cref{sec:intro}.

The intuition in this notation is of course that $f(x):B$ and $g(y):D$.
We could write it as ``$f(x):B,g(y):D$'', but we choose to tuple the terms up as in $(f(x),g(y)):(B,D)$ for a couple of reasons.
The first reason is that when doing equational reasoning (such as for the antipode calculation), the equalities generally relate entire tuples rather than single terms.
The second reason is that in general, we also need to include some ``terms without a type'' (e.g.\ coming from morphisms with empty codomain $()$, which is a judgmental representation of the unit object), and this looks a little nicer when all the terms are grouped together: we write for instance $(f(x),g(y)\mid h(z))$ to mean that $h(z):()$.

There are also, of course, function symbols with \emph{multiple} outputs.
To deal with this case we write $f_1(x)$, $f_2(x)$, and so on for the terms corresponding to all the types in the codomain.
For example, we write the composite of $f:(A,B) \to (C,D)$ with $g:(E,D)\to (F,G)$ as
\[ x:A, y:B, z:E \types (f_1(x,y),g_1(z,f_2(x,y)),g_2(z,f_2(x,y))):(C,F,G) \]
This does require one further technical device (that will be almost invisible in practice).
Suppose we have $f:()\to (B,C)$, written in type theory as $()\types (f_1,f_2):(B,C)$, and we compose/tensor it with itself to get a morphism $() \to (B,B,C,C)$.
We would na\"ively write this as $() \types (f_1,f_1,f_2,f_2)$, but this is ambiguous since we can't tell which $f_1$ matches which $f_2$.
We disambiguate the possibilities by writing $() \types (f_1,f'_1,f_2,f'_2)$ or $() \types (f_1,f'_1,f'_2,f_2)$.
Although this issue seems to only arise for morphisms with empty domain and greater than unary codomain, for consistency we formulate the syntax with a label (like $'$) on \emph{every} term former, and simply omit them informally when there is no risk of ambiguity.
We assume given an infinite alphabet of symbols \fA for this purpose (such as $','',''',\dots$, or $1,2,3,\dots$).

Now, if our terms are not simply representations of derivations, then we need to explain what terms \emph{are} before we explain what they mean.
For this purpose we define the class of \emph{pre-terms}, which have a type and a context but may not respect the linearity of the inputs.
The pre-terms, being syntax, are freely generated in an appropriate sense, so we can describe them using an auxiliary judgment $\Gamma \types^\fL M\pc A$.
The intent is that we will eventually judge $\Gamma \types (M_1,\dots,M_n):(A_1,\dots,A_n)$ where each $M_i$ is a pre-term such that $\Gamma \types M_i\pc A_i$.
We will also need to include pre-terms with no type at all, so we also include a judgment $\Gamma \types^\fL M\pc ()$ for this purpose.
In both cases the annotation \fL is a finite subset of \fA indicating which labels might have been used in the terms $M_i,N_i$, to avoid duplication.

The rules for these judgments are shown in \cref{fig:pre-terms}.
It may be tempting to require in the rules for $f$ and $\pi_i$ that all label sets $\fL_i$ in the premises should be disjoint, but this is \emph{not} correct.
For instance, if $f:A\to (B,C)$ and $g:(B,C) \to D$, the composite $g \circ_{(B,C)} f$ will be represented by the pre-term
\begin{mathpar}
  \inferrule*{
    x:A \types^{\fa} f^\fa_1(x)\pc B \\
    x:A \types^{\fa} f^\fa_2(x)\pc C \\
    g\in \cG(B,C;D)\\
    \fb\notin\{\fa\}
  }{
    x:A \types^{\{\fa,\fb\}} g^\fb(f^\fa_1(x),f^\fa_2(x)):D
  }
\end{mathpar}
in which the label \fa appears in multiple premises.
All that matters is that the \emph{new} label being introduced to mark $g$ has not been used before.

We allow an arbitrary finite set of labels in the ``variable'' and $\ttt$ rules to ensure that the pre-term judgment is closed under adding finitely many more unused labels.
(This can be proved by an easy induction.)
This makes some things more convenient to state, and doesn't matter because \fA is infinite so there will always be fresh labels available.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\fL\finsubset\fA \\ \fa\notin\fL}{\Gamma,x:A \types^{\fL\cup\{\fa\}} x^\fa\pc A}\and
    \inferrule{
      \Gamma \types^{\fL_1} M_1\pc A_1 \\ \Gamma\types^{\fL_2} M_2\pc A_2 \\ \cdots \\ \Gamma\types^{\fL_n} M_n\pc A_n \\\\
      f\in \cG(A_1,\dots,A_n; B_1,\dots,B_m)\\ 1\le j\le m \\ \forall i(\fa \notin \fL_i)
    }{
      \Gamma \types^{\fL_1\cup\cdots\cup\fL_n \cup \{\fa\}} f_j^\fa(M_1,\dots,M_n)\pc B_j
    }\and
    \inferrule{
      \Gamma \types^{\fL_1} M_1\pc A_1 \\ \Gamma\types^{\fL_2} M_2\pc A_2 \\ \cdots \\ \Gamma\types^{\fL_n} M_n\pc A_n \\\\
      f\in \cG(A_1,\dots,A_n; \cdot ) \\ \forall i(\fa \notin \fL_i)
    }{
      \Gamma \types^{\fL_1\cup\cdots\cup\fL_n \cup \{\fa\}} f^\fa(M_1,\dots,M_n)\pc ()
    }\and
    \inferrule{
      \Gamma\types^{\fL_1} M\pc A \\ \Gamma\types^{\fL_2} N\pc B \\
      \fa\notin \fL_1\cup\fL_2
    }{
      \Gamma\types^{\fL_1\cup\fL_2\cup\{\fa\}} \pair{M}{N}^\fa \pc A\tensor B
    }\and
    \inferrule{\fL \finsubset \fA \\ \fa\notin\fL}{\Gamma\types^{\fL\cup\{\fa\}} \ttt^\fa\pc\unit}\and
    \inferrule{\Gamma\types^\fL M\pc A\tensor B \\ \fa\notin\fL}{\Gamma\types^{\fL\cup\{\fa\}} \pi_1^\fa(M) \pc A}\and
    \inferrule{\Gamma\types^\fL M\pc A\tensor B \\ \fa\notin\fL}{\Gamma\types^{\fL\cup\{\fa\}} \pi_2^\fa(M) \pc B}\and
    \inferrule{\Gamma\types^\fL M\pc \unit \\ \fa\notin\fL}{\Gamma\types^{\fL\cup\{\fa\}} \cancel{M}^\fa:()}\and
  \end{mathpar}
  \caption{Rules for pre-terms}
  \label{fig:pre-terms}
\end{figure}

Next we give the rules for the term judgment.
To incorporate morphisms with codomain $()$, we write the judgment as
\[\Gamma \types^\fL (M_1,\dots,M_m\mid N_1,\dots,N_n):(A_1,\dots,A_m)\]
with the intended invariants that $\Gamma\types^{\fL} M_i\pc A_i$ for each $i$ and $\Gamma\types^{\fL} N_j\pc ()$ for each $j$.
Like the pre-term judgment, the term judgment is closed under adding finitely many more unused labels.
If $n=0$ we write simply $(M_1,\dots,M_m)$, omitting the $|$, and if $m=1$ as well we may omit the parentheses.

We write $\vec M$ for a list of pre-terms, $\vec{x}$ for a list of variables, and $\vec M,N$ for concatenation thereof.
We use capital Greek letters $\Gamma,\Delta,\dots$ both for \emph{contexts} (lists of types with assigned variables such as $x:A,y:B$) and for simple lists of types; the latter can be made into a context by supposing variables $\vec{x}:\Gamma$, or appear as the consequent with assigned terms $\vec{M}:\Gamma$.
Finally, if $f\in\cG(A_1,\dots,A_n;B_1,\dots,B_m)$ we write $\vec{f}^\fa(\vec{M})$ for the list of pre-terms
\[(f_1^\fa(M_1,\dots,M_n),f_2^\fa(M_1,\dots,M_n),\dots,f_m^\fa(M_1,\dots,M_n))\]
where each $M_i \pc A_i$.
Note that the variables in a context are not literally treated ``linearly'', since they can occur multiple times in the multiple ``components'' of a map $f$.
For instance, a morphism $f:(A,B)\to (C,D)$ is represented by $x:A, y:B \types (f_1(x,y),f_2(x,y)):(C,D)$.
Instead their ``usages'' are controlled by the codomain arity of the morphisms applied to them.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{
      x_1:A_1,\dots,x_n:A_n\types^\fL (M_1,\dots,M_m\mid Z_1,\dots,Z_p):(B_1,\dots,B_m)\\
      \sigma\in \Sigma_n \\ \tau\in\Sigma_m \\ \rho\in\Sigma_p
    }{
      x_{\sigma 1}:A_{\sigma 1},\dots,x_{\sigma n}:A_{\sigma n}\types^\fL (M_{\tau 1},\dots,M_{\tau m}\mid Z_{\rho 1},\dots,Z_{\rho p}):(B_{\tau 1},\dots,B_{\tau m})
    }\and
    \inferrule{\fL\finsubset\fA}{() \types^{\fL} ():()}\and
    \inferrule{\Gamma\types^\fL (\vec{M}\mid \vec{Z}):\Delta\\
      \types A\type\\
      \fa\notin\fL
    }{
      \Gamma,x:A\types^{\fL\cup\{\fa\}} (\vec{M},x^\fa\mid \vec{Z}):(\Delta,A)
    }\and
    \inferrule{
      \Gamma\types^\fL (\vec M,\vec N\mid\vec{Z}):(\Xi,\Psi) \\
      f\in\cG(\Psi; \Delta)\\
      |\Delta|>0\\
      \fa\notin\fL
    }{
      \Gamma \types^{\fL\cup \{\fa\}} (\vec M,\vec f^\fa(\vec N)\mid\vec{Z}):(\Xi,\Delta)
    }\and
    \inferrule{
      \Gamma\types^\fL (\vec M,\vec N\mid\vec{Z}):(\Xi,\Psi) \\
      f\in\cG(\Psi; ())\\
      \fa\notin\fL
    }{
      \Gamma \types^{\fL\cup\{\fa\}} (\vec M\mid\vec{Z},f^\fa(\vec N)):\Xi
    }\and
    \inferrule{
      \Gamma\types^\fL (\vec{M},N,P\mid\vec{Z}):(\Delta,A,B)\\
      \fa\notin\fL
    }{
      \Gamma\types^{\fL\cup\{\fa\}} (\vec{M},\pair{N}{P}^\fa \mid\vec{Z}):(\Delta,A\tensor B)
    }\;\tensorI\and
    \inferrule{
      \Gamma\types^\fL (\vec{M},P\mid\vec{Z}):(\Delta,A\tensor B)\\
      \fa\notin\fL
    }{
      \Gamma\types^{\fL\cup\{\fa\}} (\vec{M},\pi_1^\fa(P),\pi_2^\fa(P)\mid\vec{Z}):(\Delta,A,B)
    }\;\tensorE\\
    \inferrule{
      \Gamma\types^\fL(\vec M\mid\vec Z):\Delta\\
      \fa\notin\fL
    }{
      \Gamma\types^{\fL\cup\{\fa\}} (\vec M,\ttt^\fa\mid\vec Z):(\Delta,\one)
    }\;\one I\and
    \inferrule{
      \Gamma\types^\fL(\vec M,N\mid\vec Z):(\Delta,\one)\\
      \fa\notin\fL
    }{
      \Gamma\types^{\fL\cup\{\fa\}}(\vec M\mid \cancel{N}^\fa,\vec Z):\Delta
    }\;\one E\and
  \end{mathpar}
  \caption{Rules for terms}
  \label{fig:cl-smc-terms}
\end{figure}

With these notations, the rules for the term judgment are shown  in \cref{fig:cl-smc-terms}.
For clarity, we have made the exchange rule explicit at the beginning.
Note that these rules are exactly the natural deduction rules from \cref{fig:clnatded-smc}, with the additional annotation by pre-terms.
Moreover, when we annotate the two problematic derivations discussed above, we obtain the same pre-term.
\begin{mathpar}
  \inferrule*[Right=$g$]{
    \inferrule*{
      \inferrule*[Right=$f$]{
        \inferrule*{\inferrule*{ }{()\types():()}}{x:A\types x:A}
      }{
        x:A\types f(x):B
      }}{
      x:A,y:C\types (f(x),y):(B,C)
    }}{
    x:A,y:C\types (f(x),g(y)):(B,D)
  }\and
  \inferrule*[Right=$f$]{
    \inferrule*{
      \inferrule*[Right=$g$]{
        \inferrule*{\inferrule*{ }{()\types():()}}{y:C\types y:C}
      }{
        y:C\types g(y):D
      }}{
      x:A,y:C\types (x,g(y)):(A,D)
    }}{
    x:A,y:C\types (f(x),g(y)):(B,D)
  }\and
\end{mathpar}

Finally, we have an equality judgment.
This takes the form
\[\Gamma \types^{\fL;\fM} (\vec M\mid\vec Z)\equiv (\vec N\mid\vec W) :(\vec B)\]
which should respect the invariants
\begin{alignat*}{2}
  \Gamma&\types^{\fL} M_i\pc B_i &\qquad
  \Gamma&\types^{\fL} Z_j\pc ()\\
  \Gamma&\types^{\fM} N_i\pc B_i &\qquad
  \Gamma&\types^{\fM} W_j\pc ()
\end{alignat*}
Note that these may not be well-typed term judgments.
% TODO: Should they be?
We assert that this relation $\equiv$ is a congruence on lists of pre-terms with respect to concatenation, substitution, and permutation, and that it is invariant under permutations of the label set \fA \emph{separately on each side}.
That is, there is no requirement that the actual labels used on the two sides of $\equiv$ match up; to the eyes of $\equiv$, the label annotations simply define a partition of the ``label occurrences'' in a list of pre-terms.
(We do, of course, have to keep track of individual labels at the level of pre-terms; it's only after they are grouped into lists that we can ignore the identity of the labels.)
% TODO: Do we need to worry more about what happens to the labels here?

Relative to the above closure conditions, $\equiv$ is generated by the following relations on individual pre-terms (occurring anywhere in a list):
\begin{mathpar}
  \pi_1^\fa(\pair M N^\fb) \equiv M\and
  \pi_2^\fa(\pair M N^\fb) \equiv N\and
  \pair{\pi_1^\fa(P)}{\pi_2^\fa(P)}^\fb \equiv P\and
  \cancel{\ttt^\fa}^\fb \equiv ()\and
\end{mathpar}
and also the following relations on lists:
\begin{mathpar}
  (\vec M,\ttt^\fa \mid \vec Z,\cancel{W}^\fb) \equiv (\vec M,W\mid \vec Z)\and
  (\vec M \mid \vec{Z}) \equiv (\vec M \mid \vec{\sigma Z})\and
\end{mathpar}
The last relation means that we can permute the $Z$'s arbitrarily (of course, we can also permute the $M$'s, but only if we also permute their types).

We now commence the metatheoretic analysis.

\begin{lem}
  If $\Gamma\types^\fL (M_1,\dots,M_m\mid N_1,\dots,N_n):(B_1,\dots,B_m)$ is derivable, then so are $\Gamma\types^{\fL} M_i\pc B_i$ and $\Gamma\types^{\fL} N_j\pc ()$ for each $i,j$.
\end{lem}
\begin{proof}
  A straightforward induction over derivations.
\end{proof}

\begin{lem}\label{thm:preterm-subadm}
  The following rule of substitution into pre-terms is admissible:
  \begin{mathpar}
    \inferrule{
      \Gamma \types^{\fM} N_1\pc A_1 \\ \cdots \\ \Gamma\types^{\fM} N_n\pc A_n\\\\
      \Phi,y_1:A_1,\dots,y_n,A_n \types^\fL M\pc B\\
      \fL\cap\fM = \emptyset
    }{
      \Gamma,\Phi \types^{\fL\cup\fM} M[N_1/y_1,\dots,N_n/y_n]\pc B
    }
  \end{mathpar}
\end{lem}
\begin{proof}
  We induct over the derivation of $\Phi,y:A \types^\fL M\pc B$.
  In almost all cases we simply apply the inductive hypothesis to the premise, thereby defining the meaning of substitution on pre-terms by recursing through the structure as usual.
  We require $\fL\cap\fM=\emptyset$ so that the rules that introduce new labels can still be re-applied to the substituted premise, since they require that their new label be fresh.

  As usual, the one rule that behaves differently is $\Gamma,x:A \types^{\fL} x^\fa\pc A$ when $x=y$.
  In this case the result of substitution is just $N$.
\end{proof}

We may write $M[\vec N/\vec y]$ as a shorthand for $M[N_1/y_1,\dots,N_n/y_n]$.
The requirement of label disjointness in \cref{thm:preterm-subadm} means that we cannot decompose these ``simultaneous substitutions'' into iterated single substitutions.
For instance, if $N_1$ and $N_2$ share labels, then \cref{thm:preterm-subadm} would not justify $M[N_1/y_1][N_2/y_2]$.

\begin{lem}\label{thm:prop-smc-subadm}
  Substitution for terms (i.e.\ the cut rule for derivations) is admissible:
  \[\inferrule{
    \Gamma\types^\fL(\vec M,\vec N\mid\vec Z):(\Xi,\Psi) \\
    \vec y:\Psi,\Phi \types^\fM (\vec P\mid\vec W):\Delta\\
    \fM\cap\fL=\emptyset
  }{
    \Gamma,\Phi \types^{\fM\cup\fL} (\vec M, \vec P[\vec N/\vec y] \mid \vec Z, \vec W[\vec N/\vec y]):(\Xi,\Delta)
  }\]
\end{lem}
\begin{proof}
  As is usual for natural deductions, a very straightforward induction.
  Note that the requirement $\fM\cap\fL=\emptyset$ is a generalization of the requirement $\fa\notin\fL$ appearing in the basic rules.
\end{proof}

We define the \textbf{height} of a derivation to be the number of rules appearing in it \emph{other} than the exchange rule.
The following ``invertibility'' lemma is key to the initiality theorem.

\begin{lem}\label{thm:prop-smc-invertible}
  If we have a derivation of a given sequent \sQ, and an instance \cR of a rule whose conclusion is \sQ, then we can construct a derivation of \sQ that ends with \cR and has the same height as the given one.
\end{lem}
\begin{proof}
  This is trivially true for the exchange rule (since it supplies its own inverses and contributes no height) and for the trivial rule $()\types ():()$.
  For all the other rules \cR, suppose the given derivation ends with a different rule application $\cS$ (disregarding any exchanges in between).
  Now note that each rule introduces a new label and introduces exactly those pre-terms in the judgment whose outer connective has that label.
  Therefore, the rule applications $\cR$ and $\cS$, being different, must introduce different labels, and therefore introduce \emph{disjoint} sets of pre-terms.
  (This is where the argument would fail without labels.)

  It follows that the premise of \cS, say $\sQ'$, is also a conclusion of some instance $\cR'$ of the same rule as \cR.
  Therefore, we can apply the inductive hypothesis to obtain a derivation of this premise ending with $\cR'$, whose premise is $\sQ''$ say.
  Now by the same disjointness argument, we can apply an instance $\cS'$ of the same rule as \cS to $\sQ''$, and then $\cR$ to the conclusion of that rule, obtaining our desired derivation.
  The height is clearly preserved.
\end{proof}

That proof was written very abstractly, so consider the following example.
We omit to write exchange rules, as well as the obvious variable rules at the top.
\begin{mathpar}
  \inferrule*[Right=$\tensorE$]{
    \inferrule*[Right=$f$]{
      x:A,y:B\tensor C \types (y,x):(B\tensor C,A)
    }{
      x:A,y:B\tensor C \types (y,f_2(x),f_1(x)):(B\tensor C,E,D)
    }
  }{
    x:A,y:B\tensor C \types (\pi_1(y),f_2(x),\pi_2(y),f_1(x)):(B,E,C,D)
  }
\end{mathpar}
The conclusion of this rule is (modulo exchange) the conclusion of an instance of the $f$-generator rule.
Now the $\pi$-pre-terms introduced by the actual rule $\tensorE$ that leads to this conclusion are disjoint from the $f$-pre-terms that would be introduced by this $f$-generator rule.
Thus, by induction the premise $x:A,y:B\tensor C \types (y,f_2(x),f_1(x)):(B\tensor C,E,D)$ has a derivation of the same height ending with the $f$-generator rule; in this simple case that is in fact the derivation of it that we were given.
Now we can apply $\tensorE$ to the premise of \emph{that} rule, namely $x:A,y:B\tensor C \types (y,x):(B\tensor C,A)$, and then follow it by the $f$-generator rule, obtaining the following derviation:
\begin{mathpar}
  \inferrule*[Right=$f$]{
    \inferrule*[Right=$\tensorE$]{
      x:A,y:B\tensor C \types (y,x):(B\tensor C,A)
    }{
      x:A,y:B\tensor C \types (\pi_1(y),\pi_2(y),x):(B,C,A)
    }
  }{
    x:A,y:B\tensor C \types (\pi_1(y),f_2(x),\pi_2(y),f_1(x)):(B,E,C,D)
  }
\end{mathpar}

\begin{thm}\label{thm:prop-smc-initial}
  The free symmetric monoidal category on a symmetric polygraph can be presented using this type theory: its morphisms $\Gamma\to\Delta$ are the \emph{lists of pre-terms} such that $\Gamma\types (M_1,\dots,M_n \mid Z_1,\dots,Z_p):\Delta$ is derivable.
  (In particular, if two derivations give the same pre-term, we stipulate that only one morphism results.)
\end{thm}
\begin{proof}
  \cref{thm:prop-smc-subadm} tells us how to compose morphisms.
  Associativity, unitality, and equivariance follow as usual, since substitution into pre-terms is basically ordinary substitution.
  In particular, these axioms hold as syntactic equalities of pre-terms.

  For the interchange rule, suppose given derivations of $\Gamma\types (\vec M\mid\vec Z):\Delta$ and $\Phi\types (\vec N\mid\vec W):\Xi$.
  We can then substitute them along $()$ in either order to get
  \begin{align*}
    \Gamma,\Phi &\types (\vec M,\vec N\mid \vec Z,\vec W):\Delta,\Xi\\
    \Phi,\Gamma &\types (\vec N,\vec M\mid \vec W,\vec Z):\Xi,\Delta\\
\intertext{Applying exchange to the second, we obtain}
    \Gamma,\Phi &\types (\vec M,\vec N\mid \vec W,\vec Z):\Delta,\Xi
  \end{align*}
  and we have $(\vec M,\vec N\mid \vec Z,\vec W) \equiv (\vec M,\vec N\mid \vec W,\vec Z)$ by the final generating rule of $\equiv$.
  In particular, unlike the other axioms, interchange holds only up to $\equiv$.

  In any case, we now have a PROP $\F\bSMC\cG$.
  The $\beta$- and $\eta$-reduction rules give it tensor and unit structure, so it is a symmetric monoidal category.

  Now suppose \cM is a symmetric monoidal category and $P:\cG\to\cM$ is a morphism of polygraphs.
  We extend $P$ to the PROP $\F\bSMC\cG$ by induction on derivations.
  This is straightforward using the PROP structure and tensors and units in \cM; the nontrivial thing is showing that the result depends only on the pre-term judgment rather than on the particular derivation.
  This is the purpose of \cref{thm:prop-smc-invertible}.

  Suppose we have two derivations $\sD_1$ and $\sD_2$ of the same sequent $\sQ$, ending with rules $\cR_1$ and $\cR_2$ with premises $\sQ_1$ and $\sQ_2$ respectively.
  By disjointness of pre-terms as in the proof of \cref{thm:prop-smc-invertible}, we can say that $\sQ_1$ is a conclusion of an instance of $\cR_2$, and similarly $\sQ_2$ is a conclusion of an instance of $\cR_1$.
  By \cref{thm:prop-smc-invertible}, therefore, $\sQ_1$ has a derivation $\sD_1'$ of the same height ending with $\sR_2$, while $\sQ_2$ has a derivation $\sD_2'$ of the same height ending with $\sR_1$.
  Moreover, up to exchange (as always) the premise of these two ending rules must be the same sequent $\sQ_3$.
  By induction (which is applicable since the heights have been preserved), the given derivations of $\sQ_1$ and $\sQ_2$ yield the same morphism in \cM as these other derivations $\sD_1'$ and $\sD_2'$.

  Thus, it will suffice to show that once the sequent $\sQ_3$ is interpreted according to some particular derivation of it, we obtain the same interpretation of $\sQ$ by deriving it in the following two ways:
  \begin{mathpar}
    \inferrule*[Right=$\cR_1$]{\inferrule*[Right=$\cR_2$]{\vdots\\\\\sQ_3}{\sQ_1}}{\sQ}\and
    \inferrule*[Right=$\cR_2$]{\inferrule*[Right=$\cR_1$]{\vdots\\\\\sQ_3}{\sQ_2}}{\sQ}\and
  \end{mathpar}
  This follows from the associativity and interchange rules in \cM.
\end{proof}

[TODO: Finally, algebraic theory.]




\chapter{First-order logic}
\label{chap:fol}




\bibliographystyle{alpha}
\bibliography{all}

\end{document}
