\chapter{First-order logic}
\label{chap:fol}

\section{Predicate logic}
\label{sec:fol}

In \cref{sec:logic} we saw that the posetal reduction of a simple type theory can be regarded as a deductive system for logic (intuitionistic, linear, relevant, classical, etc.\ depending on the type theory).
However, these logics are only \emph{propositional}, lacking variables and the ability to quantify over them with statements such as ``for all $x$'' or ``there exists an $x$ such that''.
Similarly, in \cref{sec:fp-theories} we saw that simple type theory is adequate to express finite-product theories such as groups and rings, but not more complicated theories such as categories or fields.
The solution to both of these problems is the same: we combine \emph{two} type theories, one representing the objects (like a finite-product theory) and one representing the logic in which we speak about those objects.

The types in the second type theory, which we will henceforth call \textbf{propositions} instead of types to avoid confusion, will be \emph{dependent} on the types in the first type theory (which we sometimes call the \emph{base type theory}).
This means that terms belonging to types can appear in propositions.
More formally, it means that unlike the judgment $\types A \type$ for types (in the base type theory), the judgment for propositions \emph{has a context of types}, so we write it $\Gamma\types \ph\prop$.
We will have rules such as
\[ \inferrule*{\Gamma \types M:A \\ \Gamma\types N:A}{\Gamma\types (M =_A N)\prop} \]
allowing the logic (the type theory of propositions) to ``talk about'' equality of terms (morphisms between types).
Finally, since propositions depend on a context of types, their morphism judgment (which we also call \textbf{entailment}) must also depend on such a context.
Thus it has \emph{two} contexts, one of types and one of propositions, which we separate with a vertical bar: $\Gamma \cb \Theta \types \ph$.

In this section, we will describe and study type theories of this sort, with one simple type theory dependent on another simple type theory.
Unlike the type theories considered in \cref{chap:simple}, which were directly motivated by a corresponding categorical structure, in the present case it seems more natural to describe the type theory first and then define an appropriate categorical structure in order to match it.
(This is not to say that there are not lots of naturally occurring examples of this categorical structure; there are!
It's just that without the type theory in mind, we might not be led to define and study that exact class of categorical structures.)
Thus, we postpone consideration of their categorical semantics to \cref{sec:hyperdoctrines,sec:subobjects}.

We will also make several simplifying assumptions in this section.
Firstly, the base type theory will always be a bare theory of cartesian multicategories under some multigraph, with no type operations and no axioms.
The lack of axioms is not much of a problem, since once we have equality propositions we can use those instead.
The lack of type operations is a temporary simplification, but identifies our current subject as \emph{first-order} logic; in \cref{chap:hol} on ``higher-order logic'' we will reintroduce type operations.
The \emph{cartesianness} of the base type theory is also a simplifying assumption, but one that we will not (in this book) ever generalize away from.
People have attempted to define first-order logics over non-cartesian base theories, but in general the results are more complicated and less intuitive, and there are fewer interesting categorical examples.

Secondly, in this section the logic will be posetal, so that we care only about the existence of derivations rather than their values, and hence we will not introduce terms belonging to propositions.
We will generalize away from this assumption in \cref{sec:indexed-moncat}.
With this generalization in mind, we will use natural deduction style for the logic in this section, though a sequent calculus would work just as well (see \cref{ex:fol-seqcalc}).

With all those preliminary remarks out of the way, we proceed to describe the theory.
As mentioned above, the base type theory is that for cartesian multicategories under a multigraph \cG:
\begin{mathpar}
  \inferrule{\types A\type \\ (x:A)\in \Gamma}{\Gamma\types x:A}\;\idfunc
  \and
  \inferrule{f\in \cG(A_1,\dots,A_n;B) \\ \Gamma\types M_1:A_1 \\ \cdots \\ \Gamma \types M_n:A_n}{\Gamma\types f(M_1,\dots,M_n):B}\;f
\end{mathpar}
As usual, cut/substitution is admissible for this theory.
For the propositions, we have two kinds of judgment:
\begin{mathpar}
  \Gamma\types \ph\prop \and
  \Gamma\cb\Theta\types \ph
\end{mathpar}
where $\Theta$ is a context (i.e.\ a list) of propositions.
Here the proposition $\ph$ should be regarded as a sort of ``term'' for the proposition judgment, that can be shown to uniquely determine a derivation of $\Gamma\types \ph\prop$.

Since our logic is posetal in this section, we simplify our lives by asserting any desired structural rules for the propositions as primitive.
Thus we may choose some or all of the following:
\begin{mathpar}
  \inferrule*[right=exchange]{\Gamma\cb\Theta,\ph,\psi,\Delta\types \chi}{\Gamma\cb\Theta,\psi,\ph,\Delta\types \chi}\and
  \inferrule*[right=weakening]{\Gamma\cb \Theta,\Delta\types \chi}{\Gamma\cb\Theta,\ph,\Delta\types \chi}\and
  \inferrule*[right=contraction]{\Gamma\cb\Theta,\ph,\ph,\Delta\types \chi}{\Gamma\cb\Theta,\ph,\Delta\types \chi}\and
\end{mathpar}
and depending on which we choose, we speak of \textbf{intuitionistic first-order logic} (all the structural rules), \textbf{intuitionistic first-order linear logic} (exchange only), etc.
As in \cref{sec:logic}, we simplify our lives by always assuming exchange; and of course we also always have the identity rule:
\[ \inferrule*{ }{\Gamma\cb \ph\types\ph} \]

In this section we will also depart from our general principle and take the cut rule \emph{for propositions} to be primitive rather than admissible:
\[ \inferrule{\Gamma\cb\Theta \types \ph \\ \Gamma\cb \Psi,\ph\types \psi}{\Gamma\cb \Psi,\Theta\types \psi}\]
This simplifies a few of the rules, like the elimination rules for existential quantification and equality, and does not entail much loss: since we are considering the propositions to be posetal, we don't care about whether a judgment has more than one derivation, so it doesn't matter to have extra rules around.
(In \cref{sec:indexed-moncat} we will generalize away from the posetal case, necessitating the more complicated versions of these rules.)

There is one other structural rule, however, that we \emph{do} want to make admissible: substitution of terms into propositions (and their entailments).
Written as rules these look like
\begin{mathpar}
  \inferrule{\Gamma\types M:A \\ \Gamma,x:A\types \ph\prop}{\Gamma \types \ph[M/x] \prop}\and
  \inferrule{\Gamma\types M:A \\ \Gamma,x:A\cb\Theta\types \ph}{\Gamma\cb\Theta[M/x] \types \ph[M/x]}\and
\end{mathpar}
but we emphasize that these are \emph{not} primitive rules; instead we will be later prove them to be admissible.
This is important for the same reason that admissibility of substitution into terms is: we certainly want to be \emph{able} to use these rules, but if we asserted them as primitive then (to maintain the unique correspondence between names for propositions $\ph$ and the derivations of $\Gamma\types \ph\prop$) we would have to introduce ``$\ph[M/x]$'' as basic syntax, rather than an operation on syntax.
For instance, we want to be able to substitute $M$ for $x$ and $N$ for $y$ into $x=y$, and we want to be able to actually \emph{do} that substitution on the syntax to get $M=N$, rather than having to write $(x=y)[M/x,N/y]$ everywhere.
Another possibility would be to break the ``propositions are derivations'' correspondence and allow one proposition to have multiple derivations, but that has the same problems as breaking the ``terms are derivations'' correspondence in simple type theory; we do care about \emph{which} proposition we are talking about.

Fortunately, it is just as easy to ensure that substitution into propositions is admissible as it is to ensure that cut is admissible in a natural deduction.
We just make sure to ``build enough substitutions'' into the rules for the proposition judgment so that their conclusions always have a fully general context.
While we are at it, we do the same for the entailment rules, so that subsitution into entailments is also admissible, though the arguments for this are not as compelling (in the posetal case).

Now, what \emph{are} the rules for the proposition and entailment judgments?
To start with, there will be the usual rules for propositional logic from \cref{sec:logic}.
We import these rules into our present theory by assigning all of them an arbitrary context of types in the base theory that remains unchanged between the premises and the conclusion.
For instance, the rules for $\join$ are
\begin{mathpar}
  \inferrule{\Gamma\cb\Theta\types A}{\Gamma\cb\Theta\types A\join B}\;\joinI1
  \and
  \inferrule{\Gamma\cb\Theta\types B}{\Gamma\cb\Theta\types A\join B}\;\joinI2
  \and
  \inferrule{
    \Gamma\cb\Psi\types A\join B \\ \Gamma\cb\Theta,A \types C \\ \Gamma\cb\Theta,B\types C
  }{\Gamma\cb\Theta,\Psi \types C}\;\joinE
\end{mathpar}
and likewise we have rules for $\bot,\meet,\top,\tensor,\one$, and $\hom$.\footnote{Since cut is primitive, we could simplify the rule $\joinI1$ to $\Gamma\cb A\types A\join B$, but we will stick with the familiar versions of the rules we have already encountered.}
Of course, in the cartesian case we can dispense with $\tensor$ and $\one$ (since they coincide with $\meet$ and $\top$), and write $\hom$ instead as $\To$ or $\to$, and we could also formulate the rules with an unchanging proposition context as in \cref{sec:heyting-algebras}.
The modularity of type theory means we can also mix and match, choosing the rules corresponding to some of these connectives but not others; in \cref{sec:subobjects} we will see that some groups of connectives are particularly natural from a categorical perspective.

The interesting new things happen with the \emph{new} operations on propositions that \emph{do} change the type context.
We will consider three such operations, which are particularly natural both categorically and logically.
The first two are the \emph{quantifiers} ``for all'' (the ``universal quantifier'') and ``there exists'' (the ``existential quantifier'').
The rules introducing these two propositions both look the same:
\begin{mathpar}
  \inferrule{\Gamma,x:A\types \ph\prop}{\Gamma\types (\forall x:A.\ph) \prop}\and
  \inferrule{\Gamma,x:A\types \ph\prop}{\Gamma\types (\exists x:A.\ph) \prop}\and
\end{mathpar}
(Note that in both cases the variable $x$ is \emph{bound} in the resulting proposition, just as it is in $\lambda x.M$.
If there is no danger of confusion, we may abbreviate these to $\all x.\ph$ and $\exis x.\ph$, but in general the type annotation is necessary to make type-checking possible.)
But the rules governing entailments involving them, of course, are different.

Recall that in natural deduction, each type operation has both one or more \emph{introduction} rules and one or more \emph{elimination} rules (while in sequent calculus, these are rephrased as \emph{right} and \emph{left} rules respectively).
In the past we have motivated these rules by appeal to universal properties in a categorical structure, with one group of rules giving the basic data and the other giving their universal factorization property.
The rules for $\exis$ and $\all$ do correspond to universal properties, but since we have postponed the semantics of first-order logic to \cref{sec:hyperdoctrines} we will attempt to instead motivate their rules from an intuitive understanding of logic.

Informally, how do we prove that $\forall x:A.\ph$?
Arguably the most basic way to do it is to assume given an arbitrary $x:A$ and prove that $\ph$ is true (here $\ph$ is a statement involving $x$, hence involving our arbitrary assumed $x:A$).
This suggests the following introduction rule:
\begin{mathpar}
  \inferrule{\Gamma,x:A\cb \Theta\types \ph}{\Gamma\cb\Theta\types \forall x:A.\ph}\;\forallI
\end{mathpar}
Note that since $\Theta$ appears in the conclusion, where $x$ is no longer in the type context, $\Theta$ cannot depend on $x$, even though syntactically the premise would allow that.

Similarly, what good does it do to know that $\forall x:A.\ph$?
The most basic thing it tells us is that if we have any particular element $M$ of $A$, then $\ph$ is true about $M$, i.e.\ with $M$ replacing $x$.
This suggests the following elimination rule:
\begin{mathpar}
  \inferrule{\Gamma\types M:A \\ \Gamma\cb\Theta\types \forall x:A.\ph}{\Gamma\cb\Theta\types \ph[M/x]}\;\forallE
\end{mathpar}

\begin{rmk}
  Note that this rule involves substitution into propositions.
  Thus, formally speaking we have to state all the rules for the proposition judgment $\Gamma\types \ph\prop$ first, \emph{then} prove that substitution into propositions is admissible (thereby defining the notation $\ph[M/x]$), and only after that can we state all the rules for the entailment judgment $\Gamma\cb\Theta\types\ph$.
  A similar situation obtained for the equality judgment $\equiv$ for simple and unary type theories, which often involved substitution into terms (e.g.\ $(\lambda x.M)N \equiv M[N/x]$), so that we had to prove the admissibility of the latter before stating the rules for $\equiv$ (and likewise, when proving the initiality theorems, we had to show that our functor-in-progress took substitution to composition before defining it on equalities).
  However, in practice we actually state all the rules at once, with the implicit understanding that afterwards we will define substitution so that the rules involving it make sense.
  
  We do have to be careful, when taking such a shortcut, to notice whether we are introducing any ``cyclic dependencies''.
  For instance, if there are any rules for the term or proposition judgments whose premises involve the entailment judgment, it is no longer possible to complete the definition of the former, \emph{then} define substitution for them, and \emph{then} give the definition of the latter: we would have to give the definition all at once, including (somehow) defining substitution at the same time.
  It is possible to do this, but it is much more difficult and leads us into the realm of dependent type theory; see \cref{chap:dtt}.

  In this chapter and \cref{chap:hol} none of our rules will introduce such cyclic dependencies.
  We mention the possibility only as a warning to the reader, because it is easy (especially when adding rules to a type theory one by one) to fail to notice a cyclic dependency when it appears.
  %See also \cref{rmk:subset-quotient}.
\end{rmk}

Moving on to the existential quantifier, the most basic way to prove $\exists x:A.\ph$ is to exhibit a particular element $M$ of $A$ and prove that it has the property $\ph$ (that is, $\ph$ with $M$ replacing $x$ is true).
This is of course a ``constructive'' proof.
In classical mathematics one can also give ``nonconstructive'' existence proofs, but these arise by use of the law of excluded middle or its equivalent law of double negation.
The \emph{basic} way to prove existence, which uses no other logical laws than the meaning of ``existence'', is to supply a witness.
This leads to the following introduction rule for $\exis$:
\begin{mathpar}
  \inferrule{\Gamma\types M:A \\ \Gamma\cb\Theta\types \ph[M/x]}{\Gamma\cb\Theta\types \exists x:A.\ph}\;\existsI
\end{mathpar}

Finally, what good does it do to know that $\exists x:A.\ph$?
It means we are free to assume that we have some element of $A$ satisfying $\ph$ (but about which we assume nothing else).
This leads to the following elimination rule:
\begin{mathpar}
  \inferrule{\Gamma \types \psi\prop \\ \Gamma,x:A\cb\Theta,\ph\types \psi}{\Gamma\cb\Theta,\exists x:A.\ph\types \psi}\;\existsE
\end{mathpar}
This is perhaps the least intuitive of the quantifier rules: it says that if we can prove some other statement $\psi$ under the assumption of some arbitrary $x:A$ that satisfies $\ph$, then we can also conclude $\psi$ under the assumption of $\exists x:A.\ph$.
(Note the similarity in structure between $\existsE$ and $\tensorE$; this suggests the eventual universal property we will find corresponding to $\exis$.)

We include the premise $\Gamma \types \psi\prop$ to ensure that $\psi$ doesn't depend on the variable $x$, since otherwise we would not want to let ourselves write $\Gamma\cb\Theta\types \psi$ (with $x$ not appearing in $\Gamma$, as implied by our conventions and the fact that in a premise we wrote $\Gamma,x:A$).
We also don't want $x$ to appear in $\Theta$, but the derivability of the other premise $\Gamma\cb\Theta\types \exists x:A.\ph$ is sufficient to ensure that.

This completes the rules for the quantifiers $\all$ and $\exis$.
The third and last new operation on propositions is perhaps the subtlest of all: the \emph{equality proposition}.
Its formation rule is unsurprising: it says that for any two terms of the same type, we can consider the proposition that they are equal.
\begin{mathpar}
  \inferrule{\Gamma\types M:A \\ \Gamma\types N:A}{\Gamma\types (M =_A N)\prop}
\end{mathpar}
(The subscript annotation $A$ in $M=_A N$ is needed for type-checking; but as usual, we will often omit it.)
But how are we to describe its behavior?
The most classical approach to equality is to assert that it is reflexive, symmetric, transitive, and ``substitutive'' (i.e.\ if $\ph[M/x]$ and $M=N$, then also $\ph[N/x]$).
This is very much like how we described the equality \emph{judgment} $M\equiv N$ in \cref{chap:unary,chap:simple}.
It works here too, but it doesn't fit the general introduction/elimination pattern of natural deduction, and therefore its categorical semantics are not as obvious.

It is one of the great insights of Lawvere~\cite{lawvere:comprehension} (presaged by Leibniz, and approximately contemporaneous with a similar observation by Martin-L\"of) that the rules of reflexivity, symmetry, transitivity, and substitutivity are equivalent to the following pair of rules:
\begin{mathpar}
  \inferrule*{\Gamma\types M:A}{\Gamma\cb ()\types (M=_A M)}\and
  \inferrule*{\Gamma\types M:A \\ \Gamma\types N:A \\\\ \Gamma,x:A,y:A\types \ph\prop \\ \Gamma,x:A\cb\Theta[x/y]\types \ph[x/y]}{\Gamma\cb\Theta[M/x,N/y],(M=_A N)\types \ph[M/x,N/y]}
\end{mathpar}
The first, right/introduction, rule is simply reflexivity.
(Of course, if we have the weakening rule, then we can more generally derive $\Gamma\cb \Theta\types (M=_A M)$ for any proposition context $\Theta$.)

We have stated the other rule as a sequent-calculus-style left rule, without building in cut, because it is hard enough to understand this way; since cut is primitive in this section, there is no problem with this.
Intuitively, this rule says that if we have a statement about $x$ and $y$, and that statement becomes true when we substitute $x$ for $y$, then that statement is true under the hypothesis that $x=y$.
More generally, we can replace the truth of a statement with the truth of an entailment $\Theta\types \ph$, where we also substitute $x$ for $y$ in $\Theta$ in the premise.
In other words, \emph{if we have a hypothesis that $x=y$, then we may as well write $x$ instead of $y$ everywhere that it appears}.

To help motivate this rule further, let us derive symmetry and transitivity from it.
Here is symmetry:
\begin{mathpar}
  \inferrule*{x:A,y:A \types (y=_A x) \prop \\ \inferrule*{ }{x:A \cb () \types (x=_A x)}}{x:A,y:A \cb (x=_A y) \types (y=_A x)}
\end{mathpar}
We use the left rule once, with $\ph$ being $y=_A x$, so that $\ph[x/y]$ is $x=_A x$, which we can prove by reflexivity.

And here is transitivity:
\begin{mathpar}
  \inferrule*{
    x:A,y:A,z:A \types (x=_A z) \prop \\
    \inferrule*{ }{x:A,y:A \cb (x=_A y) \types (x=_A y)}
  }{x:A,y:A,z:A \cb (x=_A y),(y=_A z) \types (x=_A z)}
\end{mathpar}
We again use the left rule once on the hypothesis $y=_A z$, with $\ph$ being $x=_A z$, so that $\ph[y/z]$ is $x=_A y$, which we can prove by the identity rule from the other hypothesis.

% To make cut admissible, we need at least a further substitution for $M=_A N$:
% \begin{mathpar}
%   \inferrule*{\Gamma\cb\Theta\types M=_A N \\ \Gamma,x:A,y:A\types \ph\prop \\ \Gamma,x:A\cb\Theta[x/y]\types \ph[x/y]}{\Gamma\cb\Theta[M/x,N/y]\types \ph[M/x,N/y]}
% \end{mathpar}
% However, this is still not enough to make cut admissible: the proposition context $\Theta[M/x,N/y]$ is not fully general but contains a substitution.

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{\Gamma,x:A\types \ph\prop}{\Gamma\types (\forall x:A.\ph) \prop}\and
    \inferrule{\Gamma,x:A\cb \Theta\types \ph}{\Gamma\cb\Theta\types \forall x:A.\ph}\;\forallI\and
    \inferrule{\Gamma\types M:A \\ \Gamma\cb\Theta\types \forall x:A.\ph}{\Gamma\cb\Theta\types \ph[M/x]}\;\forallE\and
    \inferrule{\Gamma,x:A\types \ph\prop}{\Gamma\types (\exists x:A.\ph) \prop}\and
    \inferrule{\Gamma\types M:A \\ \Gamma\cb\Theta\types \ph[M/x]}{\Gamma\cb\Theta\types \exists x:A.\ph}\;\existsI\and
    \inferrule{\Gamma \types \psi\prop \\ \Gamma,x:A\cb\Theta,\ph\types \psi}{\Gamma\cb\Theta,\exists x:A.\ph\types \psi}\;\existsE\and
    \inferrule{\Gamma\types M:A \\ \Gamma\types N:A}{\Gamma\types (M =_A N)\prop}\and
    \inferrule{\Gamma\types M:A}{\Gamma\cb ()\types (M=_A M)}\;\eqI\and
    \inferrule{\Gamma\types M:A \\ \Gamma\types N:A \\\\ \Gamma,x:A,y:A\types \ph\prop \\ \Gamma,x:A\cb\Theta[x/y]\types \ph[x/y]}{\Gamma\cb\Theta[M/x,N/y],(M=_A N)\types \ph[M/x,N/y]}\;\eqE
  \end{mathpar}
  \caption{Quantifier and equality rules}
  \label{fig:fol}
\end{figure}

We summarize the new rules for first-order logic (above and beyond those from that we import from the propositional logic of \cref{sec:logic} by adding an unchanging type context) in \cref{fig:fol}.
This completes the definition of \textbf{intuitionistic first-order logic} (if we include all the structural rules), as well as \textbf{intuitionistic first-order linear logic} (if we include only exchange), and so on.
The qualifier ``intuitionistic'' is because, like in \cref{sec:logic}, we cannot prove the law of excluded middle $\ph\join\neg\ph$ (where $\neg\ph$ means $\ph\hom\bot$), or its equivalent the law of double negation $\neg\neg\ph\hom\ph$.
In \cref{sec:logic} we motivated this by noting that leaving it out just means our ``logic'' has models in all Heyting algebras rather than just Boolean algebras.
We will be able to say something similar, and hopefully even more convincing, about first-order logic in \cref{sec:subobjects}.

We are still, however, missing some ``generator'' rules that would allow us to speak of a ``first-order theory''.
In addition to our multigraph \cG giving the base types and terms, we would like to also have a set \cP of ``base propositions'' (usually called \textbf{atomic propositions}).
Each of these should have an assigned \emph{type context}, i.e.\ a list of objects of \cG; we write $\cP(A_1,\dots,A_n)$ for the set of atomic propositions with context $A_1,\dots,A_n$.
Then we will have a generator rule for propositions, with substitutions built in just like the generator rule for terms:
\[ \inferrule{\ph\in \cP(A_1,\dots,A_n) \\ \Gamma\types M_1:A_1 \\ \cdots\\ \Gamma\types M_n:A_n}{\Gamma \types \ph(M_1,\dots,M_n) \prop} \]

Finally, we should have some generating entailments, i.e.\ \emph{axioms}.
Each of these should have an assigned type context $A_1,\dots,A_n$, an assigned proposition context $\Theta$, and an assigned consequent $\ph$.
Here $\ph$ and the elements of $\Theta$ should be propositions in context $x_1:A_1,\dots,x_n:A_n$ --- not just atomic propositions, but arbitrary ones derivable from the atomic ones and the rules for making new propositions.
If we write $\cA(A_1,\dots,A_n;\Theta;\ph)$ for the assertion that there is such an axiom, then the generator rule introducing axioms will be
\[ \inferrule{\cA(A_1,\dots,A_n;\Theta;\ph) \\ \Gamma\types M_1:A_1 \\ \cdots\\ \Gamma\types M_n:A_n}{\Gamma \cb \Theta[\vec M/\vec x]\types \ph[\vec M/\vec x]} \]

With this rule added to the other rules for entailment, we complete the definition of a \textbf{first-order theory} (of the appropriate sort).
A few important subsystems of intuitionistic first-order logic that will reappear later are:
\begin{itemize}
\item \emph{Coherent} logic: includes $\meet,\top,\join,\bot,\exis,=$ but not $\To$ or $\all$ (hence also not $\neg$).
\item \emph{Regular} logic: includes $\meet,\top,\exis,=$ but not $\join,\bot,\To,\neg,\all$.
\item \emph{Horn} logic: includes $\meet,\top,=$ but not $\join,\bot,\To,\neg,\all,\exis$.
% \item \emph{Left-exact} or \emph{finite-limit} logic: includes $\meet,\top,=$ but not $\join,\bot,\To,\all,\exis$.  [TODO: Need a restricted form of $\exis$, which ties a partial knot between propositions and entailment, hence including substitution too.]
\item Another important logic is \emph{geometric} logic, which is like coherent logic but also includes the ``infinitary disjunction'' from \cref{ex:frames}.
\end{itemize}

Since we are not considering categorical structures or initiality in this section, all that remains to do is prove the admissibility of substitution.

\begin{thm}\label{thm:fol-subprop-adm}
  Substitution is admissible in any first-order logic: given derivations of $\Gamma,x:A \types \ph\prop$ and $\Gamma\types M:A$, we can construct a derivation of $\Gamma \types \ph[M/x]\prop$.
\end{thm}
\begin{proof}
  As with substitution into terms, this is entirely straightforward because we have written all the rules for such judgments with an arbitrary type context.
  Some of the defining clauses are
  \begin{align*}
    (\ph\meet\psi)[M/x] &= \ph[M/x] \meet \psi[M/x]\\
    (\forall y:B.\ph)[M/x] &= \forall y:B.\ph[M/x]\\
    (N=_B P)[M/x] &= (N[M/x] =_B P[M/x])
  \end{align*}
  In the case of $\all$ (and also $\exis$), we have to ensure (by $\alpha$-equivalence if necessary) that $x$ and $y$ are distinct variables.
  This is the same issue that arose in \cref{sec:multicat-moncat,sec:multicat-prod-coprod,sec:stlc} when substituting into terms with bound variables such as $\case$ and $\lambda$-abstractions.
  As always, this is only an issue when representing derivations by terms; the underlying operation on derivations has no notion of ``bound variable''.

  Note also that substitution into an equality \emph{proposition} is defined using substitution into the \emph{terms} appearing in it.
  But since terms never involve propositions, there is no cyclic dependency: we can first prove the admissibility of substitution into terms, and then use it to prove the admissibility of substitution into propositions.
\end{proof}

Moreover, just as substitution into terms is associative, substitution into propositions satisfies as ``functoriality'' property that can be proven in the same way:
\begin{equation}
  \ph[N/y][M/x] = \ph[M/x][N[M/x]/y]\label{eq:fol-subprop-funct}
\end{equation}
Similarly, we can prove the admissibility of substitution into entailment judgments, although as mentioned before this is not as important since we consider propositions posetally here.

\begin{rmk}
  Recall from \cref{sec:natded-logic} that the natural deduction of intuitionistic propositional logic can be formulated without explicit contexts, instead ``discharging'' temporary assumptions by crossing them out.
  The same is true for intuitionistic first-order logic with $\all$ and $\exis$, if we allow ``variable assumptions'' that can be discharged by the quantifier rules; we leave the details to the reader.
  The case of equality is a bit tricker because of the arbitrary context $\Theta$ that has to be substituted into, but \cref{ex:eq-frob-from-hom} shows that as long as we also have implication we can ignore this.
\end{rmk}

\subsection*{Exercises}

\begin{ex}\label{ex:fol:cutadm}
  Modify the rules given in this section so as to make the cut rule for propositions admissible, and prove it.
\end{ex}

\begin{ex}\label{ex:fol-seqcalc}
  Formulate sequent calculus rules for $\exis,\all,=$ and prove cut admissibility for them.
\end{ex}

\begin{ex}\label{ex:eq-frob-from-hom}
  Assuming we have $\hom$, show that the rule $\eqE$ is derivable (recall \cref{rmk:admissible-derivable-1}) from the following simpler rule with no proposition context $\Theta$:
  \begin{mathpar}
    \inferrule{\Gamma\types M:A \\ \Gamma\types N:A \\\\ \Gamma,x:A,y:A\types \ph\prop \\ \Gamma,x:A\cb()\types \ph[x/y]}{\Gamma\cb(M=_A N)\types \ph[M/x,N/y]}
  \end{mathpar}
\end{ex}

\begin{ex}\label{ex:quantifier-laws}
  Three of the following four sequents are derivable in intuitionistic first-order logic (for any type $A$,  context $\Gamma$, and proposition $\Gamma,x:A\types\ph\prop$); derive them.
  \begin{align*}
    \Gamma \cb \exists x:A. \neg \ph &\types \neg\forall x:A. \ph\\
    \Gamma \cb \forall x:A. \neg \ph &\types \neg\exists x:A. \ph\\
    \Gamma \cb \neg\forall x:A. \ph &\types \exists x:A. \neg \ph\\
    \Gamma \cb \neg\exists x:A. \ph &\types \forall x:A. \neg \ph
  \end{align*}
\end{ex}

\begin{ex}\label{ex:exists-frob}
  Derive the following sequent in regular logic, for any type $A$, context $\Gamma$, and propositions $\Gamma\types\ph\prop$ and $\Gamma,x:A\types\psi\prop$:
  \begin{equation}
    \Gamma \cb \ph\meet (\exists x:A.\psi) \types (\exists x:A. (\ph\meet\psi))\label{eq:exists-frob}
  \end{equation}
  Thus this (nontrivial!)\ interaction between $\meet$ and $\exis$ is, like the distributive law of $\meet/\tensor$ over $\join$ from \cref{ex:monpos-jslat} and \cref{sec:logic}, implied automatically by the structure of our contexts and how they interact with the rules for $\meet$ and $\exis$.
  Many authors like to ``simplify'' logic by presenting it as a unary type theory, arguing that a context $\Theta = (\ph_1,\dots,\ph_n)$ can always be replaced by the conjunction $\ph_1\meet \dots \meet\ph_n$.
  This is true, but it forces one to assert laws like the distributive law and~\eqref{eq:exists-frob} ``by hand'', breaking principle~\eqref{princ:independence} and making for a less congenial theory.
\end{ex}

\begin{ex}\label{ex:fol-egs}
  Write down a first-order theory for each of the following structures.
  If you can, formulate them so that they fit inside the specified fragment.
  \begin{enumerate}
  \item Partially ordered sets (Horn)
  \item Totally ordered sets (coherent)
  \item Fields (coherent)
  \item Categories (regular)
  \end{enumerate}
\end{ex}


\section{First-order hyperdoctrines}
\label{sec:hyperdoctrines}

Now we move on to the categorical semantics of first-order logic.
Continued adherence to principle~\eqref{princ:structural} suggests that the \emph{structural rules}, including the substitution of terms into propositions, should correspond to basic operations in an appropriate categorical structure.
Thus, we should have a cartesian multicategory \cM whose objects and morphisms represent the types and terms, together with a set $\cP(A_1,\dots,A_n)$ of ``propositions'' for each finite list of objects, admitting ``substitution'' operations, and so on.

It is possible to make this precise (see \cref{ex:pshf-multicat}).
However, since (I hope) the point about contexts in type theory corresponding to domains of generalized multicategories has already been made sufficiently in \cref{chap:simple}, and such categorical generality seems in practice not to be very useful, at this point we make a small concession to established tradition and to convenience.
Thus, instead of a cartesian multicategory, we will use a category with products to represent our base theory.
Since we do not include product types in our base theory, this means that the free structure generated from a first-order logic will involve the \emph{category of contexts} introduced in \cref{sec:fp-theories}, and possess a universal property only up to equivalence.

This change enables us to describe a categorical counterpart of the propositions and their entailments much more simply.
Let \cS be a category (generally one having products), and let \fS be a faithful cartesian club.
Recall from \cref{sec:cartmulti} the notion of \fS-multicategory and \fS-multiposet.
In contrast to \cref{chap:unary,chap:simple}, in this chapter we will assume for simplicity that our multiposets \emph{do} satisfy antisymmetry: if $x\le y$ and $y\le x$ then $x=y$.
Allowing distinct isomorphic objects, while morally correct, would lead us down a 2-categorical road that we prefer to postpone until \cref{sec:indexed-moncat}.

\begin{defn}
  An \textbf{\cS-indexed \fS-multiposet} is a functor \cP from $\cS\op$ to the category of \fS-multiposets.
\end{defn}

This simple definition includes categorical counterparts of the type contexts (objects of \cS), terms (morphisms of \cS), substitution into terms (composition in \cS), propositions (objects of $\cP(\Gamma)$), entailments (morphisms of $\cP(\Gamma)$), cut for propositions (composition in $\cP(\Gamma)$), and substitution of terms into propositions and entailments (the functorial action of $\cP$).
In general for a morphism $f:\Gamma\to\Delta$ in \cS, we write $f^* : \cP(\Delta) \to \cP(\Gamma)$ for this latter action and call it a \textbf{reindexing} or \textbf{substitution} functor.

The propositional operations imported from \cref{sec:logic} are also easy to describe categorically.

\begin{defn}
  Let \cP be an \cS-indexed \fS-multiposet.
  We say that \cP has \textbf{products}, \textbf{coproducts}, is \textbf{representable}, or is \textbf{closed}, if each \fS-multiposet $\cP(\Gamma)$ has the corresponding structure, and that structure is preserved by the reindexing functors $f^*$.
\end{defn}

We did not define formally in \cref{sec:multicats-catth} what it means for a functor to preserve all these properties of a multicategory, but we trust the reader can do it.
The requirement that $f^*$ preserve these properties is necessary because substitution in type theory does, by definition, preserve the type operations: $(\ph\meet\psi)[M/x] = (\ph[M/x] \meet \psi[M/x])$ and so on.
Thus, in the free structure built from type theory the reindexing functors do preserve all the relevant structure, so we can't hope for it to be initial except in a world where that structure is always preserved.

Of course, one may naturally wonder, where do indexed multiposets with these properties come from?
We will consider this question in more depth in \cref{sec:subobjects}, but for now we mention one very important example that should help the intuition.

\begin{eg}\label{eg:subset-hyperdoctrine}
  Let $\cS=\bSet$ be the category of sets, and define $\cP(\Gamma)$ to be the poset of subsets of the set $\Gamma$, with its cartesian multiposet structure.
  The latter is in fact a Heyting algebra, and moreover a Boolean algebra: $\meet$ is intersection, $\join$ is union, $\neg$ is complement.
\end{eg}

It remains to consider categorical analogues of the quantifiers and equality.
Lawvere's fundamental insight~\cite{lawvere:adjointness,lawvere:comprehension} was that these correspond categorically to \emph{adjoint functors}.

Consider for instance the rules for $\all$.
If we remove the built-in substitution from $\forallE$, we can write the two rules as
\begin{mathpar}
  \inferrule{\Gamma,x:A\cb \Theta\types \ph}{\Gamma\cb\Theta\types \forall x:A.\ph}\;\forallI\and
  \inferrule{\Gamma\cb\Theta\types \forall x:A.\ph}{\Gamma,x:A\cb\Theta\types \ph}\;\forallE
\end{mathpar}
which are clearly inverses to each other.
Categorically, they say that to have a morphism from $\Theta$ to $\forall x:A.\ph$ in $\cP(\Gamma)$ is equivalent to having a morphism from $\Theta$ to $\ph$ in $\cP(\Gamma,A)$.
Here the second $\Theta$ technically denotes the weakening of $\Theta$ to the context $\Gamma,x:A$, which categorically will be the functorial action of $\cP$ applied to the projection $(\Gamma,A)\to \Gamma$.
Note that the latter is one of the projections of a cartesian product in the category of contexts.
This leads to the following definition.

\begin{defn}\label{defn:multicat-radj}
  Let $F:\cM\to\cN$ be a functor of \fS-multicategories.
  We say it \textbf{has a right adjoint} if for each object $B\in\cN$ there is an object $GB\in\cM$ and a morphism $\ep_B:FGB\to B$ in \cN such that for any $A_1,\dots,A_n\in \cM$, the composite
  \[ \cM(A_1,\dots,A_n;GB) \xto{F} \cN(FA_1,\dots,FA_n;FGB) \xto{\ep_B\circ -} \cN(FA_1,\dots,FA_n;B) \]
  is a bijection.
\end{defn}

The case $n=1$ of this definition implies immediately that the underlying ordinary functor of $F$ has a right adjoint in the usual sense.
Conversely, in the case when \cM and \cN are representable, it is sufficient to have such an underlying adjoint together with the fact that $F$ preserves tensor products; see \cref{ex:moncat-radj}.
Moreover, if $G$ exists, it can be made into a functor $\cN\to\cM$, that is right adjoint to \cM in an appropriate 2-category of \fS-multicategories; see \cref{ex:multicat-radj}.

We need one more thing for a categorical analogue of $\all$: we need to know that this structure is ``preserved by the reindexing functors'' in an appropriate sense.
The appropriate sense is the following.

\begin{defn}\label{defn:bc}
  Let \cS be a category, let $\cP:\cS\op\to\bCat$ be a functor, and suppose we have a commutative square in \cS:
  \[ \xymatrix{ A \ar[r]^h \ar[d]_f & C \ar[d]^g \\ B \ar[r]_k & D. } \]
  Suppose furthermore that the functors $f^*:\cP(B) \to \cP(A)$ and $g^*:\cP(D) \to\cP(C)$ have right adjoints $f_*$ and $g_*$.
  We say that \cP satisfies the \textbf{right Beck--Chevalley condition} with respect to this square (or sometimes that the square satisfies the Beck-Chevalley condition with respect to \cP) if the composite natural transformation
  \[ k^* g_* \xto{\eta k^* g_*} f_* f^* k^* g_*  = f_* h^* g^* g_* \xto{f_* h^* \ep} f_* h^* \]
  is an isomorphism.
  Dually, if $f^*$ and $g^*$ have left adjoints $f_!$ and $g_!$, we say \cP satisfies the \textbf{left Beck--Chevalley condition} with respect to the above square if the composite
  \[ f_! h^* \xto{f_! h^* \eta} f_! h^* g^* g_! = f_! f^* k^* g_! \xto{\ep k^* g_!} k^* g_! \]
  is an isomorphism.
\end{defn}

When \cP is an \cS-indexed \fS-multiposet, we apply this definition to its underlying functor into posets (regarded as categories).
Since our posets are antisymmetric, every isomorphism is an equality, and so in this case we have $k^* g_* = f_* h^*$ (or $f_! h^* = k^* g_!$).
Now we can state:

\begin{defn}
  An \cS-indexed \fS-multiposet \textbf{has universal quantifiers} if
  \begin{enumerate}
  \item For any objects $\Gamma,A\in \cS$, the reindexing functor $\cP(\Gamma) \to \cP(\Gamma\times A)$ has a right adjoint in the sense of \cref{defn:multicat-radj}; and
  \item For any morphism $f:\Gamma\to\Delta$ and object $A$ in \cS, \cP satisfies the right Beck--Chevalley condition with respect to the square
    \[ \xymatrix@C=3pc{ \Gamma\times A \ar[r]^{f\times \idfunc_A} \ar[d] & \Delta\times A \ar[d] \\ \Gamma \ar[r]_f & \Delta. } \]
  \end{enumerate}
\end{defn}

Note that the Beck--Chevalley condition is true in the syntax because the universal quantifier is preserved by substitution, by definition of substitution: $(\forall x:A.\ph)[M/y] = \forall x:A. \ph[M/y]$ as long as $y\neq x$.
In \cref{eg:subset-hyperdoctrine}, the right adjoint to $(\pi_A)^* : \cP(\Gamma) \to \cP(\Gamma\times A)$ is similarly defined by
\[ (\pi_A)_*(\ph) = \setof{ x\in \Gamma | \all y\in A. (x,y)\in\ph }. \]

The existential quantifier is similar, but a bit more subtle.
The rule $\existsE$
\[ \inferrule{\Gamma,x:A\cb\Theta,\ph\types \psi}{\Gamma\cb\Theta,(\exists x:A.\ph)\types \psi}\]
certainly looks like one direction of some kind of adjunction.
The other direction is not quite as obviously expressed by $\existsI$, but we can get it by combining $\existsI$ with a cut:
\begin{mathpar}
  \inferrule*[right=cut]{\inferrule*[right=$\existsI$]{\inferrule*{ }{\Gamma,x:A \cb \ph \types \ph}}{\Gamma,x:A \cb \ph \types \exists x:A.\ph}  \\ 
    \inferrule*[Right=weakening]{\Gamma\cb\Theta,(\exists x:A.\ph)\types \psi}{\Gamma,x:A\cb\Theta,(\exists x:A.\ph)\types \psi
    }}{\Gamma,x:A\cb\Theta,\ph\types \psi}
\end{mathpar}
There is another subtlety for $\exis$, namely the presence of the extra context $\Theta$.
Translating directly across the correspondence to multicategories, this leads to the following definition.

\begin{defn}\label{defn:multicat-hopf-ladj}
  Let $G:\cM\to\cN$ be a functor of \fS-multicategories.
  We say it \textbf{has a Hopf left adjoint} if for each object $B\in\cN$ there is an object $FB\in\cM$ and a morphism $\eta:B\to GFB$ in \cN such that for any objects $A_1,\dots,A_n,C_1,\dots,C_m,D\in \cM$, the composite
  \[ \cM(\vec A,FB,\vec C;D) \xto{G} \cN(G\vec A, GFB, G\vec C; GD) \xto{-\circ_{(n+1)} \eta} \cN(G\vec A, B, G\vec C; GD) \]
  is a bijection.
\end{defn}

As before, the case $n=m=1$ implies that the underlying ordinary functor has a left adjoint in the usual sense.
Conversely, when \cM and \cN are representable and $G$ preserves tensor products, an underlying left adjoint is a Hopf left adjoint just when some canonical maps are isomorphisms; see \cref{ex:hopf-ladj}.
Unlike the case of right adjoints, however, in general a Hopf left adjoint cannot be made into a functor of multicategories.

\begin{defn}
  An \cS-indexed \fS-multiposet \textbf{has existential quantifiers} if
  \begin{enumerate}
  \item For any objects $\Gamma$ and $A$ of \cS, the reindexing functor $\cP(\Gamma) \to \cP(\Gamma\times A)$ has a Hopf left adjoint in the sense of \cref{defn:multicat-hopf-ladj}; and
  \item For any morphism $f:\Gamma\to\Delta$ and object $A$ in \cS, \cP satisfies the left Beck--Chevalley condition with respect to the square
    \[ \xymatrix@C=3pc{ \Gamma\times A \ar[r]^{f\times \idfunc_A} \ar[d] & \Delta\times A \ar[d] \\ \Gamma \ar[r]_f & \Delta. } \]
  \end{enumerate}
\end{defn}

Unsurprisingly, in \cref{eg:subset-hyperdoctrine}, the left adjoint to $(\pi_A)^*:\cP(\Gamma) \to \cP(\Gamma\times A)$ is similarly defined by
\[ (\pi_A)_!(\ph) = \setof{ x\in \Gamma | \exis y\in A. (x,y)\in\ph }. \]

Finally, we consider the rules for equality.
If we remove the built-in substitutions, these look like
\begin{mathpar}
  \inferrule{ }{\Gamma,x:A\cb ()\types (x=_A x)}\and
  \inferrule{\Gamma,x:A\cb\Theta[x/y]\types \ph[x/y]}{\Gamma,x:A,y:B\cb\Theta,(x=_A y)\types \ph}
\end{mathpar}
As with $\exis$, we can use a cut and a substitution to conclude the opposite of the second rule:
\begin{mathpar}
  \inferrule*[right=cut]{
    \inferrule*{ }{\Gamma,x:A \cb () \types x=_A x}\\
    \inferrule*[Right=subst $x/y$]{\Gamma,x:A,y:B\cb\Theta,(x=_A y)\types \ph}
    {\Gamma,x:A\cb\Theta[x/y],(x=_A x)\types \ph[x/y]}
  }{\Gamma,x:A\cb\Theta[x/y]\types \ph[x/y]}
\end{mathpar}
This looks very much like a (Hopf) left adjoint to substitution along the diagonal $(\Gamma,A) \to (\Gamma,A,A)$ in the category of contexts; but there is \emph{no} proposition in context $(\Gamma,A)$ that it is applied to.
This suggests the following definitions.

\begin{defn}\label{defn:multicat-hopf-ladj-empty}
  Let $G:\cM\to\cN$ be a functor of \fS-multicategories.
  We say it \textbf{has a Hopf left adjoint at $()$} if there is an object $F\in\cM$ and a morphism $\eta:()\to GF$ in \cN such that for any objects $A_1,\dots,A_n,C_1,\dots,C_m,D\in \cM$, the composite
  \[ \cM(\vec A,F,\vec C;D) \xto{G} \cN(G\vec A, GF, G\vec C; GD) \xto{-\circ_{(n+1)} \eta} \cN(G\vec A, G\vec C; GD) \]
  is a bijection.
\end{defn}

\begin{defn}
  Suppose given an \cS-indexed \fS-multiposet \cP and a commutative square in \cS:
  \[ \xymatrix{ A \ar[r]^h \ar[d]_f & C \ar[d]^g \\ B \ar[r]_k & D, } \]
  and suppose that the reindexing functors $f^*$ and $g^*$ have Hopf left adjoints at $()$, given by objects $f_!\in \cP(B)$ and $g_!\in \cP(D)$.
  Then there is a unique morphism $f_! \to k^* g_!$ in $\cP(B)$ such that the composite $() \xto{\eta} f^* f_! \to f^* k^* g_!$ in $\cP(A)$ is equal to the composite $() \xto{h^* \eta} h^* g^* g_!$ (note $f^* k^* = h^* g^*$).
  We say that \cP satisfies the \textbf{left Beck--Chevalley condition at $()$} with respect to this square if this morphism $f_! \to k^* g_!$ is an isomorphism.
\end{defn}

% Note that \cref{defn:multicat-hopf-ladj} is of the form ``for every object $B$''; we say that $G$ \textbf{has a Hopf left adjoint at $B$} if the condition in \cref{defn:multicat-hopf-ladj} holds only for a particular object $B$.
% Similarly, in the situation of \cref{defn:bc}, if $g^*$ has a left adjoint at $B$, and $f^*$ has a left adjoint at $h^*B$, we say that \cP \textbf{satisfies the left Beck--Chevalley condition at $B$} with respect to the given square if the relevant map $f_! h^* B \to k^* g_! B$ is an isomorphism.

\begin{defn}
  An \cS-indexed \fS-multiposet with unit objects \textbf{has equality} if
  \begin{enumerate}
  \item For any objects $\Gamma$ and $A$ of \cS, the reindexing functor $\cP(\Gamma\times A\times A) \to \cP(\Gamma\times A)$ has a Hopf left adjoint at $()$; and
  \item For any morphism $f:\Gamma\to\Delta$ and object $A$ in \cS, \cP satisfies the left Beck--Chevalley condition at $()$ with respect to the square
    \[ \xymatrix@C=5pc{ \Gamma\times A \ar[r]^{f\times \idfunc_A} \ar[d] & \Delta\times A \ar[d] \\
      \Gamma\times A\times A \ar[r]_{f\times \idfunc_A \times \idfunc_A} & \Delta\times A\times A. } \]
  \end{enumerate}
\end{defn}

As before, the Beck--Chevalley condition is true in the syntax because ``equality is preserved by substitution''.
The relevant substitution here is not the one built into the equality rule, though, but the substitution for different variables, which doesn't change the equality proposition at all: $(x=_A y)[M/z] = (x=_A y)$ as long as $z\neq x$ and $z\neq y$.
In \cref{eg:subset-hyperdoctrine}, the left adjoint to $(\Delta_A)^* : \cP(\Gamma\times A\times A) \to \cP(\Gamma\times A)$ at $()$ is defined by
\[ (\Delta_A)_! = \setof{ (z,x,y)\in \Gamma\times A\times A | x=y }. \]

Note that we are sticking doggedly to the principle that just as the rules for a given type operation should be independent of any other type operations, the corresponding universal property should be statable without reference to any other objects with universal properties.\footnote{At least, other universal properties in the multiposets $\cP(\Gamma)$.
We do still refer to cartesian products in \cS, but we could also remove those as in \cref{ex:pshf-multicat}.}
If we \emph{do} have additional structure, particularly tensor products and units in the multiposets $\cP(\Gamma)$, then our various kinds of adjoints can be formulated in terms of those and ordinary adjunctions --- see \cref{ex:multicat-radj,ex:moncat-radj,ex:hopf-ladj,ex:hopf-ladj-at-one} --- and our examples in \cref{sec:subobjects} will mainly arise in this way.
However, to make a closer connection to the type theory we prefer to formulate them independently first.

\begin{defn}
  A \textbf{first-order \fS-hyperdoctrine} consists of a category \cS with finite products together with an \cS-indexed \fS-multiposet that is closed and representable and has products (finite meets), coproducts (finite joins), universal and existential quantifiers, and equality.

  By default, a \textbf{first-order hyperdoctrine} refers to the cartesian case where $\fS$ contains all functions; in this case representability is equivalent to having finite meets.
  More generally, an \cS-indexed cartesian multiposet is called a:
  \begin{enumerate}
  \item \textbf{coherent hyperdoctrine} if it has finite meets, finite joins, existential quantifiers, and equality;
  \item \textbf{geometric hyperdoctrine} if it has finite meets, infinite joins, existential quantifiers, and equality;
  \item \textbf{regular hyperdoctrine} if it has finite meets, existential quantifiers, and equality; and a
  \item \textbf{Horn hyperdoctrine} if it has finite meets and equality.
  \end{enumerate}
\end{defn}

Note that since all the structure of a first-order \fS-hyperdoctrine is determined by universal properties, it is unique up to isomorphism, and hence unique on the nose in an (antisymmetric) poset.
Thus, there is no need to suppose separately that we have \emph{chosen} such operations.

\begin{thm}\label{thm:fol-initial}
  The free first-order \fS-hyperdoctrine generated by a first-order \fS-theory can be presented, up to equivalence, by the type theory of the latter:
  \begin{itemize}
  \item \cS is the category of type contexts; and
  \item the poset $\cP(\Gamma)$ is obtained from the poset of proposition judgments $\Gamma\types \ph\prop$ and derivable entailments $\Gamma\cb\Theta\types\ph$ by identifying isomorphic objects (since in this section our posets are antisymmetric).
  \end{itemize}
  (And similarly for the other fragments with fewer type operations.)
\end{thm}
\begin{proof}
  We have already observed that this structure defines an indexed \fS-multicategory and that the simple type operations $\meet,\top,\join,\bot,\tensor,\one,\hom$ yield the appropriate multicategorical strurcture.
  Moreover, we defined the categorical notions of universal and existential quantifiers and equality precisely so that they would hold in the syntax; thus the description above does yield a first-order \fS-hyperdoctrine.

  Now, the underlying multigraph of a first-order \fS-theory is of course a finite-product theory without axioms, and we showed in \cref{sec:fp-theories} that the category of contexts of its type theory is, up to equivalence, the free category with products it generates.
  Thus, it maps uniquely (up to isomorphism) into the base category of any other first-order \fS-hyperdoctrine; it remains to show that this map extends uniquely to a map of hyperdoctrines, i.e.\ a natural transformation between the $\cP$-functors preserving all the structure.

  As usual, we do this by induction on derivations.
  The proposition judgment $\Gamma\types \ph\prop$ is easy: each rule corresponds to one of the objects with a universal property that we have assumed to exist in any first-order \fS-hyperdoctrine.
  Next, since the rules for entailment involve substitution of terms into propositions, before defining our functor on entailments we have to first prove that it maps such substitutions to the reindexing functors in the target; this is another straightforward induction on derivations of $\Gamma\types \ph\prop$.
  Now the rules for entailment involving simple type operations are also easy, just as in \cref{sec:logic}; the interesting part has to do with the rules for quantifiers and equality.

  The rule $\forallI$ (refer to \cref{fig:fol}) simply applies the universal property of a right adjoint, so it exists in the target.
  The rule $\forallE$ applies that universal property in reverse, then substitutes (applies a reindexing functor).

  The rule $\existsE$ simply applies the universal property of a Hopf left adjoint.
  The rule $\existsI$ is somewhat less obvious, but we can obtain it from the unit of that adjunction together with a substitution and a cut:
  \begin{mathpar}
    \inferrule*[right=cut]{
      \Gamma\cb \Theta \types \ph[M/x]\\
      \inferrule*[Right=subst $M/x$]{\Gamma\types M:A \\
        \inferrule*[Right=unit]{ }{\Gamma,x:A \cb \ph \types \exists x:A.\ph}}{\Gamma\cb \ph[M/x] \types \exists x:A.\ph}
    }{\Gamma\cb\Theta \types \exists x:A.\ph}
  \end{mathpar}
  Thus, it also exists in the target.

  The rule $\eqI$ simply applies the unit of the Hopf left adjoint at $()$, followed by a substitution.
  Finally, the rule $\eqE$ applies the universal property of that Hopf left adjoint, followed by another substitution.

  This completes the definition on entailments.
  Since everything is posetal there is not much left to do: we show that our map preserves all the hyperdoctrine structure, essentially by definition, and then that it is unique (modulo the up-to-isomorphism uniqueness of the functor on base categories), because its definition was forced at every step.
\end{proof}

\begin{rmk}\label{rmk:fol-soundness-completeness}
  As noted in \cref{rmk:soundness-completeness} for propositional logic, \cref{thm:fol-initial} implies the traditional \emph{soundness} and \emph{completeness} theorems for first-order logic with respect to hyperdoctrines.
  The soundness theorem says that if we can prove $\Gamma\cb ()\types \ph$, then when we interpret our logic into any hyperdoctrine, $\ph$ must go to the top element, i.e.\ it must ``be true''.
  In particular, this applies to models in the hyperdoctrine of sets and subsets from \cref{eg:subset-hyperdoctrine}, which are the classical notion of ``model''.
  (In \cref{sec:subobjects} we will construct hyperdoctrines from more general categories than \bSet.)
  Conversely, the completeness theorem says that if something is true in all hyperdoctrines, then it must in particular be true in the free one constructed from the type theory, and therefore must be provable in the type theory.
\end{rmk}

\begin{rmk}
  Note that all categorical structure corresponding to quantifiers and equality takes the form of \emph{certain} adjoints to reindexing functors.
  As we will see in \cref{sec:subobjects}, most examples arising in practice naturally have adjoints to \emph{all} the reindexing functors (if they have any).
  However, this is not actually an additional condition; given only the adjoints assumed in our definition of first-order hyperdoctrine, we can \emph{construct} adjoints to arbitrary reindexing functors and \emph{prove} that they satisfy some Beck--Chevalley conditions.
  At the moment, we leave this proof to the reader; see \cref{ex:hyperdoctrine-alladj}.
  (In fact, it is more usual to include all such adjoints, and their Beck--Chevalley conditions, in the definition of ``hyperdoctrine''.)
  % TODO: Do the construction of adjoints, at least, here as an example; let the reader check BC.
\end{rmk}


\subsection*{Exercises}

\begin{ex}\label{ex:pshf-multicat}
  Define a notion of ``presheaf on a cartesian multicategory'' that corresponds more directly to the structure seen in our first-order type theory, without passing to the category of contexts.
\end{ex}

\begin{ex}\label{ex:multicat-radj}
  Suppose a functor $F:\cM\to\cN$ of \fS-multicategories has a right adjoint in the sense of \cref{defn:multicat-radj}.
  \begin{enumerate}
  \item Prove that if \cM and \cN are both representable, then $F$ preserves tensor products in the sense of \cref{ex:mcat-strong-func}.
  \item Extend $G$ to a functor $G:\cN\to\cM$.
  \item Define a 2-category of \fS-multicategories and show that $G$ is right adjoint to $F$ in this 2-category (i.e.\ there are 2-cells $\eta : 1 \to G F$ and $\ep : F G \to 1$ in this 2-category satisfying the triangle identities).
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:moncat-radj}
  Let \cM and \cN be representable \fS-multicategories.
  Prove that a functor $F:\cM\to\cN$ has a right adjoint in the sense of \cref{defn:multicat-radj} if and only if (1) it preserves tensor products in the sense of \cref{ex:mcat-strong-func} and (2) its underlying ordinary functor has a right adjoint.
\end{ex}

\begin{ex}\label{ex:multicat-prod-ladj}
  Show that an \fS-multicategory \cM has binary products, in the sense defined before \cref{thm:multicat-prod}, if and only if the diagonal $\cM\to \cM\times \cM$ has a right adjoint in the sense of \cref{defn:multicat-radj}.
\end{ex}

\begin{ex}\label{ex:beck-chev}
  Let $\cP:\cS\op\to\bCat$ and suppose we have a commutative square in \cS:
  \[ \xymatrix{ A \ar[r]^h \ar[d]_f & C \ar[d]^g \\ B \ar[r]_k & D. } \]
  such that $f^*$ and $g^*$ have left adjoints and also $h^*$ and $k^*$ have right adjoints.
  Prove that \cP satisfies the left Beck-Chevalley condition with respect to this square if and only if it satisfies the right Beck--Chevalley condition with respect to the transposed square
  \[ \xymatrix{ A \ar[r]^f \ar[d]_h & B \ar[d]^k \\ C \ar[r]_g & D. } \]
\end{ex}

\begin{ex}\label{ex:hopf-ladj}
  Let \cM and \cN be representable \fS-multicategories, and $G:\cM\to\cN$ a functor preserving tensor products.
  \begin{enumerate}
  \item Show that $G$ has a Hopf left adjoint if and only if its underlying ordinary functor has a left adjoint $F$ such that the canonical map
    \begin{gather*}
      F(A\tensor G B) \to F(G F A \tensor G B) \toiso F G(F A \tensor B) \to F A \tensor B
    \end{gather*}
    is an isomorphism for any $A\in\cN$ and $B\in \cM$.
  \item If \cM and \cN are additionally closed, and $G$ is also closed in the sense that the canonical maps $G(A\hom B) \to G A \hom G B$ are isomorphisms, prove that $g$ has a Hopf left adjoint if and only if its underlying ordinary functor has a left adjoint.
  % TODO: If $G$ is not assumed to preserve tensor products, does a "Hopf left adjoint" in the multicategory sense imply a Hopf adjunction in the monoidal category sense?
  \end{enumerate}
\end{ex}

\begin{ex}\label{ex:multicat-coprod-ladj}
  Show that an \fS-multicategory \cM has binary coproducts, in the sense defined before \cref{thm:multicat-coprod}, if and only if the diagonal $\cM\to \cM\times \cM$ has a Hopf left adjoint.
\end{ex}

\begin{ex}\label{ex:hopf-ladj-at-one}
  Let \cM and \cN be \fS-multicategories, let $G:\cM\to\cN$ be a functor having a Hopf left adjoint, and assume that \cN has a unit object.
  Prove that $G$ also has a Hopf left adjoint at $()$.
\end{ex}

\begin{ex}\label{ex:hyperdoctrine-alladj}
  Suppose $\cP:\cS\op\to\mathbf{Heyt}$ is a first-order \fS-hyperdoctrine as defined in the text.
  \begin{enumerate}
  \item Prove (using type theory or commutative diagrams, your choice) that in fact the reindexing functor $f^* : \cP(\Delta) \to \cP(\Gamma)$ has a Hopf left adjoint for \emph{all} morphisms $f:\Gamma\to\Delta$ in $\cS$.
    \textit{(Hint: in the hyperdoctrine of subsets over $\mathbf{Set}$, these left adjoints can be defined by $f_!(\varphi) = \setof{ y\in \Delta | \exis x\in \Gamma. (x\in\varphi\meet f(x) = y) }$.)}
  \item Similarly, prove that $f^*$ has a right adjoint for all $f$.
  \item Prove that these left adjoints satisfy both Beck--Chevalley conditions for commutative squares of the following form:
    \[
    \xymatrix@C=3pc{ A \ar[r]^-{(1,f)} \ar[d]_{f} & A\times B \ar[d]^{f\times 1} \\ B \ar[r]_-{\Delta} & B\times B }\hspace{2cm}
    \xymatrix@C=3pc{A \ar[r]^-\Delta \ar[d]_\Delta & A\times A \ar[d]^{1\times \Delta} \\ A\times A \ar[r]_-{\Delta\times 1} & A\times A\times A}
    \]
  \end{enumerate}
\end{ex}

\section{Hyperdoctrines of subobjects}
\label{sec:subobjects}

Finally, we turn to the question of where hyperdoctrines come from.
From now on we will focus entirely on the cartesian case, which is the most-studied and most-applicable.
\cref{eg:subset-hyperdoctrine} suggests that from a category \cS, we should try to construct a hyperdoctrine such that for $\Gamma\in\cS$, $\cP(\Gamma)$ is a poset of ``subobjects'' of $\Gamma$.
Moreover, there is a standard way to define a subobject of $\Gamma$, namely as an isomorphism class of monomorphisms with target $\Gamma$.

We write this poset as $\sub_\cS(\Gamma)$, or just $\sub(\Gamma)$.
To make $\sub_\cS$ into an \cS-indexed poset in a natural way, we need \cS to have pullbacks of monomorphisms along arbitrary morphisms.
However, a category with finite products and pullbacks of monomorphisms automatically has all finite \emph{limits}, since the equalizer of $f,g:A\to B$ can be constructed as the pullback of the monomorphism $\Delta:B\to B\times B$ along $(f,g):A\to B\times B$.

Thus, \emph{from now on we assume that \cS has finite limits}.
Then $\sub_\cS$ is an \cS-indexed poset, and moreover it has products (meets) and a terminal (greatest) object; the former are given by pullback of monomorphisms and the latter by the monomorphism $\idfunc_\Gamma : \Gamma \to \Gamma$.
We can also show:

\begin{thm}
  If \cS has finite limits, then $\sub_\cS$ has equality.
  Therefore, it is a Horn hyperdoctrine.
\end{thm}
\begin{proof}
  [TODO]
\end{proof}



% \begin{rmk}\label{rmk:subset-quotient}
%   Would introduce type dependency.
% \end{rmk}


\section{Finite-limit theories}
\label{sec:lex-theories}

% Use a "cartesian proposition" judgment and the Yoneda embedding.  Other things to try would introduce type dependency; maybe do some of them as a baby starting point in \cref{chap:dtt}.


\section{Indexed monoidal categories}
\label{sec:indexed-moncat}

% Local Variables:
% TeX-master: "catlog"
% End:
